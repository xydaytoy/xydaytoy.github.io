<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>札记</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-11-29T10:54:58.252Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>xydaytoy</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>每周知识碎片1</title>
    <link href="http://example.com/2021/11/29/weekly-211129/"/>
    <id>http://example.com/2021/11/29/weekly-211129/</id>
    <published>2021-11-29T07:49:47.000Z</published>
    <updated>2021-11-29T10:54:58.252Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>疯狂写大作业的一周。</p></blockquote><span id="more"></span><h1 id="python-获取路径相关"><a href="#python-获取路径相关" class="headerlink" title="python 获取路径相关"></a>python 获取路径相关</h1><p>获取linux下的用户根路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> os</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>os.envrion[<span class="string">&#x27;HOME&#x27;</span>]</span><br><span class="line"><span class="string">&#x27;/home/user&#x27;</span></span><br></pre></td></tr></table></figure><p>连接路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import os</span><br><span class="line">&gt;&gt;&gt; os.path.join(os.envrion[&#39;HOME&#39;], &#39;xxx&#39;, &#39;yyy.txt&#39;)</span><br><span class="line">&#39;&#x2F;home&#x2F;user&#x2F;xxx&#x2F;yyy.txt&#39;</span><br></pre></td></tr></table></figure><p>glob返回所有匹配的文件路径列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from glob import glob</span><br><span class="line">&gt;&gt;&gt; files &#x3D; glob(os.path.join(ROOT, cat, &#39;*.txt&#39;))</span><br><span class="line">[&#39;xxx.txt&#39;, &#39;111.txt&#39;]</span><br></pre></td></tr></table></figure><h1 id="错误：AttributeError-module-‘threading’-has-no-attribute-‘RLock’"><a href="#错误：AttributeError-module-‘threading’-has-no-attribute-‘RLock’" class="headerlink" title="错误：AttributeError: module ‘threading’ has no attribute ‘RLock’"></a>错误：AttributeError: module ‘threading’ has no attribute ‘RLock’</h1><p><strong>错误原因</strong>：自己的代码文件与python自带的脚本文件重名，导致import引入包的时候引入错误。</p><p><strong>修改办法</strong>：根据错误提示，修改相关文件的文件名，要注意除了提示的包，在引入该包的过程中引入的其他包的错误也可能导致这个错误提示，例如该错误提示为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">nohup: ignoring input</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;data/split_data.py&quot;, line 4, in &lt;module&gt;</span><br><span class="line">    import pandas as pd</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/pandas/__init__.py&quot;, line 11, in &lt;module&gt;</span><br><span class="line">    __import__(dependency)</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/numpy/__init__.py&quot;, line 142, in &lt;module&gt;</span><br><span class="line">    from . import core</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/numpy/core/__init__.py&quot;, line 103, in &lt;module&gt;</span><br><span class="line">    from . import _internal</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/numpy/core/_internal.py&quot;, line 11, in &lt;module&gt;</span><br><span class="line">    import platform</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/platform.py&quot;, line 117, in &lt;module&gt;</span><br><span class="line">    import sys, os, re, subprocess</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/subprocess.py&quot;, line 131, in &lt;module&gt;</span><br><span class="line">    import threading</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/threading.py&quot;, line 7, in &lt;module&gt;</span><br><span class="line">    from traceback import format_exc as _format_exc</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/traceback.py&quot;, line 5, in &lt;module&gt;</span><br><span class="line">    import linecache</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/linecache.py&quot;, line 11, in &lt;module&gt;</span><br><span class="line">    import tokenize</span><br><span class="line">  File &quot;/home/user/text-classify/Bert-THUCNews-Classification-master/data/tokenize.py&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    from pytorch_pretrained_bert import BertTokenizer</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/pytorch_pretrained_bert/__init__.py&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    from .tokenization import BertTokenizer, BasicTokenizer, WordpieceTokenizer</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/pytorch_pretrained_bert/tokenization.py&quot;, line 20, in &lt;module&gt;</span><br><span class="line">    import logging</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/logging/__init__.py&quot;, line 212, in &lt;module&gt;</span><br><span class="line">    _lock = threading.RLock()</span><br><span class="line">AttributeError: module &#x27;threading&#x27; has no attribute &#x27;RLock&#x27;</span><br></pre></td></tr></table></figure><p>但并不是<code>threading</code>包有重名问题，而是在<code>import tokenize</code>这步中，本文件夹下有一个<code>tokenize.py</code>文件，可以看到引入的file路径和其他明显不同。</p><h1 id="错误：No-module-named-xxx"><a href="#错误：No-module-named-xxx" class="headerlink" title="错误：No module named xxx"></a>错误：No module named xxx</h1><p>引入自己写的代码中的文件时<code>from filename import xxx</code>，出现这个错误，检查下面两点：</p><p>1.在被引入(import)的目录下需要有一个<code>__init__.py</code>文件，该文件可以为空。</p><p>2.同级目录不可以直接导入，需要加上上一级的文件名，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from folder.filename import xxx</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;疯狂写大作业的一周。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="weekly" scheme="http://example.com/categories/weekly/"/>
    
    
  </entry>
  
  <entry>
    <title>linux-tips</title>
    <link href="http://example.com/2021/11/28/linux-tips/"/>
    <id>http://example.com/2021/11/28/linux-tips/</id>
    <published>2021-11-28T05:29:06.000Z</published>
    <updated>2021-11-29T05:54:49.413Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>linux服务器下的一些常用操作。</p></blockquote><span id="more"></span><h1 id="0-在线手册"><a href="#0-在线手册" class="headerlink" title="0 在线手册"></a>0 在线手册</h1><p>1.<a href="https://www.linuxcool.com/">Linux命令大全（手册）</a></p><p>2.<a href="https://www.cnblogs.com/jingmoxukong/p/7867397.html">一篇文章让你彻底掌握shell语言</a></p><p>3.<a href="https://www.runoob.com/linux/linux-command-manual.html">Linux命令大全（RUNOOB.COM）</a></p><h1 id="1-资源占用情况"><a href="#1-资源占用情况" class="headerlink" title="1 资源占用情况"></a>1 资源占用情况</h1><h2 id="1-1-GPU显卡占用"><a href="#1-1-GPU显卡占用" class="headerlink" title="1.1 GPU显卡占用"></a>1.1 GPU显卡占用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p>显示如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Sun Nov 28 13:47:48 2021       </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 430.64       Driver Version: 430.64       CUDA Version: 10.1     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  Tesla V100-PCIE...  Off  | 00000000:04:00.0 Off |                    0 |</span><br><span class="line">| N/A   60C    P0    73W / 250W |  16487MiB / 32510MiB |     39%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   1  Tesla V100-PCIE...  Off  | 00000000:06:00.0 Off |                    0 |</span><br><span class="line">| N/A   40C    P0    27W / 250W |      0MiB / 32510MiB |      0%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   2  Tesla V100S-PCI...  Off  | 00000000:0C:00.0 Off |                    0 |</span><br><span class="line">| N/A   75C    P0   166W / 250W |  17412MiB / 32510MiB |     27%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   3  Tesla V100S-PCI...  Off  | 00000000:0E:00.0 Off |                    0 |</span><br><span class="line">| N/A   72C    P0   140W / 250W |  15442MiB / 32510MiB |     28%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                       GPU Memory |</span><br><span class="line">|  GPU       PID   Type   Process name                             Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|    0     10685      C   ...g/anaconda3/envs/torch/bin/python 16475MiB |</span><br><span class="line">|    2     11578      C   ...g/anaconda3/envs/torch/bin/python 17401MiB |</span><br><span class="line">|    3     11579      C   ...g/anaconda3/envs/torch/bin/python 15431MiB |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="1-2-查看进程"><a href="#1-2-查看进程" class="headerlink" title="1.2 查看进程"></a>1.2 查看进程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top</span><br></pre></td></tr></table></figure><p>显示如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND </span><br><span class="line">   10010      20  10  10.15g  3.63g   200200   4.0  1.5   0:30:31 top</span><br></pre></td></tr></table></figure><p>查看某一用户的所有进程（top下）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">u username</span><br></pre></td></tr></table></figure><p>展开具体命令（top下）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c</span><br></pre></td></tr></table></figure><p>退出top命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctrl+c</span><br></pre></td></tr></table></figure><p>查看某一进程的详细信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux | grep PID</span><br></pre></td></tr></table></figure><p>结束进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill PID</span><br></pre></td></tr></table></figure><h1 id="2-文件路径"><a href="#2-文件路径" class="headerlink" title="2 文件路径"></a>2 文件路径</h1><p>1.当前路径：<code>$pwd</code></p><p>2.当前目录：<code>.</code>，上级目录：<code>..</code></p><p>3.根目录：<code>/</code></p><p>4.用户的home路径：<code>~</code>，<code>~user</code>表示用户名为user（在/etc/passwd中存在的用户名）的home路径</p><h1 id="3-从网页下载数据"><a href="#3-从网页下载数据" class="headerlink" title="3 从网页下载数据"></a>3 从网页下载数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget 数据链接</span><br></pre></td></tr></table></figure><h1 id="4-防误删（回收站）"><a href="#4-防误删（回收站）" class="headerlink" title="4 防误删（回收站）"></a>4 防误删（回收站）</h1><p>目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\.bashrc</span><br><span class="line">\00-trash</span><br><span class="line">-trash.sh</span><br><span class="line">-.trash</span><br><span class="line">-.bashrc</span><br></pre></td></tr></table></figure><h2 id="4-1-移入回收站"><a href="#4-1-移入回收站" class="headerlink" title="4.1 移入回收站"></a>4.1 移入回收站</h2><p>将<code>rm</code>命令删除的文件移入<code>.trash</code>文件中，并用时间命名防止重复。</p><p>使用时在<code>.bashrc</code>文件中加入以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># trash for rm command</span><br><span class="line">alias rm&#x3D;&#x2F;user&#x2F;00-trash&#x2F;trash.sh</span><br></pre></td></tr></table></figure><p>执行<code>source .bashrc</code>使其生效。</p><p><code>trash.sh</code>如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">#一个简易回收站</span><br><span class="line">#https:&#x2F;&#x2F;github.com&#x2F;LIU-LIU-LIU&#x2F;Recycle_bin</span><br><span class="line"></span><br><span class="line">TarshDir&#x3D;&quot;&#96;echo ~&#96;&#x2F;00-trash&#x2F;.trash&quot;</span><br><span class="line">FileNamePrefix&#x3D;&#96;date +%y-%m-%d-%H-%M-%S&#96;</span><br><span class="line"></span><br><span class="line">error()&#123;</span><br><span class="line">echo -e &quot;\033[31m 错误!\n移动至回收站不需要任何命令选项。格式:rm [文件]  \033[0m&quot;</span><br><span class="line">exit 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tarsh()&#123;</span><br><span class="line">case $1 in</span><br><span class="line">-*|.&#x2F;)</span><br><span class="line">        error</span><br><span class="line">;;</span><br><span class="line">&#x2F;)</span><br><span class="line">        echo -e &quot;\033[31m 此操作风险太高,已禁止! \033[0m&quot;</span><br><span class="line">        exit 1</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">        mkdir -p &quot;$TarshDir&quot;&#x2F;&quot;$FileNamePrefix&quot;</span><br><span class="line">        &#x2F;bin&#x2F;mv -i &quot;$@&quot; &quot;&quot;$TarshDir&quot;&#x2F;&quot;$FileNamePrefix&quot;&#x2F;&quot;</span><br><span class="line">        if [ &quot;$?&quot; &#x3D;&#x3D; &quot;0&quot; ] ; then</span><br><span class="line">                echo &quot;已将&quot;$@&quot;移动至回收站(&quot;$TarshDir&quot;&#x2F;&quot;$FileNamePrefix&quot;&#x2F;)&quot;</span><br><span class="line">        else</span><br><span class="line">                echo -e &quot;\033[31m 移动至回收站失败。\033[0m&quot;</span><br><span class="line">                &#x2F;bin&#x2F;rm -rf &quot;$TarshDir&quot;&#x2F;&quot;$FileNamePrefix&quot;</span><br><span class="line">        fi</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$1&quot; ];then</span><br><span class="line">        error</span><br><span class="line">else</span><br><span class="line">        tarsh $*</span><br><span class="line">fi</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="4-2-清空回收站"><a href="#4-2-清空回收站" class="headerlink" title="4.2 清空回收站"></a>4.2 清空回收站</h1><p>在<code>.trash</code>文件夹中，写一个<code>.bashrc</code>文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alias rm&#x3D;&quot;rm&quot;</span><br></pre></td></tr></table></figure><p>执行<code>source .bashrc</code>文件恢复原始<code>rm</code>命令功能，清空回收站。</p><h1 id="5-解压文件"><a href="#5-解压文件" class="headerlink" title="5 解压文件"></a>5 解压文件</h1><h2 id="5-1-zip文件"><a href="#5-1-zip文件" class="headerlink" title="5.1 zip文件"></a>5.1 zip文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip filename</span><br></pre></td></tr></table></figure><h2 id="5-2-tar-gz文件"><a href="#5-2-tar-gz文件" class="headerlink" title="5.2 tar.gz文件"></a>5.2 tar.gz文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf filename</span><br></pre></td></tr></table></figure><h1 id="6-执行进程相关"><a href="#6-执行进程相关" class="headerlink" title="6 执行进程相关"></a>6 执行进程相关</h1><h2 id="6-1-指定进程使用的GPU"><a href="#6-1-指定进程使用的GPU" class="headerlink" title="6.1 指定进程使用的GPU"></a>6.1 指定进程使用的GPU</h2><p>在终端执行时指定：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0,1，当设为-1时代表不使用任何gpu</span><br></pre></td></tr></table></figure><p>在sh脚本文件中指定:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#export CUDA_VISIBLE_DEVICES&#x3D;0,1</span><br></pre></td></tr></table></figure><p>在python程序中指定：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import os; os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] &#x3D; &quot;0&quot;</span><br></pre></td></tr></table></figure><h2 id="6-2-后台运行不挂断"><a href="#6-2-后台运行不挂断" class="headerlink" title="6.2 后台运行不挂断"></a>6.2 后台运行不挂断</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python -u train.py &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>该命令会将输出重定向到nohup.log文件中。</p><h1 id="7-磁盘空间"><a href="#7-磁盘空间" class="headerlink" title="7 磁盘空间"></a>7 磁盘空间</h1><p>查看整个服务器的空间:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df -h</span><br></pre></td></tr></table></figure><p>查看当前目录下每一级目录的空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">du -h --max-depth&#x3D;1</span><br></pre></td></tr></table></figure><h1 id="8-文本内容抽取和对比"><a href="#8-文本内容抽取和对比" class="headerlink" title="8 文本内容抽取和对比"></a>8 文本内容抽取和对比</h1><h1 id="8-1-文本比对"><a href="#8-1-文本比对" class="headerlink" title="8.1 文本比对"></a>8.1 文本比对</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vimdiff file1 file2</span><br></pre></td></tr></table></figure><h2 id="8-2-内容抽取：cut"><a href="#8-2-内容抽取：cut" class="headerlink" title="8.2 内容抽取：cut"></a>8.2 内容抽取：cut</h2><p>对每行抽取需要的部分，常用参数如下：</p><ol><li><code>-d</code>：指定分隔符，默认为“TAB”</li><li><code>-f</code>：抽取指定部分</li><li><code>N-</code>：从第N个部分到结尾</li><li><code>N-M</code>：从第N个部分到第M个（包括M）部分</li><li><code>-M</code>：从第1个部分到第M个（包括M）部分</li></ol><p><strong>举例：</strong> 在文件ldc_test.result中抽取出以<code>-T</code>开头的句子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-S: 决议 要求 埃塞俄比亚 立即 采取 具体 步骤 , 使 厄 埃 边界 委员会 能 在 没有 先决条件 的 情况 下 迅速 标@@ 定 边界 ; 要求 厄立特里亚 不再 拖延 , 不 设 先决条件 地 取消 对 埃@@ 厄 特派 团 的 行动 和 作业 的 所有 限制 .</span><br><span class="line">-T: The resolution requires Ethiopia to immediately take concrete steps to allow the Erit@@ rea - Ethiopia Boundary Commission to speedily demarc@@ ate the border without any preconditions ; and requires Erit@@ rea to cancel all of its restrictions on UN@@ ME@@ E &#39;s actions and operations without any further delay and without setting any preconditions .</span><br><span class="line">-P: The resolution asked Ethiopia to take specific steps immediately to enable the Erit@@ rean border committee to rapidly set its boundary without a precondition ; Erit@@ rea would no longer delay or set a precedent for the removal of all restrictions on the actions and operations of the Erit@@ rean special missions .</span><br><span class="line"></span><br><span class="line">-S: 有关 部门 应 强化 低 保@@ 户 在 享受 低 保 时 须 履行 的 义务 : 如 及时 通报 家庭 人员 及 收入 变化 情况 , 汇报 就业 情况 , 接受 定期 复@@ 审 等 , 而 有关 部门 则 应 加大 监督 检查 的 力度 .</span><br><span class="line">-T: The relevant department should stress the obligations that welfare recipients must carry out while enjoying the welfare : for example , promptly notifying the changes in the family members and incomes , reporting the status of employment , accepting regular reviews , etc. On the other hand , the relevant department should step up monitoring and inspection .</span><br><span class="line">-P: The relevant departments should strengthen the obligation of low - bonded households to carry out such tasks as helping low - income families to enjoy low - income insured : if timely reporting of changes in the income and changes in the income and reporting on employment , and receiving regular reviews , the relevant departments should intensify supervision and inspection .</span><br></pre></td></tr></table></figure><p>使用命令：<code>grep ^-T ldc_test.result | cut -f2- -d&quot; &quot; &gt; new.result</code>。其中，<code>-d&quot; &quot;</code>将空格设为分隔符，<code>-f2-</code>在用空格分开后的部分中，从第二个部分起，抽取后面所有部分。效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The resolution requires Ethiopia to immediately take concrete steps to allow the Erit@@ rea - Ethiopia Boundary Commission to speedily demarc@@ ate the border without any preconditions ; and requires Erit@@ rea to cancel all of its restrictions on UN@@ ME@@ E &#39;s actions and operations without any further delay and without setting any preconditions .</span><br><span class="line">The relevant department should stress the obligations that welfare recipients must carry out while </span><br></pre></td></tr></table></figure><h1 id="9-统计目录下的文件数量"><a href="#9-统计目录下的文件数量" class="headerlink" title="9 统计目录下的文件数量"></a>9 统计目录下的文件数量</h1><p>1.统计当前目录下的一般文件的数量（不包括子文件夹，也不包括子文件夹内的文件）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -l |grep &quot;^-&quot;|wc -l</span><br></pre></td></tr></table></figure><p>2.统计当前目录下的子文件夹数量（不包括一般文件，也不包括子文件夹内的文件）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -l |grep &quot;^ｄ&quot;|wc -l</span><br></pre></td></tr></table></figure><p>3.统计当前目录下的一般文件数量（包括子文件夹内的一般文件，但不包括子文件夹，也不包括子文件夹内的子文件夹）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -lR|grep &quot;^-&quot;|wc -l </span><br></pre></td></tr></table></figure><p>解释：</p><p><code>ls-l</code>命令输出当前目录下的文件信息（包含目录、链接、设备文件等），每一行对应一个文件。</p><p><code>ls -lR</code>命令输出当前目录下的包括子目录的文件信息。</p><p><code>grep &quot;^-&quot;</code>过滤<code>ls</code>的输出信息，只保留一般文件。</p><p><code>grep &quot;^d&quot;</code>过滤<code>ls</code>的输出信息，只保留子文件夹。</p><p><code>wc -l</code>命令统计输出信息的行数。</p><blockquote><p>参考：</p><p><a href="https://hannlp.github.io/2021-01-15-Linux-&amp;-Shell-Usage-Record/">https://hannlp.github.io/2021-01-15-Linux-&amp;-Shell-Usage-Record/</a></p><p><a href="https://github.com/LIU-LIU-LIU/Recycle_bin">https://github.com/LIU-LIU-LIU/Recycle_bin</a></p><p><a href="https://www.cnblogs.com/dylancao/p/9012790.html">https://www.cnblogs.com/dylancao/p/9012790.html</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;linux服务器下的一些常用操作。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="other" scheme="http://example.com/categories/other/"/>
    
    <category term="linux" scheme="http://example.com/categories/other/linux/"/>
    
    
  </entry>
  
  <entry>
    <title>chinese_text_classify</title>
    <link href="http://example.com/2021/11/28/chinese-text-classify/"/>
    <id>http://example.com/2021/11/28/chinese-text-classify/</id>
    <published>2021-11-28T05:16:23.000Z</published>
    <updated>2021-12-03T15:30:34.915Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>中文文本分类流程</p></blockquote><span id="more"></span><h1 id="1-相关工具及目录结构"><a href="#1-相关工具及目录结构" class="headerlink" title="1 相关工具及目录结构"></a>1 相关工具及目录结构</h1><h2 id="1-1-相关工具"><a href="#1-1-相关工具" class="headerlink" title="1.1 相关工具"></a>1.1 相关工具</h2><p>除<strong>jieba</strong>是使用<code>pip install</code>安装外，其他几个工具都是建议直接克隆库到自己的用户目录中，方便使用其脚本(<strong>moses</strong>/<strong>subword-nmt</strong>)，或未来可能要自己拓展其中的模型(<strong>fairseq</strong>)</p><ol><li><p>jieba (中文分词组件)，安装指令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure></li></ol><h2 id="1-2-目录结构与初始化"><a href="#1-2-目录结构与初始化" class="headerlink" title="1.2 目录结构与初始化"></a>1.2 目录结构与初始化</h2><h3 id="1-2-1-目录结构"><a href="#1-2-1-目录结构" class="headerlink" title="1.2.1 目录结构"></a>1.2.1 目录结构</h3><p>提前组织一个目录结构的好处是可以让后面的一系列操作更加统一、规范化。下表中<code>~</code>代表linux系统中<strong>我的用户目录</strong>, THUCNews目录名代表此次我使用的数据集名称</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">~</span><br><span class="line">├── mosesdecoder</span><br><span class="line">└── text-classify</span><br><span class="line">    ├── master</span><br><span class="line">    ├── THUCNews</span><br><span class="line">    ├── split_data.py   # 用于划分train,valid,test</span><br><span class="line">        ├── data            # 用于存放数据</span><br><span class="line">            ├── train.txt          </span><br><span class="line">            ├── train_long.txt</span><br><span class="line">            ├── dev.txt  </span><br><span class="line">            ├── dev_long.txt </span><br><span class="line">            ├── test.txt </span><br><span class="line">            ├── test_long.txt </span><br><span class="line">            ├── class.txt  </span><br><span class="line">            ├── embedding_SougouNews.npz  </span><br><span class="line">            ├── embedding_Tencent.npz  </span><br><span class="line">            └── vocab.pkl</span><br><span class="line">            └── saved_dict      # 用于存放翻译结果</span><br><span class="line">    ├── models                  </span><br><span class="line">        ├── Bert.py  </span><br><span class="line">        ├── TextCNN.py  </span><br><span class="line">        ├── TextRNN_Att.py  </span><br><span class="line">        └── Transformer.py</span><br><span class="line">    ├── pytorch_pretrained_bert</span><br><span class="line">    ├── scripts                 # 一些脚本</span><br><span class="line">    ├── train_bert.sh  </span><br><span class="line">    ├── train_bilstm_att.sh  </span><br><span class="line">    ├── train_cnn.sh  </span><br><span class="line">    └── train_transformer.sh</span><br><span class="line">    ├── utils.py                # 一些其他工具</span><br><span class="line">    ├── utils_bert.py           </span><br><span class="line">    ├── run.py# 模型运行</span><br><span class="line">    ├── run_bert.py</span><br><span class="line">    ├── train_eval.py</span><br><span class="line">    └── train_eval_bert.py</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="2-数据的准备"><a href="#2-数据的准备" class="headerlink" title="2 数据的准备"></a>2 数据的准备</h1><h2 id="2-1-THUCNews"><a href="#2-1-THUCNews" class="headerlink" title="2.1 THUCNews"></a>2.1 THUCNews</h2><p>本文采用 <a href="http://thuctc.thunlp.org/">THUCNews中文文本开源语料</a> ，THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。共分为14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。</p><p><strong>原始数据统计：</strong></p><p>总数量：836075条（科技：162929；股票：154398；体育：131604；娱乐：92632；时政：63086；社会：50849；教育：41936；财经：37098；家居：32586；游戏：24373；房产：20050；时尚：13368；彩票：7588；星座：3578）</p><h2 id="2-2-数据预处理"><a href="#2-2-数据预处理" class="headerlink" title="2.2 数据预处理"></a>2.2 数据预处理</h2><h3 id="2-2-1-原始数据格式"><a href="#2-2-1-原始数据格式" class="headerlink" title="2.2.1 原始数据格式"></a>2.2.1 原始数据格式</h3><p>分类放置，每个文档包含一篇新闻文本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">└── THUCNews</span><br><span class="line">        └── 体育     </span><br><span class="line">            └── 29016.txt</span><br><span class="line">        └── 财经     </span><br><span class="line">            └── 11016.txt</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>文档格式如下：由标题和正文两部分组成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">组图：9月全球折扣明星带你去血拼</span><br><span class="line">　　导读：每年的8，9两月，都是普天下血拼狂人大开杀戒的日子。因为全球性的折扣季节已正式启动。以折扣率最疯狂的美国来说，无论一线大牌还是时尚潮物，统统为5 折为起点，一路狂飙到2折左右。去年8月，Sami就曾以0.5折的夸张扣率抢到一条MiuMiu连衣裙。此番Sami特别请到20位美国本土女星，她们让现身说法，带你掏遍折扣季节最值得入货的高性价比超值单品。</span><br><span class="line">　　Heidi Montag推荐超值折扣单品：GUCCI凉鞋</span><br><span class="line">　　Ashley Tisdale推荐超值折扣单品：破洞牛仔小脚裤</span><br></pre></td></tr></table></figure><h3 id="2-2-2-数据集生成"><a href="#2-2-2-数据集生成" class="headerlink" title="2.2.2 数据集生成"></a>2.2.2 数据集生成</h3><p>从中抽取10个类别（财经、房产、股票、教育、科技、社会、时政、体育、游戏、娱乐）随机抽取共20万条数据，其中训练集18万数据，验证集1万数据，测试集1万数据。</p><p>首先，从原始数据集中选择10个类别抽取部分数据，每个类别随机抽取18000条数据作为训练集，1000条数据作为验证集，1000条数据作为测试集，使用split_data.py划分：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python split_data.py</span><br></pre></td></tr></table></figure><p>使用shuffle.py打乱顺序：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python shuffle.py</span><br></pre></td></tr></table></figure><p>短文本（标题）效果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">中国人寿承诺不裁员不减薪        0</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>长文本（标题+正文）效果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">四新股冻结资金4112亿元 王亚伟青睐九安医疗\　　⊙见习记者方俊○编辑朱绍勇　　据今日公告披露，本周一发行的中小板新股兆驰股份、杭氧股份、九安医疗、棕榈园林合计冻结资金4112.5亿元，而整体来看打新资金对个股的选择出现了严重分化，其中兆驰股份和九安医疗的中签率分别为2.445%和0.487%，相差近5倍。　　公告显示，兆驰股份网上定价发行中签率为2.445%，仅次于此前科伦药业3.1%中签率，超额认购倍数为41倍，冻结资金549.68亿元，网下有效申购资金为53.37亿元。杭氧股份网上中签率为1.1%，超额认购倍数为90倍，冻结资金924.58亿元，网下有效申购资金为80.65亿元。棕榈园林网上中签率为0.88%，超额认购倍数为113倍，冻结资金1219.26亿元，网下有效申购资金为196.47亿元。　　九安医疗网上中签率为0.487%，超额认购倍数为205倍，冻结资金986.79亿元，网下有效申购资金为101.7亿元。而网下配售结果显示，九安医疗也受到了127家配售对象的青睐，其中包括王亚伟执掌的华夏大盘和华夏策略。0</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>最后，THUCNews/data目录中有如下数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── THUCNews</span><br><span class="line">    └── data</span><br><span class="line">        ├── train.txt</span><br><span class="line">        ├── train_long.txt</span><br><span class="line">        ├── dev.txt</span><br><span class="line">        ├── dev_long.txt</span><br><span class="line">        ├── test.txt</span><br><span class="line">        └── test_long.txt</span><br></pre></td></tr></table></figure><h1 id="3-训练过程"><a href="#3-训练过程" class="headerlink" title="3 训练过程"></a>3 训练过程</h1><h2 id="3-1-bert模型"><a href="#3-1-bert模型" class="headerlink" title="3.1 bert模型"></a>3.1 bert模型</h2><p>1.运行脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run_bert.py --model bert &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>2.运行log</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">180000it [00:35, 5049.92it&#x2F;s]</span><br><span class="line">10000it [00:02, 4529.50it&#x2F;s]</span><br><span class="line">10000it [00:02, 3977.26it&#x2F;s]</span><br><span class="line">Time usage: 0:00:40</span><br><span class="line">model to cpu</span><br><span class="line">model to train</span><br><span class="line">no error -1</span><br><span class="line">no error 0</span><br><span class="line">no error 1</span><br><span class="line">no error 2</span><br><span class="line">Epoch [1&#x2F;3]</span><br><span class="line">&#x2F;data&#x2F;yyfxu&#x2F;text-classify&#x2F;tmp&#x2F;pytorch_pretrained_bert&#x2F;optimization.py:275: UserWarning: This overload of add_ is deprecated:</span><br><span class="line">add_(Number alpha, Tensor other)</span><br><span class="line">Consider using one of the following signatures instead:</span><br><span class="line">add_(Tensor other, *, Number alpha) (Triggered internally at  &#x2F;opt&#x2F;conda&#x2F;conda-bld&#x2F;pytorch_1595629403081&#x2F;work&#x2F;torch&#x2F;csrc&#x2F;utils&#x2F;python_arg_parser.cpp:766.)</span><br><span class="line">  next_m.mul_(beta1).add_(1 - beta1, grad)</span><br><span class="line">Iter:      0,  Train Loss:   2.5,  Train Acc:  6.25%,  Val Loss:   2.3,  Val Acc: 10.13%,  Time: 0:00:24 *</span><br><span class="line">Iter:    100,  Train Loss:  0.88,  Train Acc: 75.00%,  Val Loss:  0.73,  Val Acc: 81.03%,  Time: 0:01:24 *</span><br><span class="line">Iter:    200,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:  0.41,  Val Acc: 87.58%,  Time: 0:02:18 *</span><br><span class="line">Iter:    300,  Train Loss:   0.7,  Train Acc: 81.25%,  Val Loss:   0.4,  Val Acc: 88.25%,  Time: 0:03:13 *</span><br><span class="line">Iter:    400,  Train Loss:   1.0,  Train Acc: 81.25%,  Val Loss:  0.38,  Val Acc: 88.62%,  Time: 0:04:07 *</span><br><span class="line">Iter:    500,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.39,  Val Acc: 88.75%,  Time: 0:05:02 </span><br><span class="line">Iter:    600,  Train Loss:  0.31,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 89.58%,  Time: 0:05:57 *</span><br><span class="line">Iter:    700,  Train Loss:  0.29,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.25%,  Time: 0:06:52 </span><br><span class="line">Iter:    800,  Train Loss:  0.35,  Train Acc: 93.75%,  Val Loss:  0.41,  Val Acc: 88.22%,  Time: 0:07:46 </span><br><span class="line">Iter:    900,  Train Loss: 0.036,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 89.76%,  Time: 0:08:42 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.57,  Train Acc: 87.50%,  Val Loss:  0.42,  Val Acc: 89.01%,  Time: 0:09:38 </span><br><span class="line">Iter:   1100,  Train Loss: 0.017,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 88.69%,  Time: 0:10:32 </span><br><span class="line">Iter:   1200,  Train Loss:  0.97,  Train Acc: 81.25%,  Val Loss:   0.4,  Val Acc: 88.91%,  Time: 0:11:27 </span><br><span class="line">Iter:   1300,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.38,  Val Acc: 88.72%,  Time: 0:12:19 </span><br><span class="line">Iter:   1400,  Train Loss:  0.69,  Train Acc: 81.25%,  Val Loss:  0.44,  Val Acc: 87.96%,  Time: 0:13:11 </span><br><span class="line">Iter:   1500,  Train Loss:  0.52,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 88.91%,  Time: 0:14:05 </span><br><span class="line">Iter:   1600,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:  0.52,  Val Acc: 86.66%,  Time: 0:14:57 </span><br><span class="line">Iter:   1700,  Train Loss:  0.12,  Train Acc: 93.75%,  Val Loss:  0.41,  Val Acc: 89.22%,  Time: 0:15:52 </span><br><span class="line">Iter:   1800,  Train Loss: 0.054,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 88.87%,  Time: 0:16:44 </span><br><span class="line">Iter:   1900,  Train Loss:  0.12,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 88.96%,  Time: 0:17:38 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.34,  Test Acc: 90.05%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.8957    0.8670    0.8811      1000</span><br><span class="line">       realty     0.9262    0.9290    0.9276      1000</span><br><span class="line">       stocks     0.8510    0.8170    0.8337      1000</span><br><span class="line">    education     0.9732    0.9430    0.9578      1000</span><br><span class="line">      science     0.7790    0.9130    0.8407      1000</span><br><span class="line">      society     0.9564    0.8550    0.9029      1000</span><br><span class="line">     politics     0.8578    0.9290    0.8920      1000</span><br><span class="line">       sports     0.9092    0.9710    0.9391      1000</span><br><span class="line">         game     0.9383    0.9130    0.9255      1000</span><br><span class="line">entertainment     0.9538    0.8680    0.9089      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9005     10000</span><br><span class="line">    macro avg     0.9041    0.9005    0.9009     10000</span><br><span class="line"> weighted avg     0.9041    0.9005    0.9009     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[867  13  84   2  19   1   9   5   0   0]</span><br><span class="line"> [ 17 929   6   0  16   4  12   7   5   4]</span><br><span class="line"> [ 40  29 817   0  74   0  33   4   3   0]</span><br><span class="line"> [  5   2   3 943   8   9  15   6   2   7]</span><br><span class="line"> [  3   3  17   2 913  10  18   4  24   6]</span><br><span class="line"> [ 18  16   0  12  41 855  41   6   2   9]</span><br><span class="line"> [  8   3  25   6  17   6 929   3   0   3]</span><br><span class="line"> [  3   2   3   2   8   0   4 971   2   5]</span><br><span class="line"> [  2   0   4   0  52   6   7   8 913   8]</span><br><span class="line"> [  5   6   1   2  24   3  15  54  22 868]]</span><br><span class="line">Time usage: 0:00:21</span><br><span class="line">model finish</span><br></pre></td></tr></table></figure><p>使用了预训练的bert模型，长文本超出长度限制（512），bert没有进行长文本实验。</p><h2 id="3-2-bilstm-attention模型"><a href="#3-2-bilstm-attention模型" class="headerlink" title="3.2 bilstm+attention模型"></a>3.2 bilstm+attention模型</h2><p>1.运行脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run.py --model TextRNN_Att &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>2.运行log</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:02, 70404.96it/s]</span><br><span class="line">10000it [00:00, 90716.68it/s]</span><br><span class="line">10000it [00:00, 92472.73it/s]</span><br><span class="line">Time usage: 0:00:03</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)</span><br><span class="line">  (tanh1): Tanh()</span><br><span class="line">  (tanh2): Tanh()</span><br><span class="line">  (fc1): Linear(in_features=256, out_features=64, bias=True)</span><br><span class="line">  (fc): Linear(in_features=64, out_features=10, bias=True)</span><br><span class="line"><span class="meta">)&gt;</span></span><br><span class="line"><span class="bash">start train</span></span><br><span class="line">Epoch [1/10]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc:  6.25%,  Val Loss:   2.3,  Val Acc: 10.01%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:  0.75,  Train Acc: 75.00%,  Val Loss:  0.78,  Val Acc: 73.34%,  Time: 0:00:05 *</span><br><span class="line">Iter:    200,  Train Loss:  0.76,  Train Acc: 72.66%,  Val Loss:  0.58,  Val Acc: 81.38%,  Time: 0:00:07 *</span><br><span class="line">Iter:    300,  Train Loss:   0.4,  Train Acc: 88.28%,  Val Loss:  0.53,  Val Acc: 83.07%,  Time: 0:00:10 *</span><br><span class="line">Iter:    400,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.47,  Val Acc: 84.44%,  Time: 0:00:14 *</span><br><span class="line">Iter:    500,  Train Loss:  0.44,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 85.70%,  Time: 0:00:18 *</span><br><span class="line">Iter:    600,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 86.46%,  Time: 0:00:22 *</span><br><span class="line">Iter:    700,  Train Loss:  0.41,  Train Acc: 85.94%,  Val Loss:  0.41,  Val Acc: 86.79%,  Time: 0:00:25 *</span><br><span class="line">Iter:    800,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 87.86%,  Time: 0:00:29 *</span><br><span class="line">Iter:    900,  Train Loss:  0.44,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 87.53%,  Time: 0:00:33 </span><br><span class="line">Iter:   1000,  Train Loss:  0.26,  Train Acc: 90.62%,  Val Loss:  0.37,  Val Acc: 88.04%,  Time: 0:00:36 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.29,  Train Acc: 92.19%,  Val Loss:  0.39,  Val Acc: 86.97%,  Time: 0:00:40 </span><br><span class="line">Iter:   1200,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.41%,  Time: 0:00:43 *</span><br><span class="line">Iter:   1300,  Train Loss:  0.34,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.54%,  Time: 0:00:47 </span><br><span class="line">Iter:   1400,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.44%,  Time: 0:00:50 </span><br><span class="line">Epoch [2/10]</span><br><span class="line">Iter:   1500,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 88.15%,  Time: 0:00:54 </span><br><span class="line">Iter:   1600,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 88.53%,  Time: 0:00:57 </span><br><span class="line">Iter:   1700,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 88.63%,  Time: 0:01:00 *</span><br><span class="line">Iter:   1800,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.31%,  Time: 0:01:03 *</span><br><span class="line">Iter:   1900,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.55%,  Time: 0:01:07 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 89.12%,  Time: 0:01:10 </span><br><span class="line">Iter:   2100,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 89.42%,  Time: 0:01:14 </span><br><span class="line">Iter:   2200,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 89.52%,  Time: 0:01:17 </span><br><span class="line">Iter:   2300,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.31%,  Time: 0:01:21 *</span><br><span class="line">Iter:   2400,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 88.97%,  Time: 0:01:24 </span><br><span class="line">Iter:   2500,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.31,  Val Acc: 89.67%,  Time: 0:01:28 *</span><br><span class="line">Iter:   2600,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.74%,  Time: 0:01:31 </span><br><span class="line">Iter:   2700,  Train Loss:  0.25,  Train Acc: 93.75%,  Val Loss:  0.31,  Val Acc: 89.83%,  Time: 0:01:35 *</span><br><span class="line">Iter:   2800,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 89.59%,  Time: 0:01:39 </span><br><span class="line">Epoch [3/10]</span><br><span class="line">Iter:   2900,  Train Loss:  0.32,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.21%,  Time: 0:01:42 </span><br><span class="line">Iter:   3000,  Train Loss:  0.21,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 89.80%,  Time: 0:01:45 </span><br><span class="line">Iter:   3100,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.34,  Val Acc: 89.29%,  Time: 0:01:50 </span><br><span class="line">Iter:   3200,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.51%,  Time: 0:01:53 </span><br><span class="line">Iter:   3300,  Train Loss:  0.31,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.85%,  Time: 0:01:57 </span><br><span class="line">Iter:   3400,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.31,  Val Acc: 89.98%,  Time: 0:02:01 </span><br><span class="line">Iter:   3500,  Train Loss:  0.12,  Train Acc: 95.31%,  Val Loss:  0.34,  Val Acc: 89.42%,  Time: 0:02:04 </span><br><span class="line">Iter:   3600,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 90.14%,  Time: 0:02:08 </span><br><span class="line">Iter:   3700,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.31,  Val Acc: 89.92%,  Time: 0:02:11 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:   0.3,  Test Acc: 89.75%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9120    0.8600    0.8852      1000</span><br><span class="line">       realty     0.9037    0.9200    0.9118      1000</span><br><span class="line">       stocks     0.8652    0.7960    0.8292      1000</span><br><span class="line">    education     0.9660    0.9080    0.9361      1000</span><br><span class="line">      science     0.7947    0.8750    0.8329      1000</span><br><span class="line">      society     0.8610    0.9290    0.8937      1000</span><br><span class="line">     politics     0.8918    0.8740    0.8828      1000</span><br><span class="line">       sports     0.9777    0.9660    0.9718      1000</span><br><span class="line">         game     0.9430    0.8930    0.9173      1000</span><br><span class="line">entertainment     0.8801    0.9540    0.9155      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.8975     10000</span><br><span class="line">    macro avg     0.8995    0.8975    0.8976     10000</span><br><span class="line"> weighted avg     0.8995    0.8975    0.8976     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[860  24  59   4  20  13  11   3   1   5]</span><br><span class="line"> [  8 920  16   0  11  15   7   4   4  15]</span><br><span class="line"> [ 53  29 796   1  77   3  32   1   6   2]</span><br><span class="line"> [  2   4   2 908  10  43  13   1   3  14]</span><br><span class="line"> [ 11  10  12   5 875  15  20   3  28  21]</span><br><span class="line"> [  0  15   1  10   8 929  16   1   3  17]</span><br><span class="line"> [  5   6  26   8  19  40 874   1   2  19]</span><br><span class="line"> [  1   1   2   0   1   5   3 966   0  21]</span><br><span class="line"> [  1   4   5   1  69   6   3   2 893  16]</span><br><span class="line"> [  2   5   1   3  11  10   1   6   7 954]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>长文本（标题+正文）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14699.32it&#x2F;s]</span><br><span class="line">10000it [00:00, 12818.24it&#x2F;s]</span><br><span class="line">10000it [00:00, 16106.52it&#x2F;s]</span><br><span class="line">Time usage: 0:00:14</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">(base) yyfxu@cip57:~&#x2F;text-classify&#x2F;master$ vim nohup_bi_long.log </span><br><span class="line">(base) yyfxu@cip57:~&#x2F;text-classify&#x2F;master$ head -n 20 nohup_bi_long.log </span><br><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14699.32it&#x2F;s]</span><br><span class="line">10000it [00:00, 12818.24it&#x2F;s]</span><br><span class="line">10000it [00:00, 16106.52it&#x2F;s]</span><br><span class="line">Time usage: 0:00:14</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (lstm): LSTM(300, 128, num_layers&#x3D;2, batch_first&#x3D;True, dropout&#x3D;0.5, bidirectional&#x3D;True)</span><br><span class="line">  (tanh1): Tanh()</span><br><span class="line">  (tanh2): Tanh()</span><br><span class="line">  (fc1): Linear(in_features&#x3D;256, out_features&#x3D;64, bias&#x3D;True)</span><br><span class="line">  (fc): Linear(in_features&#x3D;64, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;10]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc: 18.75%,  Val Loss:   2.3,  Val Acc: 10.03%,  Time: 0:00:00 *</span><br><span class="line">Iter:    100,  Train Loss:  0.55,  Train Acc: 85.16%,  Val Loss:  0.55,  Val Acc: 82.30%,  Time: 0:00:02 *</span><br><span class="line">Iter:    200,  Train Loss:  0.41,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 85.91%,  Time: 0:00:04 *</span><br><span class="line">Iter:    300,  Train Loss:  0.57,  Train Acc: 82.03%,  Val Loss:  0.41,  Val Acc: 86.64%,  Time: 0:00:06 *</span><br><span class="line">Iter:    400,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.37,  Val Acc: 87.85%,  Time: 0:00:07 *</span><br><span class="line">Iter:    500,  Train Loss:   0.3,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.41%,  Time: 0:00:09 *</span><br><span class="line">Iter:    600,  Train Loss:  0.34,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.66%,  Time: 0:00:11 *</span><br><span class="line">Iter:    700,  Train Loss:  0.21,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 89.61%,  Time: 0:00:12</span><br><span class="line">Iter:    800,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:   0.3,  Val Acc: 90.31%,  Time: 0:00:14 *</span><br><span class="line">Iter:    900,  Train Loss:  0.55,  Train Acc: 85.94%,  Val Loss:  0.28,  Val Acc: 91.13%,  Time: 0:00:15 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.29,  Train Acc: 88.28%,  Val Loss:  0.28,  Val Acc: 91.06%,  Time: 0:00:17 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.70%,  Time: 0:00:19 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.31,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.70%,  Time: 0:00:21 *</span><br><span class="line">Iter:   1300,  Train Loss:  0.24,  Train Acc: 93.75%,  Val Loss:  0.28,  Val Acc: 91.10%,  Time: 0:00:22</span><br><span class="line">Iter:   1400,  Train Loss:  0.27,  Train Acc: 89.84%,  Val Loss:  0.25,  Val Acc: 91.79%,  Time: 0:00:24 *</span><br><span class="line">Epoch [2&#x2F;10]</span><br><span class="line">Iter:   1500,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.28,  Val Acc: 90.45%,  Time: 0:00:26</span><br><span class="line">Iter:   1600,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.70%,  Time: 0:00:27</span><br><span class="line">Iter:   1700,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.27,  Val Acc: 91.42%,  Time: 0:00:29</span><br><span class="line">Iter:   1800,  Train Loss:  0.14,  Train Acc: 95.31%,  Val Loss:  0.26,  Val Acc: 91.92%,  Time: 0:00:31</span><br><span class="line">Iter:   1900,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.33%,  Time: 0:00:32 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.24,  Train Acc: 89.06%,  Val Loss:  0.24,  Val Acc: 92.20%,  Time: 0:00:34</span><br><span class="line">Iter:   2100,  Train Loss:  0.27,  Train Acc: 92.19%,  Val Loss:  0.25,  Val Acc: 91.82%,  Time: 0:00:36</span><br><span class="line">Iter:   2200,  Train Loss:   0.2,  Train Acc: 92.19%,  Val Loss:  0.25,  Val Acc: 92.04%,  Time: 0:00:38</span><br><span class="line">Iter:   2300,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.06%,  Time: 0:00:42</span><br><span class="line">Iter:   2400,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.24,  Val Acc: 92.40%,  Time: 0:00:45 *</span><br><span class="line">Iter:   2500,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.22,  Val Acc: 92.59%,  Time: 0:00:48 *</span><br><span class="line">Iter:   2600,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.22,  Val Acc: 92.81%,  Time: 0:00:51 *</span><br><span class="line">Iter:   2700,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.24,  Val Acc: 92.30%,  Time: 0:00:54</span><br><span class="line">Iter:   2800,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 92.73%,  Time: 0:00:57</span><br><span class="line">Epoch [3&#x2F;10]</span><br><span class="line">Iter:   2900,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.26,  Val Acc: 91.74%,  Time: 0:01:01</span><br><span class="line">Iter:   3000,  Train Loss:  0.22,  Train Acc: 94.53%,  Val Loss:  0.23,  Val Acc: 92.44%,  Time: 0:01:04</span><br><span class="line">Iter:   3100,  Train Loss:  0.18,  Train Acc: 94.53%,  Val Loss:  0.23,  Val Acc: 92.74%,  Time: 0:01:07</span><br><span class="line">Iter:   3200,  Train Loss:   0.1,  Train Acc: 96.88%,  Val Loss:  0.22,  Val Acc: 92.83%,  Time: 0:01:11</span><br><span class="line">Iter:   3300,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 92.88%,  Time: 0:01:14</span><br><span class="line">Iter:   3400,  Train Loss:  0.19,  Train Acc: 95.31%,  Val Loss:  0.24,  Val Acc: 92.12%,  Time: 0:01:17</span><br><span class="line">Iter:   3500,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.23,  Val Acc: 92.45%,  Time: 0:01:21</span><br><span class="line">Iter:   3600,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.20%,  Time: 0:01:24</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.22,  Test Acc: 92.82%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9044    0.9080    0.9062      1000</span><br><span class="line">       realty     0.9308    0.9410    0.9359      1000</span><br><span class="line">       stocks     0.8939    0.8850    0.8894      1000</span><br><span class="line">    education     0.9541    0.9560    0.9550      1000</span><br><span class="line">      science     0.9121    0.8610    0.8858      1000</span><br><span class="line">      society     0.8962    0.9240    0.9099      1000</span><br><span class="line">     politics     0.9086    0.9340    0.9211      1000</span><br><span class="line">       sports     0.9869    0.9830    0.9850      1000</span><br><span class="line">         game     0.9336    0.9560    0.9447      1000</span><br><span class="line">entertainment     0.9629    0.9340    0.9482      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9282     10000</span><br><span class="line">    macro avg     0.9283    0.9282    0.9281     10000</span><br><span class="line"> weighted avg     0.9283    0.9282    0.9281     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[908  17  49   0   2  13   7   1   1   2]</span><br><span class="line"> [  7 941  14   4   8  14   6   0   3   3]</span><br><span class="line"> [ 51  17 885   3  18   3  17   0   5   1]</span><br><span class="line"> [  4   4   2 956   2  17   8   0   1   6]</span><br><span class="line"> [ 13   8  23   5 861  23  22   1  40   4]</span><br><span class="line"> [  5   9   3  15   6 924  27   1   2   8]</span><br><span class="line"> [  6   7   6  14   7  18 934   0   2   6]</span><br><span class="line"> [  3   1   1   0   1   3   2 983   1   5]</span><br><span class="line"> [  3   2   4   0  30   2   2   0 956   1]</span><br><span class="line"> [  4   5   3   5   9  14   3  10  13 934]]</span><br><span class="line">Time usage: 0:00:01</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure><h2 id="3-3-transformer模型（encoder）"><a href="#3-3-transformer模型（encoder）" class="headerlink" title="3.3 transformer模型（encoder）"></a>3.3 transformer模型（encoder）</h2><p>1.运行脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run.py --model Transformer &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>2.运行log</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:02, 71756.33it&#x2F;s]</span><br><span class="line">10000it [00:00, 91902.35it&#x2F;s]</span><br><span class="line">10000it [00:00, 89357.25it&#x2F;s]</span><br><span class="line">Time usage: 0:00:03</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (postion_embedding): Positional_Encoding(</span><br><span class="line">    (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  )</span><br><span class="line">  (encoder): Encoder(</span><br><span class="line">    (attention): Multi_Head_Attention(</span><br><span class="line">      (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">      (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">    (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">      (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">      (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (encoders): ModuleList(</span><br><span class="line">    (0): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (fc1): Linear(in_features&#x3D;9600, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.4,  Train Acc: 10.16%,  Val Loss:   4.6,  Val Acc: 10.02%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:   1.4,  Train Acc: 53.12%,  Val Loss:   1.4,  Val Acc: 57.23%,  Time: 0:00:03 *</span><br><span class="line">Iter:    200,  Train Loss:   1.3,  Train Acc: 55.47%,  Val Loss:   1.0,  Val Acc: 68.83%,  Time: 0:00:05 *</span><br><span class="line">Iter:    300,  Train Loss:  0.82,  Train Acc: 67.97%,  Val Loss:   0.9,  Val Acc: 74.19%,  Time: 0:00:07 *</span><br><span class="line">Iter:    400,  Train Loss:  0.84,  Train Acc: 75.00%,  Val Loss:  0.83,  Val Acc: 76.70%,  Time: 0:00:09 *</span><br><span class="line">Iter:    500,  Train Loss:  0.65,  Train Acc: 78.12%,  Val Loss:  0.76,  Val Acc: 78.63%,  Time: 0:00:12 *</span><br><span class="line">Iter:    600,  Train Loss:  0.76,  Train Acc: 75.78%,  Val Loss:  0.76,  Val Acc: 79.15%,  Time: 0:00:15 *</span><br><span class="line">Iter:    700,  Train Loss:  0.63,  Train Acc: 78.12%,  Val Loss:  0.78,  Val Acc: 79.21%,  Time: 0:00:17 </span><br><span class="line">Iter:    800,  Train Loss:  0.59,  Train Acc: 81.25%,  Val Loss:  0.63,  Val Acc: 82.16%,  Time: 0:00:19 *</span><br><span class="line">Iter:    900,  Train Loss:   0.7,  Train Acc: 76.56%,  Val Loss:  0.69,  Val Acc: 80.84%,  Time: 0:00:21 </span><br><span class="line">Iter:   1000,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.69,  Val Acc: 80.87%,  Time: 0:00:23 </span><br><span class="line">Iter:   1100,  Train Loss:  0.52,  Train Acc: 83.59%,  Val Loss:  0.62,  Val Acc: 82.61%,  Time: 0:00:26 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.63,  Train Acc: 82.81%,  Val Loss:  0.63,  Val Acc: 82.48%,  Time: 0:00:28 </span><br><span class="line">Iter:   1300,  Train Loss:  0.59,  Train Acc: 80.47%,  Val Loss:  0.55,  Val Acc: 85.06%,  Time: 0:00:32 *</span><br><span class="line">Iter:   1400,  Train Loss:  0.76,  Train Acc: 74.22%,  Val Loss:  0.56,  Val Acc: 84.04%,  Time: 0:00:35 </span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:  0.63,  Train Acc: 79.69%,  Val Loss:  0.57,  Val Acc: 84.12%,  Time: 0:00:38 </span><br><span class="line">Iter:   1600,  Train Loss:  0.52,  Train Acc: 78.91%,  Val Loss:  0.65,  Val Acc: 82.71%,  Time: 0:00:41 </span><br><span class="line">Iter:   1700,  Train Loss:  0.59,  Train Acc: 82.81%,  Val Loss:  0.58,  Val Acc: 83.98%,  Time: 0:00:44 </span><br><span class="line">Iter:   1800,  Train Loss:  0.46,  Train Acc: 87.50%,  Val Loss:  0.54,  Val Acc: 85.62%,  Time: 0:00:47 *</span><br><span class="line">Iter:   1900,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.53,  Val Acc: 85.76%,  Time: 0:00:50 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.58,  Train Acc: 82.81%,  Val Loss:  0.58,  Val Acc: 84.32%,  Time: 0:00:53 </span><br><span class="line">Iter:   2100,  Train Loss:  0.56,  Train Acc: 86.72%,  Val Loss:  0.55,  Val Acc: 84.90%,  Time: 0:00:56 </span><br><span class="line">Iter:   2200,  Train Loss:  0.36,  Train Acc: 89.84%,  Val Loss:  0.52,  Val Acc: 85.77%,  Time: 0:01:00 *</span><br><span class="line">Iter:   2300,  Train Loss:  0.43,  Train Acc: 86.72%,  Val Loss:  0.55,  Val Acc: 84.77%,  Time: 0:01:03 </span><br><span class="line">Iter:   2400,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.56,  Val Acc: 85.17%,  Time: 0:01:06 </span><br><span class="line">Iter:   2500,  Train Loss:   0.4,  Train Acc: 85.94%,  Val Loss:  0.56,  Val Acc: 84.99%,  Time: 0:01:10 </span><br><span class="line">Iter:   2600,  Train Loss:  0.57,  Train Acc: 79.69%,  Val Loss:  0.55,  Val Acc: 84.93%,  Time: 0:01:13 </span><br><span class="line">Iter:   2700,  Train Loss:  0.44,  Train Acc: 87.50%,  Val Loss:  0.48,  Val Acc: 86.88%,  Time: 0:01:16 *</span><br><span class="line">Iter:   2800,  Train Loss:  0.57,  Train Acc: 82.81%,  Val Loss:   0.5,  Val Acc: 86.41%,  Time: 0:01:20 </span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.59,  Train Acc: 79.69%,  Val Loss:  0.59,  Val Acc: 84.07%,  Time: 0:01:23 </span><br><span class="line">Iter:   3000,  Train Loss:  0.47,  Train Acc: 86.72%,  Val Loss:  0.52,  Val Acc: 86.12%,  Time: 0:01:27 </span><br><span class="line">Iter:   3100,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.48,  Val Acc: 86.31%,  Time: 0:01:29 </span><br><span class="line">Iter:   3200,  Train Loss:  0.71,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 86.09%,  Time: 0:01:33 </span><br><span class="line">Iter:   3300,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.48,  Val Acc: 87.24%,  Time: 0:01:36 *</span><br><span class="line">Iter:   3400,  Train Loss:  0.51,  Train Acc: 82.03%,  Val Loss:  0.47,  Val Acc: 87.28%,  Time: 0:01:39 *</span><br><span class="line">Iter:   3500,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.54,  Val Acc: 86.01%,  Time: 0:01:42 </span><br><span class="line">Iter:   3600,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.49,  Val Acc: 87.23%,  Time: 0:01:46 </span><br><span class="line">Iter:   3700,  Train Loss:  0.59,  Train Acc: 80.47%,  Val Loss:  0.48,  Val Acc: 86.53%,  Time: 0:01:49 </span><br><span class="line">Iter:   3800,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.52,  Val Acc: 86.27%,  Time: 0:01:52 </span><br><span class="line">Iter:   3900,  Train Loss:  0.51,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 86.02%,  Time: 0:01:56 </span><br><span class="line">Iter:   4000,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.48,  Val Acc: 87.14%,  Time: 0:02:01 </span><br><span class="line">Iter:   4100,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.48,  Val Acc: 87.02%,  Time: 0:02:04 </span><br><span class="line">Iter:   4200,  Train Loss:  0.52,  Train Acc: 84.38%,  Val Loss:  0.49,  Val Acc: 86.53%,  Time: 0:02:08 </span><br><span class="line">Epoch [4&#x2F;20]</span><br><span class="line">Iter:   4300,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 87.06%,  Time: 0:02:11 </span><br><span class="line">Iter:   4400,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.47,  Val Acc: 87.44%,  Time: 0:02:14 *</span><br><span class="line">Iter:   4500,  Train Loss:  0.47,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 87.48%,  Time: 0:02:17 *</span><br><span class="line">Iter:   4600,  Train Loss:  0.38,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 87.91%,  Time: 0:02:21 *</span><br><span class="line">Iter:   4700,  Train Loss:  0.52,  Train Acc: 82.81%,  Val Loss:  0.45,  Val Acc: 87.24%,  Time: 0:02:25 </span><br><span class="line">Iter:   4800,  Train Loss:  0.33,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 87.63%,  Time: 0:02:28 </span><br><span class="line">Iter:   4900,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 87.85%,  Time: 0:02:30 </span><br><span class="line">Iter:   5000,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 87.79%,  Time: 0:02:33 </span><br><span class="line">Iter:   5100,  Train Loss:  0.53,  Train Acc: 80.47%,  Val Loss:  0.45,  Val Acc: 87.48%,  Time: 0:02:36 </span><br><span class="line">Iter:   5200,  Train Loss:  0.58,  Train Acc: 85.16%,  Val Loss:  0.47,  Val Acc: 87.18%,  Time: 0:02:40 </span><br><span class="line">Iter:   5300,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.48,  Val Acc: 86.80%,  Time: 0:02:43 </span><br><span class="line">Iter:   5400,  Train Loss:  0.67,  Train Acc: 82.03%,  Val Loss:  0.45,  Val Acc: 87.63%,  Time: 0:02:46 </span><br><span class="line">Iter:   5500,  Train Loss:  0.42,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 87.70%,  Time: 0:02:48 *</span><br><span class="line">Iter:   5600,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.02%,  Time: 0:02:51 *</span><br><span class="line">Epoch [5&#x2F;20]</span><br><span class="line">Iter:   5700,  Train Loss:  0.41,  Train Acc: 86.72%,  Val Loss:  0.42,  Val Acc: 88.50%,  Time: 0:02:54 *</span><br><span class="line">Iter:   5800,  Train Loss:  0.22,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.11%,  Time: 0:02:57 </span><br><span class="line">Iter:   5900,  Train Loss:  0.39,  Train Acc: 91.41%,  Val Loss:  0.44,  Val Acc: 88.15%,  Time: 0:03:00 </span><br><span class="line">Iter:   6000,  Train Loss:  0.44,  Train Acc: 83.59%,  Val Loss:  0.41,  Val Acc: 88.30%,  Time: 0:03:03 *</span><br><span class="line">Iter:   6100,  Train Loss:  0.44,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 88.09%,  Time: 0:03:07 </span><br><span class="line">Iter:   6200,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.42,  Val Acc: 88.42%,  Time: 0:03:09 </span><br><span class="line">Iter:   6300,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.43,  Val Acc: 88.17%,  Time: 0:03:12 </span><br><span class="line">Iter:   6400,  Train Loss:  0.21,  Train Acc: 94.53%,  Val Loss:  0.41,  Val Acc: 88.25%,  Time: 0:03:15 *</span><br><span class="line">Iter:   6500,  Train Loss:  0.44,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 88.49%,  Time: 0:03:19 </span><br><span class="line">Iter:   6600,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.01%,  Time: 0:03:22 </span><br><span class="line">Iter:   6700,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.41,  Val Acc: 88.09%,  Time: 0:03:25 </span><br><span class="line">Iter:   6800,  Train Loss:  0.35,  Train Acc: 89.84%,  Val Loss:  0.45,  Val Acc: 88.01%,  Time: 0:03:28 </span><br><span class="line">Iter:   6900,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.41,  Val Acc: 88.16%,  Time: 0:03:32 </span><br><span class="line">Iter:   7000,  Train Loss:  0.41,  Train Acc: 84.38%,  Val Loss:   0.4,  Val Acc: 88.43%,  Time: 0:03:36 *</span><br><span class="line">Epoch [6&#x2F;20]</span><br><span class="line">Iter:   7100,  Train Loss:  0.41,  Train Acc: 83.59%,  Val Loss:   0.4,  Val Acc: 88.95%,  Time: 0:03:39 *</span><br><span class="line">Iter:   7200,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 88.71%,  Time: 0:03:41 </span><br><span class="line">Iter:   7300,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.42,  Val Acc: 88.67%,  Time: 0:03:44 </span><br><span class="line">Iter:   7400,  Train Loss:   0.6,  Train Acc: 77.34%,  Val Loss:  0.43,  Val Acc: 87.97%,  Time: 0:03:47 </span><br><span class="line">Iter:   7500,  Train Loss:  0.34,  Train Acc: 84.38%,  Val Loss:  0.42,  Val Acc: 88.02%,  Time: 0:03:50 </span><br><span class="line">Iter:   7600,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 87.11%,  Time: 0:03:54 </span><br><span class="line">Iter:   7700,  Train Loss:   0.4,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.16%,  Time: 0:03:57 *</span><br><span class="line">Iter:   7800,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 88.64%,  Time: 0:04:00 </span><br><span class="line">Iter:   7900,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 88.47%,  Time: 0:04:03 </span><br><span class="line">Iter:   8000,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 89.05%,  Time: 0:04:06 </span><br><span class="line">Iter:   8100,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.42,  Val Acc: 88.54%,  Time: 0:04:09 </span><br><span class="line">Iter:   8200,  Train Loss:  0.37,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.00%,  Time: 0:04:13 </span><br><span class="line">Iter:   8300,  Train Loss:  0.33,  Train Acc: 89.84%,  Val Loss:  0.41,  Val Acc: 87.85%,  Time: 0:04:16 </span><br><span class="line">Iter:   8400,  Train Loss:  0.53,  Train Acc: 78.91%,  Val Loss:  0.37,  Val Acc: 88.97%,  Time: 0:04:20 *</span><br><span class="line">Epoch [7&#x2F;20]</span><br><span class="line">Iter:   8500,  Train Loss:   0.5,  Train Acc: 85.16%,  Val Loss:  0.43,  Val Acc: 88.20%,  Time: 0:04:22 </span><br><span class="line">Iter:   8600,  Train Loss:  0.25,  Train Acc: 92.19%,  Val Loss:  0.42,  Val Acc: 88.93%,  Time: 0:04:26 </span><br><span class="line">Iter:   8700,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.48%,  Time: 0:04:29 </span><br><span class="line">Iter:   8800,  Train Loss:  0.44,  Train Acc: 82.81%,  Val Loss:  0.39,  Val Acc: 88.89%,  Time: 0:04:33 </span><br><span class="line">Iter:   8900,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 89.18%,  Time: 0:04:36 </span><br><span class="line">Iter:   9000,  Train Loss:  0.23,  Train Acc: 91.41%,  Val Loss:  0.39,  Val Acc: 89.37%,  Time: 0:04:39 </span><br><span class="line">Iter:   9100,  Train Loss:   0.5,  Train Acc: 84.38%,  Val Loss:  0.39,  Val Acc: 89.34%,  Time: 0:04:43 </span><br><span class="line">Iter:   9200,  Train Loss:   0.4,  Train Acc: 90.62%,  Val Loss:   0.4,  Val Acc: 88.81%,  Time: 0:04:46 </span><br><span class="line">Iter:   9300,  Train Loss:  0.41,  Train Acc: 85.16%,  Val Loss:  0.39,  Val Acc: 89.10%,  Time: 0:04:50 </span><br><span class="line">Iter:   9400,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.43,  Val Acc: 88.22%,  Time: 0:04:54 </span><br><span class="line">Iter:   9500,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:   0.4,  Val Acc: 89.02%,  Time: 0:04:57 </span><br><span class="line">Iter:   9600,  Train Loss:  0.47,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.25%,  Time: 0:05:01 </span><br><span class="line">Iter:   9700,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 88.91%,  Time: 0:05:04 </span><br><span class="line">Iter:   9800,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:  0.39,  Val Acc: 89.03%,  Time: 0:05:07 </span><br><span class="line">Epoch [8&#x2F;20]</span><br><span class="line">Iter:   9900,  Train Loss:  0.51,  Train Acc: 78.91%,  Val Loss:  0.39,  Val Acc: 89.19%,  Time: 0:05:11 </span><br><span class="line">Iter:  10000,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:   0.4,  Val Acc: 88.75%,  Time: 0:05:14 </span><br><span class="line">Iter:  10100,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.38,  Val Acc: 89.28%,  Time: 0:05:17 </span><br><span class="line">Iter:  10200,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:05:20 </span><br><span class="line">Iter:  10300,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 89.62%,  Time: 0:05:24 *</span><br><span class="line">Iter:  10400,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.22%,  Time: 0:05:27 </span><br><span class="line">Iter:  10500,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.41,  Val Acc: 88.86%,  Time: 0:05:31 </span><br><span class="line">Iter:  10600,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:   0.4,  Val Acc: 89.12%,  Time: 0:05:34 </span><br><span class="line">Iter:  10700,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 88.98%,  Time: 0:05:37 </span><br><span class="line">Iter:  10800,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.39,  Val Acc: 88.85%,  Time: 0:05:41 </span><br><span class="line">Iter:  10900,  Train Loss:  0.42,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 89.07%,  Time: 0:05:45 </span><br><span class="line">Iter:  11000,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.04%,  Time: 0:05:48 </span><br><span class="line">Iter:  11100,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 88.94%,  Time: 0:05:51 </span><br><span class="line">Iter:  11200,  Train Loss:  0.44,  Train Acc: 82.81%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:05:54 </span><br><span class="line">Epoch [9&#x2F;20]</span><br><span class="line">Iter:  11300,  Train Loss:  0.33,  Train Acc: 86.72%,  Val Loss:  0.38,  Val Acc: 89.51%,  Time: 0:05:57 </span><br><span class="line">Iter:  11400,  Train Loss:  0.39,  Train Acc: 85.16%,  Val Loss:   0.4,  Val Acc: 88.89%,  Time: 0:06:01 </span><br><span class="line">Iter:  11500,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:   0.4,  Val Acc: 89.20%,  Time: 0:06:04 </span><br><span class="line">Iter:  11600,  Train Loss:  0.19,  Train Acc: 95.31%,  Val Loss:  0.37,  Val Acc: 89.70%,  Time: 0:06:06 </span><br><span class="line">Iter:  11700,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.89%,  Time: 0:06:11 *</span><br><span class="line">Iter:  11800,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:06:14 </span><br><span class="line">Iter:  11900,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.39,  Val Acc: 89.47%,  Time: 0:06:17 </span><br><span class="line">Iter:  12000,  Train Loss:  0.48,  Train Acc: 86.72%,  Val Loss:  0.39,  Val Acc: 89.43%,  Time: 0:06:20 </span><br><span class="line">Iter:  12100,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.36,  Val Acc: 89.82%,  Time: 0:06:27 *</span><br><span class="line">Iter:  12200,  Train Loss:   0.3,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.53%,  Time: 0:06:30 </span><br><span class="line">Iter:  12300,  Train Loss:   0.3,  Train Acc: 92.97%,  Val Loss:  0.39,  Val Acc: 88.93%,  Time: 0:06:33 </span><br><span class="line">Iter:  12400,  Train Loss:  0.23,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.35%,  Time: 0:06:36 </span><br><span class="line">Iter:  12500,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 89.21%,  Time: 0:06:39 </span><br><span class="line">Iter:  12600,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.75%,  Time: 0:06:44 *</span><br><span class="line">Epoch [10&#x2F;20]</span><br><span class="line">Iter:  12700,  Train Loss:  0.27,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.64%,  Time: 0:06:48 *</span><br><span class="line">Iter:  12800,  Train Loss:   0.2,  Train Acc: 91.41%,  Val Loss:  0.37,  Val Acc: 89.23%,  Time: 0:06:51 </span><br><span class="line">Iter:  12900,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 89.47%,  Time: 0:06:54 </span><br><span class="line">Iter:  13000,  Train Loss:  0.38,  Train Acc: 91.41%,  Val Loss:   0.4,  Val Acc: 89.49%,  Time: 0:06:58 </span><br><span class="line">Iter:  13100,  Train Loss:  0.41,  Train Acc: 88.28%,  Val Loss:  0.37,  Val Acc: 89.39%,  Time: 0:07:01 </span><br><span class="line">Iter:  13200,  Train Loss:  0.55,  Train Acc: 85.16%,  Val Loss:  0.37,  Val Acc: 89.49%,  Time: 0:07:06 </span><br><span class="line">Iter:  13300,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 89.90%,  Time: 0:07:09 </span><br><span class="line">Iter:  13400,  Train Loss:  0.28,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.26%,  Time: 0:07:12 </span><br><span class="line">Iter:  13500,  Train Loss:   0.3,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.65%,  Time: 0:07:15 </span><br><span class="line">Iter:  13600,  Train Loss:  0.34,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 88.99%,  Time: 0:07:18 </span><br><span class="line">Iter:  13700,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.44%,  Time: 0:07:21 </span><br><span class="line">Iter:  13800,  Train Loss:  0.43,  Train Acc: 83.59%,  Val Loss:  0.35,  Val Acc: 89.62%,  Time: 0:07:25 </span><br><span class="line">Iter:  13900,  Train Loss:  0.28,  Train Acc: 92.19%,  Val Loss:   0.4,  Val Acc: 88.97%,  Time: 0:07:29 </span><br><span class="line">Iter:  14000,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.51%,  Time: 0:07:32 </span><br><span class="line">Epoch [11&#x2F;20]</span><br><span class="line">Iter:  14100,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.14%,  Time: 0:07:35 </span><br><span class="line">Iter:  14200,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.48%,  Time: 0:07:39 </span><br><span class="line">Iter:  14300,  Train Loss:  0.27,  Train Acc: 89.84%,  Val Loss:  0.37,  Val Acc: 89.69%,  Time: 0:07:42 </span><br><span class="line">Iter:  14400,  Train Loss:  0.23,  Train Acc: 90.62%,  Val Loss:  0.37,  Val Acc: 89.92%,  Time: 0:07:46 </span><br><span class="line">Iter:  14500,  Train Loss:  0.33,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.72%,  Time: 0:07:49 </span><br><span class="line">Iter:  14600,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.83%,  Time: 0:07:52 </span><br><span class="line">Iter:  14700,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 89.63%,  Time: 0:07:55 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.34,  Test Acc: 89.88%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.8745    0.8710    0.8727      1000</span><br><span class="line">       realty     0.9173    0.9210    0.9192      1000</span><br><span class="line">       stocks     0.8308    0.8350    0.8329      1000</span><br><span class="line">    education     0.9356    0.9450    0.9403      1000</span><br><span class="line">      science     0.8395    0.8160    0.8276      1000</span><br><span class="line">      society     0.9097    0.9070    0.9084      1000</span><br><span class="line">     politics     0.8994    0.8760    0.8875      1000</span><br><span class="line">       sports     0.9896    0.9530    0.9710      1000</span><br><span class="line">         game     0.9312    0.9070    0.9189      1000</span><br><span class="line">entertainment     0.8661    0.9570    0.9093      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.8988     10000</span><br><span class="line">    macro avg     0.8994    0.8988    0.8988     10000</span><br><span class="line"> weighted avg     0.8994    0.8988    0.8988     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[871  18  74   6  13   7   5   1   0   5]</span><br><span class="line"> [ 16 921  21   1   9  15   4   1   1  11]</span><br><span class="line"> [ 59  22 835   1  38   4  25   1  10   5]</span><br><span class="line"> [  3   4   0 945   8  11   8   1   4  16]</span><br><span class="line"> [ 15  10  40   9 816  15  24   1  38  32]</span><br><span class="line"> [  5  12   1  21   7 907  22   0   7  18]</span><br><span class="line"> [ 15   3  24  12  21  26 876   0   2  21]</span><br><span class="line"> [  1   2   2   3   5   3   6 953   0  25]</span><br><span class="line"> [  6   5   5   6  47   4   3   2 907  15]</span><br><span class="line"> [  5   7   3   6   8   5   1   3   5 957]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure><p>长文本（标题+正文）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14883.19it&#x2F;s]</span><br><span class="line">10000it [00:00, 12805.79it&#x2F;s]</span><br><span class="line">10000it [00:00, 14804.58it&#x2F;s]Time usage: 0:00:14</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (postion_embedding): Positional_Encoding(</span><br><span class="line">    (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  )</span><br><span class="line">  (encoder): Encoder(</span><br><span class="line">    (attention): Multi_Head_Attention(</span><br><span class="line">      (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">      (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">    (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">      (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">      (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (encoders): ModuleList(</span><br><span class="line">    (0): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (fc1): Linear(in_features&#x3D;9600, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.5,  Train Acc:  8.59%,  Val Loss:   5.1,  Val Acc: 10.00%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:   1.2,  Train Acc: 54.69%,  Val Loss:   1.4,  Val Acc: 59.11%,  Time: 0:00:05 *</span><br><span class="line">Iter:    200,  Train Loss:  0.91,  Train Acc: 64.06%,  Val Loss:  0.91,  Val Acc: 73.32%,  Time: 0:00:08 *</span><br><span class="line">Iter:    300,  Train Loss:  0.93,  Train Acc: 66.41%,  Val Loss:  0.67,  Val Acc: 80.04%,  Time: 0:00:11 *</span><br><span class="line">Iter:    400,  Train Loss:  0.94,  Train Acc: 68.75%,  Val Loss:  0.67,  Val Acc: 81.33%,  Time: 0:00:14</span><br><span class="line">Iter:    500,  Train Loss:   0.7,  Train Acc: 75.00%,  Val Loss:  0.59,  Val Acc: 83.27%,  Time: 0:00:18 *</span><br><span class="line">Iter:    600,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.56,  Val Acc: 84.59%,  Time: 0:00:21 *</span><br><span class="line">Iter:    700,  Train Loss:  0.59,  Train Acc: 79.69%,  Val Loss:  0.53,  Val Acc: 85.44%,  Time: 0:00:24 *</span><br><span class="line">Iter:    800,  Train Loss:  0.47,  Train Acc: 85.16%,  Val Loss:  0.66,  Val Acc: 82.96%,  Time: 0:00:27</span><br><span class="line">Iter:    900,  Train Loss:  0.71,  Train Acc: 81.25%,  Val Loss:  0.54,  Val Acc: 85.88%,  Time: 0:00:30</span><br><span class="line">Iter:   1000,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.48,  Val Acc: 86.87%,  Time: 0:00:34 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.51,  Train Acc: 84.38%,  Val Loss:   0.5,  Val Acc: 86.69%,  Time: 0:00:37</span><br><span class="line">Iter:   1200,  Train Loss:  0.63,  Train Acc: 82.03%,  Val Loss:   0.5,  Val Acc: 87.30%,  Time: 0:00:40</span><br><span class="line">Iter:   1300,  Train Loss:  0.48,  Train Acc: 83.59%,  Val Loss:  0.48,  Val Acc: 87.17%,  Time: 0:00:43</span><br><span class="line">Iter:   1400,  Train Loss:  0.52,  Train Acc: 83.59%,  Val Loss:  0.49,  Val Acc: 87.48%,  Time: 0:00:46</span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:   0.6,  Train Acc: 79.69%,  Val Loss:  0.48,  Val Acc: 87.80%,  Time: 0:00:49 *</span><br><span class="line">Iter:   1600,  Train Loss:  0.46,  Train Acc: 86.72%,  Val Loss:  0.48,  Val Acc: 87.64%,  Time: 0:00:50</span><br><span class="line">Iter:   1700,  Train Loss:  0.43,  Train Acc: 85.16%,  Val Loss:  0.56,  Val Acc: 85.89%,  Time: 0:00:52</span><br><span class="line">Iter:   1800,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.43,  Val Acc: 88.56%,  Time: 0:00:54 *</span><br><span class="line">Iter:   1900,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.43,  Val Acc: 89.19%,  Time: 0:00:56</span><br><span class="line">Iter:   2000,  Train Loss:  0.43,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 88.27%,  Time: 0:00:57</span><br><span class="line">Iter:   2100,  Train Loss:  0.58,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 88.74%,  Time: 0:00:59 *</span><br><span class="line">Iter:   2200,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:  0.45,  Val Acc: 88.39%,  Time: 0:01:01</span><br><span class="line">Iter:   2300,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.75%,  Time: 0:01:02 *</span><br><span class="line">Iter:   2400,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 88.05%,  Time: 0:01:04</span><br><span class="line">Iter:   2500,  Train Loss:  0.55,  Train Acc: 82.03%,  Val Loss:  0.45,  Val Acc: 88.85%,  Time: 0:01:05</span><br><span class="line">Iter:   2600,  Train Loss:  0.44,  Train Acc: 83.59%,  Val Loss:  0.44,  Val Acc: 89.37%,  Time: 0:01:07</span><br><span class="line">Iter:   2700,  Train Loss:  0.52,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 89.23%,  Time: 0:01:09</span><br><span class="line">Iter:   2800,  Train Loss:   0.5,  Train Acc: 82.03%,  Val Loss:  0.41,  Val Acc: 89.03%,  Time: 0:01:11</span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.41,  Val Acc: 89.40%,  Time: 0:01:12</span><br><span class="line">Iter:   3000,  Train Loss:  0.38,  Train Acc: 86.72%,  Val Loss:  0.41,  Val Acc: 89.39%,  Time: 0:01:14</span><br><span class="line">Iter:   3100,  Train Loss:  0.26,  Train Acc: 87.50%,  Val Loss:  0.45,  Val Acc: 89.05%,  Time: 0:01:15</span><br><span class="line">Iter:   3200,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 90.10%,  Time: 0:01:17 *</span><br><span class="line">Iter:   3300,  Train Loss:  0.37,  Train Acc: 91.41%,  Val Loss:   0.4,  Val Acc: 90.25%,  Time: 0:01:19</span><br><span class="line">Iter:   3400,  Train Loss:  0.43,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 89.23%,  Time: 0:01:20</span><br><span class="line">Iter:   3500,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.35,  Val Acc: 90.79%,  Time: 0:01:22 *</span><br><span class="line">Iter:   3600,  Train Loss:  0.51,  Train Acc: 87.50%,  Val Loss:   0.4,  Val Acc: 89.65%,  Time: 0:01:24</span><br><span class="line">Iter:   3700,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 90.74%,  Time: 0:01:26 *</span><br><span class="line">Iter:   3800,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 90.38%,  Time: 0:01:27</span><br><span class="line">Iter:   3900,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.41,  Val Acc: 89.36%,  Time: 0:01:29</span><br><span class="line">Iter:   4000,  Train Loss:  0.47,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 90.55%,  Time: 0:01:31</span><br><span class="line">Iter:   4100,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:  0.36,  Val Acc: 90.73%,  Time: 0:01:33</span><br><span class="line">Iter:   4200,  Train Loss:  0.31,  Train Acc: 92.97%,  Val Loss:  0.45,  Val Acc: 88.83%,  Time: 0:01:35</span><br><span class="line">Epoch [4&#x2F;20]</span><br><span class="line">Iter:   4300,  Train Loss:   0.2,  Train Acc: 93.75%,  Val Loss:   0.4,  Val Acc: 89.56%,  Time: 0:01:37</span><br><span class="line">Iter:   4400,  Train Loss:  0.34,  Train Acc: 86.72%,  Val Loss:  0.34,  Val Acc: 90.85%,  Time: 0:01:38 *</span><br><span class="line">Iter:   4500,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.45,  Val Acc: 89.03%,  Time: 0:01:40</span><br><span class="line">Iter:   4600,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 90.92%,  Time: 0:01:42</span><br><span class="line">Iter:   4700,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.38,  Val Acc: 90.14%,  Time: 0:01:44</span><br><span class="line">Iter:   4800,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 90.78%,  Time: 0:01:46</span><br><span class="line">Iter:   4900,  Train Loss:  0.49,  Train Acc: 85.16%,  Val Loss:  0.36,  Val Acc: 90.53%,  Time: 0:01:47</span><br><span class="line">Iter:   5000,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 90.31%,  Time: 0:01:49</span><br><span class="line">Iter:   5100,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 90.70%,  Time: 0:01:51 *</span><br><span class="line">Iter:   5200,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 90.89%,  Time: 0:01:53 *</span><br><span class="line">Iter:   5300,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.32,  Val Acc: 91.47%,  Time: 0:01:54 *</span><br><span class="line">Iter:   5400,  Train Loss:  0.28,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 91.17%,  Time: 0:01:56</span><br><span class="line">Iter:   5500,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.32,  Val Acc: 90.96%,  Time: 0:01:58 *</span><br><span class="line">Iter:   5600,  Train Loss:  0.35,  Train Acc: 87.50%,  Val Loss:  0.31,  Val Acc: 91.41%,  Time: 0:02:00 *</span><br><span class="line">Epoch [5&#x2F;20]</span><br><span class="line">Iter:   5700,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.33,  Val Acc: 90.82%,  Time: 0:02:02</span><br><span class="line">Iter:   5800,  Train Loss:  0.28,  Train Acc: 90.62%,  Val Loss:  0.32,  Val Acc: 91.13%,  Time: 0:02:03</span><br><span class="line">Iter:   5900,  Train Loss:  0.28,  Train Acc: 87.50%,  Val Loss:  0.33,  Val Acc: 90.78%,  Time: 0:02:05</span><br><span class="line">Iter:   6000,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.31,  Val Acc: 91.33%,  Time: 0:02:07</span><br><span class="line">Iter:   6100,  Train Loss:  0.18,  Train Acc: 95.31%,  Val Loss:  0.36,  Val Acc: 90.30%,  Time: 0:02:08</span><br><span class="line">Iter:   6200,  Train Loss:  0.46,  Train Acc: 84.38%,  Val Loss:  0.35,  Val Acc: 90.57%,  Time: 0:02:10</span><br><span class="line">Iter:   6300,  Train Loss:  0.24,  Train Acc: 91.41%,  Val Loss:   0.3,  Val Acc: 91.65%,  Time: 0:02:12 *</span><br><span class="line">Iter:   6400,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:   0.3,  Val Acc: 91.65%,  Time: 0:02:14</span><br><span class="line">Iter:   6500,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.31,  Val Acc: 91.30%,  Time: 0:02:16</span><br><span class="line">Iter:   6600,  Train Loss:  0.32,  Train Acc: 86.72%,  Val Loss:   0.3,  Val Acc: 91.93%,  Time: 0:02:18</span><br><span class="line">Iter:   6700,  Train Loss:   0.2,  Train Acc: 94.53%,  Val Loss:  0.31,  Val Acc: 91.30%,  Time: 0:02:20</span><br><span class="line">Iter:   6800,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.71%,  Time: 0:02:21 *</span><br><span class="line">Iter:   6900,  Train Loss:  0.21,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.89%,  Time: 0:02:23</span><br><span class="line">Iter:   7000,  Train Loss:  0.44,  Train Acc: 85.94%,  Val Loss:   0.3,  Val Acc: 91.50%,  Time: 0:02:25</span><br><span class="line">Epoch [6&#x2F;20]</span><br><span class="line">Iter:   7100,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.28,  Val Acc: 91.57%,  Time: 0:02:27 *</span><br><span class="line">Iter:   7200,  Train Loss:   0.2,  Train Acc: 92.19%,  Val Loss:  0.29,  Val Acc: 91.81%,  Time: 0:02:29</span><br><span class="line">Iter:   7300,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.34,  Val Acc: 90.86%,  Time: 0:02:31</span><br><span class="line">Iter:   7400,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.29,  Val Acc: 91.68%,  Time: 0:02:33</span><br><span class="line">Iter:   7500,  Train Loss:  0.21,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 92.08%,  Time: 0:02:35</span><br><span class="line">Iter:   7600,  Train Loss:  0.26,  Train Acc: 89.06%,  Val Loss:   0.3,  Val Acc: 91.82%,  Time: 0:02:36</span><br><span class="line">Iter:   7700,  Train Loss:  0.23,  Train Acc: 90.62%,  Val Loss:   0.3,  Val Acc: 91.81%,  Time: 0:02:38</span><br><span class="line">Iter:   7800,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.28,  Val Acc: 91.81%,  Time: 0:02:40 *</span><br><span class="line">Iter:   7900,  Train Loss:  0.34,  Train Acc: 91.41%,  Val Loss:  0.27,  Val Acc: 92.38%,  Time: 0:02:42 *</span><br><span class="line">Iter:   8000,  Train Loss:  0.51,  Train Acc: 82.81%,  Val Loss:  0.28,  Val Acc: 92.01%,  Time: 0:02:44</span><br><span class="line">Iter:   8100,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.69%,  Time: 0:02:46</span><br><span class="line">Iter:   8200,  Train Loss:  0.34,  Train Acc: 89.84%,  Val Loss:  0.31,  Val Acc: 91.31%,  Time: 0:02:48</span><br><span class="line">Iter:   8300,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.32,  Val Acc: 91.25%,  Time: 0:02:49</span><br><span class="line">Iter:   8400,  Train Loss:  0.27,  Train Acc: 92.19%,  Val Loss:  0.28,  Val Acc: 92.02%,  Time: 0:02:51</span><br><span class="line">Epoch [7&#x2F;20]</span><br><span class="line">Iter:   8500,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.79%,  Time: 0:02:53</span><br><span class="line">Iter:   8600,  Train Loss:  0.26,  Train Acc: 89.84%,  Val Loss:  0.28,  Val Acc: 92.21%,  Time: 0:02:55</span><br><span class="line">Iter:   8700,  Train Loss:  0.25,  Train Acc: 89.84%,  Val Loss:  0.29,  Val Acc: 91.82%,  Time: 0:02:57</span><br><span class="line">Iter:   8800,  Train Loss:  0.23,  Train Acc: 92.97%,  Val Loss:  0.27,  Val Acc: 91.80%,  Time: 0:02:59</span><br><span class="line">Iter:   8900,  Train Loss:  0.29,  Train Acc: 92.19%,  Val Loss:  0.29,  Val Acc: 91.47%,  Time: 0:03:01</span><br><span class="line">Iter:   9000,  Train Loss:  0.17,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.84%,  Time: 0:03:03</span><br><span class="line">Iter:   9100,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.31,  Val Acc: 90.89%,  Time: 0:03:05</span><br><span class="line">Iter:   9200,  Train Loss:  0.18,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.57%,  Time: 0:03:07</span><br><span class="line">Iter:   9300,  Train Loss:  0.34,  Train Acc: 89.84%,  Val Loss:  0.27,  Val Acc: 91.93%,  Time: 0:03:08</span><br><span class="line">Iter:   9400,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.27,  Val Acc: 92.09%,  Time: 0:03:10</span><br><span class="line">Iter:   9500,  Train Loss:  0.25,  Train Acc: 92.19%,  Val Loss:  0.29,  Val Acc: 92.05%,  Time: 0:03:12</span><br><span class="line">Iter:   9600,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.28,  Val Acc: 92.03%,  Time: 0:03:13</span><br><span class="line">Iter:   9700,  Train Loss:  0.26,  Train Acc: 89.84%,  Val Loss:   0.3,  Val Acc: 91.83%,  Time: 0:03:15</span><br><span class="line">Iter:   9800,  Train Loss:  0.19,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.76%,  Time: 0:03:17</span><br><span class="line">Epoch [8&#x2F;20]</span><br><span class="line">Iter:   9900,  Train Loss:  0.29,  Train Acc: 88.28%,  Val Loss:  0.28,  Val Acc: 91.96%,  Time: 0:03:18</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.26,  Test Acc: 92.28%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9250    0.8760    0.8998      1000</span><br><span class="line">       realty     0.9103    0.9340    0.9220      1000</span><br><span class="line">       stocks     0.8696    0.9000    0.8845      1000</span><br><span class="line">    education     0.9550    0.9330    0.9439      1000</span><br><span class="line">      science     0.9078    0.8660    0.8864      1000</span><br><span class="line">      society     0.9214    0.8790    0.8997      1000</span><br><span class="line">     politics     0.8917    0.9300    0.9104      1000</span><br><span class="line">       sports     0.9879    0.9820    0.9850      1000</span><br><span class="line">         game     0.9367    0.9620    0.9492      1000</span><br><span class="line">entertainment     0.9262    0.9660    0.9457      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9228     10000</span><br><span class="line">    macro avg     0.9231    0.9228    0.9227     10000</span><br><span class="line"> weighted avg     0.9231    0.9228    0.9227     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[876  19  72   1   8   9   7   0   0   8]</span><br><span class="line"> [  9 934  19   2  10  11   8   0   2   5]</span><br><span class="line"> [ 36  12 900   2  22   2  17   1   6   2]</span><br><span class="line"> [  1  12   4 933   2  12  24   1   3   8]</span><br><span class="line"> [ 12   8  22   5 866  10  15   1  43  18]</span><br><span class="line"> [  4  19   5  24  17 879  33   2   3  14]</span><br><span class="line"> [  4   9   7   5  10  23 930   1   2   9]</span><br><span class="line"> [  1   2   2   0   2   2   2 982   0   7]</span><br><span class="line"> [  4   6   4   1  13   2   2   0 962   6]</span><br><span class="line"> [  0   5   0   4   4   4   5   6   6 966]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure><h2 id="3-4-cnn模型"><a href="#3-4-cnn模型" class="headerlink" title="3.4 cnn模型"></a>3.4 cnn模型</h2><p>1.运行脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run.py --model TextCNN &gt; nohup_cnn.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>2.运行log</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:03, 47432.48it&#x2F;s]</span><br><span class="line">10000it [00:00, 46997.85it&#x2F;s]</span><br><span class="line">10000it [00:00, 48451.14it&#x2F;s]Time usage: 0:00:04</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (convs): ModuleList(</span><br><span class="line">    (0): Conv2d(1, 256, kernel_size&#x3D;(2, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(1, 256, kernel_size&#x3D;(3, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (2): Conv2d(1, 256, kernel_size&#x3D;(4, 300), stride&#x3D;(1, 1))</span><br><span class="line">  )</span><br><span class="line">  (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  (fc): Linear(in_features&#x3D;768, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc: 14.84%,  Val Loss:   2.7,  Val Acc: 10.70%,  Time: 0:00:02 *</span><br><span class="line">Iter:    100,  Train Loss:  0.77,  Train Acc: 70.31%,  Val Loss:   0.7,  Val Acc: 78.36%,  Time: 0:00:07 *</span><br><span class="line">Iter:    200,  Train Loss:  0.74,  Train Acc: 74.22%,  Val Loss:  0.55,  Val Acc: 83.18%,  Time: 0:00:12 *</span><br><span class="line">Iter:    300,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.49,  Val Acc: 84.72%,  Time: 0:00:16 *</span><br><span class="line">Iter:    400,  Train Loss:  0.72,  Train Acc: 79.69%,  Val Loss:  0.48,  Val Acc: 85.10%,  Time: 0:00:20 *</span><br><span class="line">Iter:    500,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.44,  Val Acc: 86.10%,  Time: 0:00:24 *</span><br><span class="line">Iter:    600,  Train Loss:  0.49,  Train Acc: 84.38%,  Val Loss:  0.42,  Val Acc: 86.77%,  Time: 0:00:27 *</span><br><span class="line">Iter:    700,  Train Loss:  0.49,  Train Acc: 83.59%,  Val Loss:  0.41,  Val Acc: 87.15%,  Time: 0:00:31 *</span><br><span class="line">Iter:    800,  Train Loss:  0.43,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 87.80%,  Time: 0:00:36 *</span><br><span class="line">Iter:    900,  Train Loss:  0.47,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 88.04%,  Time: 0:00:40 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.38,  Val Acc: 88.54%,  Time: 0:00:45 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.38,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 88.64%,  Time: 0:00:50 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.38,  Train Acc: 86.72%,  Val Loss:  0.37,  Val Acc: 88.99%,  Time: 0:00:55 *</span><br><span class="line">Iter:   1300,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.81%,  Time: 0:01:00 *</span><br><span class="line">Iter:   1400,  Train Loss:  0.52,  Train Acc: 82.81%,  Val Loss:  0.35,  Val Acc: 88.96%,  Time: 0:01:04 *</span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:   0.4,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.09%,  Time: 0:01:10 *</span><br><span class="line">Iter:   1600,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.15%,  Time: 0:01:14</span><br><span class="line">Iter:   1700,  Train Loss:   0.4,  Train Acc: 86.72%,  Val Loss:  0.35,  Val Acc: 89.68%,  Time: 0:01:18 *</span><br><span class="line">Iter:   1800,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.90%,  Time: 0:01:22</span><br><span class="line">Iter:   1900,  Train Loss:  0.36,  Train Acc: 89.84%,  Val Loss:  0.34,  Val Acc: 89.37%,  Time: 0:01:27 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.35,  Train Acc: 85.16%,  Val Loss:  0.35,  Val Acc: 89.47%,  Time: 0:01:31</span><br><span class="line">Iter:   2100,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.40%,  Time: 0:01:35 *</span><br><span class="line">Iter:   2200,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.43%,  Time: 0:01:40</span><br><span class="line">Iter:   2300,  Train Loss:  0.31,  Train Acc: 93.75%,  Val Loss:  0.34,  Val Acc: 89.67%,  Time: 0:01:44 *</span><br><span class="line">Iter:   2400,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.90%,  Time: 0:01:49 *</span><br><span class="line">Iter:   2500,  Train Loss:  0.16,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.18%,  Time: 0:01:53 *</span><br><span class="line">Iter:   2600,  Train Loss:   0.4,  Train Acc: 83.59%,  Val Loss:  0.33,  Val Acc: 90.04%,  Time: 0:01:58</span><br><span class="line">Iter:   2700,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.90%,  Time: 0:02:02</span><br><span class="line">Iter:   2800,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.67%,  Time: 0:02:06</span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 89.80%,  Time: 0:02:11</span><br><span class="line">Iter:   3000,  Train Loss:  0.25,  Train Acc: 91.41%,  Val Loss:  0.34,  Val Acc: 89.71%,  Time: 0:02:15</span><br><span class="line">Iter:   3100,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 89.98%,  Time: 0:02:19</span><br><span class="line">Iter:   3200,  Train Loss:  0.39,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 89.88%,  Time: 0:02:24</span><br><span class="line">Iter:   3300,  Train Loss:  0.29,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.11%,  Time: 0:02:28 *</span><br><span class="line">Iter:   3400,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.97%,  Time: 0:02:33</span><br><span class="line">Iter:   3500,  Train Loss:  0.16,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 90.13%,  Time: 0:02:38</span><br><span class="line">Iter:   3600,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:02:42</span><br><span class="line">Iter:   3700,  Train Loss:  0.36,  Train Acc: 86.72%,  Val Loss:  0.33,  Val Acc: 90.08%,  Time: 0:02:47</span><br><span class="line">Iter:   3800,  Train Loss:  0.36,  Train Acc: 84.38%,  Val Loss:  0.32,  Val Acc: 90.17%,  Time: 0:02:52 *</span><br><span class="line">Iter:   3900,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 90.20%,  Time: 0:02:56</span><br><span class="line">Iter:   4000,  Train Loss:  0.24,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 90.23%,  Time: 0:03:01</span><br><span class="line">Iter:   4100,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:03:05</span><br><span class="line">Iter:   4200,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:03:11</span><br><span class="line">Epoch [4&#x2F;20]</span><br><span class="line">Iter:   4300,  Train Loss:  0.19,  Train Acc: 92.97%,  Val Loss:  0.32,  Val Acc: 90.27%,  Time: 0:03:15 *</span><br><span class="line">Iter:   4400,  Train Loss:  0.19,  Train Acc: 93.75%,  Val Loss:  0.32,  Val Acc: 90.37%,  Time: 0:03:19</span><br><span class="line">Iter:   4500,  Train Loss:  0.37,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 90.21%,  Time: 0:03:24</span><br><span class="line">Iter:   4600,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 90.02%,  Time: 0:03:29</span><br><span class="line">Iter:   4700,  Train Loss:  0.48,  Train Acc: 87.50%,  Val Loss:  0.32,  Val Acc: 90.25%,  Time: 0:03:34</span><br><span class="line">Iter:   4800,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 90.34%,  Time: 0:03:38</span><br><span class="line">Iter:   4900,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 90.26%,  Time: 0:03:41</span><br><span class="line">Iter:   5000,  Train Loss:  0.21,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.88%,  Time: 0:03:46</span><br><span class="line">Iter:   5100,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.21%,  Time: 0:03:50</span><br><span class="line">Iter:   5200,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 90.12%,  Time: 0:03:55</span><br><span class="line">Iter:   5300,  Train Loss:  0.19,  Train Acc: 96.09%,  Val Loss:  0.32,  Val Acc: 90.46%,  Time: 0:04:00 *</span><br><span class="line">Iter:   5400,  Train Loss:  0.47,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 90.17%,  Time: 0:04:04</span><br><span class="line">Iter:   5500,  Train Loss:  0.27,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.41%,  Time: 0:04:08</span><br><span class="line">Iter:   5600,  Train Loss:  0.13,  Train Acc: 95.31%,  Val Loss:  0.33,  Val Acc: 90.17%,  Time: 0:04:13</span><br><span class="line">Epoch [5&#x2F;20]</span><br><span class="line">Iter:   5700,  Train Loss:  0.24,  Train Acc: 94.53%,  Val Loss:  0.34,  Val Acc: 90.07%,  Time: 0:04:18</span><br><span class="line">Iter:   5800,  Train Loss:  0.16,  Train Acc: 93.75%,  Val Loss:  0.34,  Val Acc: 90.14%,  Time: 0:04:22</span><br><span class="line">Iter:   5900,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 90.52%,  Time: 0:04:28</span><br><span class="line">Iter:   6000,  Train Loss:  0.15,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.45%,  Time: 0:04:32</span><br><span class="line">Iter:   6100,  Train Loss:  0.26,  Train Acc: 89.06%,  Val Loss:  0.32,  Val Acc: 90.58%,  Time: 0:04:36</span><br><span class="line">Iter:   6200,  Train Loss:  0.13,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 90.46%,  Time: 0:04:41</span><br><span class="line">Iter:   6300,  Train Loss:  0.12,  Train Acc: 96.88%,  Val Loss:  0.33,  Val Acc: 90.56%,  Time: 0:04:45</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.29,  Test Acc: 91.34%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9193    0.8890    0.9039      1000</span><br><span class="line">       realty     0.9134    0.9490    0.9308      1000</span><br><span class="line">       stocks     0.8363    0.8790    0.8571      1000</span><br><span class="line">    education     0.9540    0.9550    0.9545      1000</span><br><span class="line">      science     0.8800    0.8580    0.8689      1000</span><br><span class="line">      society     0.9067    0.9140    0.9104      1000</span><br><span class="line">     politics     0.9054    0.8900    0.8976      1000</span><br><span class="line">       sports     0.9578    0.9540    0.9559      1000</span><br><span class="line">         game     0.9383    0.9130    0.9255      1000</span><br><span class="line">entertainment     0.9265    0.9330    0.9297      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9134     10000</span><br><span class="line">    macro avg     0.9138    0.9134    0.9134     10000</span><br><span class="line"> weighted avg     0.9138    0.9134    0.9134     10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[889  17  59   4  10   9   7   3   0   2]</span><br><span class="line"> [  9 949  13   2   4   8   4   2   3   6]</span><br><span class="line"> [ 40  24 879   3  21   0  25   3   4   1]</span><br><span class="line"> [  2   2   1 955   6  13   5   2   1  13]</span><br><span class="line"> [  8   8  38   6 858  15  16   5  36  10]</span><br><span class="line"> [  4  22   3  11  10 914  26   2   2   6]</span><br><span class="line"> [  9   7  32  10  15  27 890   4   0   6]</span><br><span class="line"> [  3   2   7   1   3   8   4 954   2  16]</span><br><span class="line"> [  2   1  15   4  36   4   3   8 913  14]</span><br><span class="line"> [  1   7   4   5  12  10   3  13  12 933]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure><p>长文本（标题+正文）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14970.72it&#x2F;s]</span><br><span class="line">10000it [00:00, 13435.87it&#x2F;s]</span><br><span class="line">10000it [00:00, 16773.90it&#x2F;s]</span><br><span class="line">Time usage: 0:00:13</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (convs): ModuleList(</span><br><span class="line">    (0): Conv2d(1, 256, kernel_size&#x3D;(2, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(1, 256, kernel_size&#x3D;(3, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (2): Conv2d(1, 256, kernel_size&#x3D;(4, 300), stride&#x3D;(1, 1))</span><br><span class="line">  )</span><br><span class="line">  (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  (fc): Linear(in_features&#x3D;768, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (convs): ModuleList(</span><br><span class="line">    (0): Conv2d(1, 256, kernel_size&#x3D;(2, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(1, 256, kernel_size&#x3D;(3, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (2): Conv2d(1, 256, kernel_size&#x3D;(4, 300), stride&#x3D;(1, 1))</span><br><span class="line">  )</span><br><span class="line">  (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  (fc): Linear(in_features&#x3D;768, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc: 11.72%,  Val Loss:   2.5,  Val Acc: 10.00%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:  0.53,  Train Acc: 83.59%,  Val Loss:  0.53,  Val Acc: 84.06%,  Time: 0:00:07 *</span><br><span class="line">Iter:    200,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.42,  Val Acc: 87.16%,  Time: 0:00:13 *</span><br><span class="line">Iter:    300,  Train Loss:  0.57,  Train Acc: 80.47%,  Val Loss:  0.37,  Val Acc: 88.62%,  Time: 0:00:17 *</span><br><span class="line">Iter:    400,  Train Loss:  0.43,  Train Acc: 88.28%,  Val Loss:  0.34,  Val Acc: 89.45%,  Time: 0:00:23 *</span><br><span class="line">Iter:    500,  Train Loss:   0.3,  Train Acc: 89.84%,  Val Loss:  0.32,  Val Acc: 89.96%,  Time: 0:00:29 *</span><br><span class="line">Iter:    600,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 89.63%,  Time: 0:00:33</span><br><span class="line">Iter:    700,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:   0.3,  Val Acc: 90.63%,  Time: 0:00:39 *</span><br><span class="line">Iter:    800,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.21%,  Time: 0:00:44 *</span><br><span class="line">Iter:    900,  Train Loss:  0.47,  Train Acc: 87.50%,  Val Loss:  0.28,  Val Acc: 91.13%,  Time: 0:00:49 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.28,  Train Acc: 89.84%,  Val Loss:  0.27,  Val Acc: 91.50%,  Time: 0:00:55 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.33,  Train Acc: 90.62%,  Val Loss:  0.26,  Val Acc: 92.00%,  Time: 0:01:01 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.51,  Train Acc: 86.72%,  Val Loss:  0.26,  Val Acc: 91.98%,  Time: 0:01:06</span><br><span class="line">Iter:   1300,  Train Loss:  0.28,  Train Acc: 88.28%,  Val Loss:  0.26,  Val Acc: 91.88%,  Time: 0:01:12 *</span><br><span class="line">Iter:   1400,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.82%,  Time: 0:01:16 *</span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:   0.4,  Train Acc: 89.84%,  Val Loss:  0.25,  Val Acc: 92.30%,  Time: 0:01:22 *</span><br><span class="line">Iter:   1600,  Train Loss:  0.35,  Train Acc: 92.19%,  Val Loss:  0.25,  Val Acc: 92.14%,  Time: 0:01:28 *</span><br><span class="line">Iter:   1700,  Train Loss:  0.31,  Train Acc: 93.75%,  Val Loss:  0.25,  Val Acc: 92.26%,  Time: 0:01:33</span><br><span class="line">Iter:   1800,  Train Loss:  0.23,  Train Acc: 89.84%,  Val Loss:  0.26,  Val Acc: 91.56%,  Time: 0:01:39</span><br><span class="line">Iter:   1900,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.49%,  Time: 0:01:43 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.25,  Val Acc: 92.26%,  Time: 0:01:49</span><br><span class="line">Iter:   2100,  Train Loss:  0.31,  Train Acc: 92.97%,  Val Loss:  0.24,  Val Acc: 92.43%,  Time: 0:01:55 *</span><br><span class="line">Iter:   2200,  Train Loss:  0.27,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.63%,  Time: 0:01:59 *</span><br><span class="line">Iter:   2300,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.51%,  Time: 0:02:06</span><br><span class="line">Iter:   2400,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.24,  Val Acc: 92.71%,  Time: 0:02:12</span><br><span class="line">Iter:   2500,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.25,  Val Acc: 92.40%,  Time: 0:02:17</span><br><span class="line">Iter:   2600,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 92.69%,  Time: 0:02:24 *</span><br><span class="line">Iter:   2700,  Train Loss:  0.23,  Train Acc: 93.75%,  Val Loss:  0.23,  Val Acc: 92.76%,  Time: 0:02:29</span><br><span class="line">Iter:   2800,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 92.76%,  Time: 0:02:35 *</span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.13,  Train Acc: 96.09%,  Val Loss:  0.24,  Val Acc: 92.43%,  Time: 0:02:41</span><br><span class="line">Iter:   3000,  Train Loss:  0.17,  Train Acc: 92.19%,  Val Loss:  0.23,  Val Acc: 92.86%,  Time: 0:02:47</span><br><span class="line">Iter:   3100,  Train Loss:  0.21,  Train Acc: 94.53%,  Val Loss:  0.23,  Val Acc: 92.68%,  Time: 0:02:54</span><br><span class="line">Iter:   3200,  Train Loss:  0.14,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 92.91%,  Time: 0:02:58 *</span><br><span class="line">Iter:   3300,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.83%,  Time: 0:03:05</span><br><span class="line">Iter:   3400,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.75%,  Time: 0:03:10</span><br><span class="line">Iter:   3500,  Train Loss:  0.18,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 93.05%,  Time: 0:03:16</span><br><span class="line">Iter:   3600,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.24,  Val Acc: 92.68%,  Time: 0:03:22</span><br><span class="line">Iter:   3700,  Train Loss:  0.13,  Train Acc: 96.09%,  Val Loss:  0.23,  Val Acc: 92.89%,  Time: 0:03:28</span><br><span class="line">Iter:   3800,  Train Loss:  0.21,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.93%,  Time: 0:03:34</span><br><span class="line">Iter:   3900,  Train Loss:  0.13,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 93.03%,  Time: 0:03:39</span><br><span class="line">Iter:   4000,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 93.02%,  Time: 0:03:46</span><br><span class="line">Iter:   4100,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.24,  Val Acc: 92.72%,  Time: 0:03:52</span><br><span class="line">Iter:   4200,  Train Loss:  0.15,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.88%,  Time: 0:03:58</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.23,  Test Acc: 92.71%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9365    0.9000    0.9179      1000</span><br><span class="line">       realty     0.9260    0.9380    0.9319      1000</span><br><span class="line">       stocks     0.8817    0.9170    0.8990      1000</span><br><span class="line">    education     0.9568    0.9520    0.9544      1000</span><br><span class="line">      science     0.9192    0.8760    0.8971      1000</span><br><span class="line">      society     0.8951    0.8960    0.8956      1000</span><br><span class="line">     politics     0.9204    0.9130    0.9167      1000</span><br><span class="line">       sports     0.9732    0.9810    0.9771      1000</span><br><span class="line">         game     0.9360    0.9510    0.9435      1000</span><br><span class="line">entertainment     0.9275    0.9470    0.9372      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9271     10000</span><br><span class="line">    macro avg     0.9272    0.9271    0.9270     10000</span><br><span class="line"> weighted avg     0.9272    0.9271    0.9270     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[900  18  57   0   5   9   7   0   0   4]</span><br><span class="line"> [  5 938  16   2   9  10   7   2   6   5]</span><br><span class="line"> [ 32  14 917   4  11   0  15   0   4   3]</span><br><span class="line"> [  1   2   3 952   1  20   9   0   0  12]</span><br><span class="line"> [  5   6  32   5 876  20  11   2  34   9]</span><br><span class="line"> [  5  18   0  19  18 896  20   3   3  18]</span><br><span class="line"> [  8   8   9   9   5  29 913   5   4  10]</span><br><span class="line"> [  2   3   1   0   2   1   2 981   0   8]</span><br><span class="line"> [  1   4   4   0  20   4   2   9 951   5]</span><br><span class="line"> [  2   2   1   4   6  12   6   6  14 947]]</span><br><span class="line">Time usage: 0:00:01</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure><h1 id="4-评价"><a href="#4-评价" class="headerlink" title="4 评价"></a>4 评价</h1><h2 id="4-1-评价指标"><a href="#4-1-评价指标" class="headerlink" title="4.1 评价指标"></a>4.1 评价指标</h2><h2 id="4-2-结果汇总"><a href="#4-2-结果汇总" class="headerlink" title="4.2 结果汇总"></a>4.2 结果汇总</h2><blockquote><p>参考：</p><p>[1] Convolutional Neural Networks for Sentence Classification<br>[2] Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification<br>[3] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br>[4] Attention Is All You Need</p><p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch">https://github.com/649453932/Chinese-Text-Classification-Pytorch</a></p><p><a href="https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch">https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;中文文本分类流程&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="research" scheme="http://example.com/categories/research/"/>
    
    <category term="text_classify" scheme="http://example.com/categories/research/text-classify/"/>
    
    
  </entry>
  
  <entry>
    <title>data_preprocess</title>
    <link href="http://example.com/2021/11/27/data-preprocess/"/>
    <id>http://example.com/2021/11/27/data-preprocess/</id>
    <published>2021-11-27T15:35:07.000Z</published>
    <updated>2021-11-27T15:57:49.457Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>中英机器翻译的数据预处理流程，暂存一下看到的优秀博文，过段时间改成自用版本。</p></blockquote><span id="more"></span><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文在news-commentary-v15语料上训练了<strong>中英NMT模型</strong>，并将整个流程，包括工具和数据的准备、数据的预处理、训练及解码，以及中途遇到的问题和解决方案记录在此，希望能够给予别人一些帮助。</p><h1 id="1-相关工具及目录结构"><a href="#1-相关工具及目录结构" class="headerlink" title="1 相关工具及目录结构"></a>1 相关工具及目录结构</h1><h2 id="1-1-相关工具"><a href="#1-1-相关工具" class="headerlink" title="1.1 相关工具"></a>1.1 相关工具</h2><p>除<strong>jieba</strong>是使用<code>pip install</code>安装外，其他几个工具都是建议直接克隆库到自己的用户目录中，方便使用其脚本(<strong>moses</strong>/<strong>subword-nmt</strong>)，或未来可能要自己拓展其中的模型(<strong>fairseq</strong>)</p><ol><li><p>Moses (一个SMT工具，在这里只会用到一些预处理脚本，如：tokenisation,  truecasing, cleaning)，安装指令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;moses-smt&#x2F;mosesdecoder.git</span><br></pre></td></tr></table></figure></li><li><p>subword-nmt (使用BPE算法生成子词的预处理脚本)，安装指令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;rsennrich&#x2F;subword-nmt.git</span><br></pre></td></tr></table></figure></li><li><p>jieba (中文分词组件)，安装指令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure></li><li><p>fairseq (一个基于PyTorch的序列建模工具)，安装指令如下：</p><p>fairseq安装参考：<a href="https://zhuanlan.zhihu.com/p/194176917">https://zhuanlan.zhihu.com/p/194176917</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;fairseq</span><br><span class="line">cd fairseq</span><br><span class="line">pip install --editable .&#x2F;</span><br></pre></td></tr></table></figure></li></ol><h2 id="1-2-目录结构与初始化"><a href="#1-2-目录结构与初始化" class="headerlink" title="1.2 目录结构与初始化"></a>1.2 目录结构与初始化</h2><h3 id="1-2-1-目录结构"><a href="#1-2-1-目录结构" class="headerlink" title="1.2.1 目录结构"></a>1.2.1 目录结构</h3><p>提前组织一个目录结构的好处是可以让后面的一系列操作更加统一、规范化。下表中<code>~</code>代表linux系统中<strong>我的用户目录</strong>, v15news目录名代表此次我使用的数据集名称</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">~</span><br><span class="line">├── mosesdecoder</span><br><span class="line">├── subword-nmt</span><br><span class="line">├── fairseq</span><br><span class="line">└── nmt</span><br><span class="line">    ├── data</span><br><span class="line">        └── v15news</span><br><span class="line">            ├── result          # 用于存放翻译结果</span><br><span class="line">            └── data-bin        # 用于存放二进制文件</span><br><span class="line">    ├── models                  # 用于保存过程中的model文件和checkpoint</span><br><span class="line">        └── v15news</span><br><span class="line">            └── checkpoints     # 保存checkpoints</span><br><span class="line">    ├── utils                   # 一些其他工具</span><br><span class="line">        ├── split.py            # 用于划分train,valid,test</span><br><span class="line">        └── cut2.py             # 用于划分src,tgt</span><br><span class="line">    └── scripts                 # 一些脚本</span><br></pre></td></tr></table></figure><h3 id="1-2-2-用于初始化的bash文件"><a href="#1-2-2-用于初始化的bash文件" class="headerlink" title="1.2.2 用于初始化的bash文件"></a>1.2.2 用于初始化的bash文件</h3><p>这个文件是在上述目录结构的基础下，定义了一些后面需要用到的变量(主要是<strong>各种脚本的路径</strong>)，包括tokenizer.perl, truecase.perl等，可以在linux中使用bash xx.sh运行，也可以把这些内容直接全部复制到linux命令行中按回车</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line"></span><br><span class="line">src&#x3D;zh</span><br><span class="line">tgt&#x3D;en</span><br><span class="line"></span><br><span class="line">SCRIPTS&#x3D;~&#x2F;mosesdecoder&#x2F;scripts</span><br><span class="line">TOKENIZER&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;tokenizer.perl</span><br><span class="line">DETOKENIZER&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;detokenizer.perl</span><br><span class="line">LC&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;lowercase.perl</span><br><span class="line">TRAIN_TC&#x3D;$&#123;SCRIPTS&#125;&#x2F;recaser&#x2F;train-truecaser.perl</span><br><span class="line">TC&#x3D;$&#123;SCRIPTS&#125;&#x2F;recaser&#x2F;truecase.perl</span><br><span class="line">DETC&#x3D;$&#123;SCRIPTS&#125;&#x2F;recaser&#x2F;detruecase.perl</span><br><span class="line">NORM_PUNC&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;normalize-punctuation.perl</span><br><span class="line">CLEAN&#x3D;$&#123;SCRIPTS&#125;&#x2F;training&#x2F;clean-corpus-n.perl</span><br><span class="line">BPEROOT&#x3D;~&#x2F;subword-nmt&#x2F;subword_nmt</span><br><span class="line">MULTI_BLEU&#x3D;$&#123;SCRIPTS&#125;&#x2F;generic&#x2F;multi-bleu.perl</span><br><span class="line">MTEVAL_V14&#x3D;$&#123;SCRIPTS&#125;&#x2F;generic&#x2F;mteval-v14.pl</span><br><span class="line"></span><br><span class="line">data_dir&#x3D;~&#x2F;nmt&#x2F;data&#x2F;v15news</span><br><span class="line">model_dir&#x3D;~&#x2F;nmt&#x2F;models&#x2F;v15news</span><br><span class="line">utils&#x3D;~&#x2F;nmt&#x2F;utils</span><br></pre></td></tr></table></figure><h1 id="2-数据的准备"><a href="#2-数据的准备" class="headerlink" title="2 数据的准备"></a>2 数据的准备</h1><h2 id="2-1-平行语料"><a href="#2-1-平行语料" class="headerlink" title="2.1 平行语料"></a>2.1 平行语料</h2><p>对于有监督神经机器翻译，能够找到的中英平行语料如下：</p><ol><li><a href="https://github.com/NiuTrans/NiuTrans.SMT/tree/master/sample-data">NEU nlp lab 开源语料</a> (10w，国内政治新闻领域)</li><li><a href="http://www.statmt.org/wmt20/translation-task.html">WMT新闻翻译任务News Commentary语料</a> (32w左右，国际新闻领域。其实News Commentary每年都有新闻数据集，但是基本没啥变化，每次在前一年的基础上加几百句，所以这里的链接直接指向最新的WMT20)</li><li><a href="https://catalog.ldc.upenn.edu/LDC2010T21">NIST数据集</a> (200w左右，需要购买)</li><li><a href="https://conferences.unite.un.org/UNCORPUS/zh">United Nations Parallel Corpus</a> (1500w左右，联合国文件领域)</li></ol><p>我本人使用过语料1、3，其中3是跟已购买的师兄要的，不向外提供。其实初次训练建议使用语料1，规模小训练快，能够快速体验整个流程。当然，中英还有很多其他语料，见<a href="https://chinesenlp.xyz/#/docs/machine_translation">参考资料1</a>, <a href="https://www.cluebenchmarks.com/dataSet_search.html">2</a></p><h2 id="2-2-数据预处理"><a href="#2-2-数据预处理" class="headerlink" title="2.2 数据预处理"></a>2.2 数据预处理</h2><h3 id="2-2-1-数据格式"><a href="#2-2-1-数据格式" class="headerlink" title="2.2.1 数据格式"></a>2.2.1 数据格式</h3><p>在本篇博客中，我准备使用WMT20新闻翻译任务的<strong>news-commentary-v15语料</strong>，放于以下位置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">└── nmt</span><br><span class="line">    ├── data</span><br><span class="line">        └── v15news     </span><br><span class="line">            └── news-commentary-v15.en-zh.tsv</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1929 or 1989?1929年还是1989年?</span><br><span class="line">PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。</span><br><span class="line">At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="2-2-2-切分"><a href="#2-2-2-切分" class="headerlink" title="2.2.2 切分"></a>2.2.2 切分</h3><p>首先，需要将一个单独的数据文件切分成标准格式，即源语言(raw.zh)、目标语言(raw.en)文件各一个，一行一句，附自己写的脚本(~/nmt/utils/cut2.py)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">Usage: </span><br><span class="line">python cut2.py fpath new_data_dir</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">def cut2(fpath, new_data_dir, nsrc&#x3D;&#39;zh&#39;, ntgt&#x3D;&#39;en&#39;):</span><br><span class="line">    fp &#x3D; open(fpath, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">    src_fp &#x3D; open(new_data_dir + &#39;raw.&#39; + nsrc, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">    tgt_fp &#x3D; open(new_data_dir + &#39;raw.&#39; + ntgt, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">    for line in fp.readlines():</span><br><span class="line">        tgt_line, src_line &#x3D; line.replace(&#39;\n&#39;, &#39;&#39;).split(&#39;\t&#39;)</span><br><span class="line">        src_fp.write(src_line + &#39;\n&#39;)</span><br><span class="line">        tgt_fp.write(tgt_line + &#39;\n&#39;)</span><br><span class="line">    src_fp.close()</span><br><span class="line">    tgt_fp.close()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:      </span><br><span class="line">    cut2(fpath&#x3D;sys.argv[1], new_data_dir&#x3D;sys.argv[2], nsrc&#x3D;&#39;zh&#39;, ntgt&#x3D;&#39;en&#39;)</span><br></pre></td></tr></table></figure><p>使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python $&#123;utils&#125;&#x2F;cut2.py $&#123;data_dir&#125;&#x2F;news-commentary-v15.en-zh.tsv $&#123;data_dir&#125;&#x2F;</span><br></pre></td></tr></table></figure><p><strong>后注：</strong> 在linux里可以直接用cut实现 <code>cut -f 1 fpath &gt; new_data_dir/raw.en | cut -f 2 fpath &gt; new_data_dir/raw.zh</code></p><p>切分后的文件在目录中如下格式存放：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ├── news-commentary-v15.en-zh.tsv</span><br><span class="line">        ├── raw.zh</span><br><span class="line">        └── raw.en</span><br></pre></td></tr></table></figure><h3 id="2-2-3-normalize-punctuation-可选"><a href="#2-2-3-normalize-punctuation-可选" class="headerlink" title="2.2.3 normalize-punctuation(可选)"></a>2.2.3 normalize-punctuation(可选)</h3><p>标点符号的标准化，同时对双语文件(raw.en, raw.zh)处理，使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">perl $&#123;NORM_PUNC&#125; -l en &lt; $&#123;data_dir&#125;&#x2F;raw.en &gt; $&#123;data_dir&#125;&#x2F;norm.en</span><br><span class="line">perl $&#123;NORM_PUNC&#125; -l zh &lt; $&#123;data_dir&#125;&#x2F;raw.zh &gt; $&#123;data_dir&#125;&#x2F;norm.zh</span><br></pre></td></tr></table></figure><p>处理后的文件在目录中如下格式存放：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ...</span><br><span class="line">        ├── norm.zh</span><br><span class="line">        └── norm.en</span><br></pre></td></tr></table></figure><p>效果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># raw.en</span><br><span class="line">“We can’t waste time,” he says.</span><br><span class="line">Yet, according to the political economist Moeletsi Mbeki, at his core, “Zuma is a conservative.”</span><br><span class="line"></span><br><span class="line"># norm.en</span><br><span class="line">&quot;We can&#39;t waste time,&quot; he says.</span><br><span class="line">Yet, according to the political economist Moeletsi Mbeki, at his core, &quot;Zuma is a conservative.&quot;</span><br></pre></td></tr></table></figure><h3 id="2-2-4-中文分词"><a href="#2-2-4-中文分词" class="headerlink" title="2.2.4 中文分词"></a>2.2.4 中文分词</h3><p>对标点符号标准化后的中文文件(norm.zh)进行分词处理，使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m jieba -d &quot; &quot; $&#123;data_dir&#125;&#x2F;norm.zh &gt; $&#123;data_dir&#125;&#x2F;norm.seg.zh</span><br></pre></td></tr></table></figure><p>处理后的文件在目录中如下格式存放：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ...</span><br><span class="line">        └── norm.seg.zh</span><br></pre></td></tr></table></figure><p>效果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># norm.zh</span><br><span class="line">1929年还是1989年?</span><br><span class="line">巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。</span><br><span class="line">一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。</span><br><span class="line"></span><br><span class="line"># norm.seg.zh</span><br><span class="line">1929 年 还是 1989 年 ?</span><br><span class="line">巴黎 - 随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 。</span><br><span class="line">一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 情况 ， 这样 得 类比 是 令人 宽心 的 ， 因为 这 两段 时期 意味着 典型 的 周期性 衰退 。</span><br></pre></td></tr></table></figure><h3 id="2-2-5-tokenize"><a href="#2-2-5-tokenize" class="headerlink" title="2.2.5 tokenize"></a>2.2.5 tokenize</h3><p>对上述处理后的双语文件(norm.en, norm.seg.zh)进行标记化处理，有很多功能(1.将<strong>英文单词</strong>与<strong>标点符号</strong>用空格分开 2.将多个连续空格简化为一个空格 3.将很多符号替换成转义字符，如：把<code>&quot;</code>替换成<code>&quot;</code>、把<code>can&#39;t</code>替换成<code>can &#39;t</code>)，使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$&#123;TOKENIZER&#125; -l en &lt; $&#123;data_dir&#125;&#x2F;norm.en &gt; $&#123;data_dir&#125;&#x2F;norm.tok.en</span><br><span class="line">$&#123;TOKENIZER&#125; -l zh &lt; $&#123;data_dir&#125;&#x2F;norm.seg.zh &gt; $&#123;data_dir&#125;&#x2F;norm.seg.tok.zh</span><br></pre></td></tr></table></figure><p>处理后的文件在目录中如下格式存放：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ...</span><br><span class="line">        ├── norm.tok.en</span><br><span class="line">        └── norm.seg.tok.zh</span><br></pre></td></tr></table></figure><p>效果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># norm.seg.zh</span><br><span class="line">目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲   ）   ，   要么 是 努力 的 扩展 （ 美国   ）   。</span><br><span class="line">而 历史 是 不 公平 的 。   尽管 美国 要 为 当今 的 全球 危机 负 更 大 的 责任 ， 但 美国 可能 会 比 大多数 国家 以 更 良好 的 势态 走出 困境 。</span><br><span class="line"></span><br><span class="line"># norm.seg.tok.zh</span><br><span class="line">目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲 ） ， 要么 是 努力 的 扩展 （ 美国 ） 。</span><br><span class="line">而 历史 是 不 公平 的 。 尽管 美国 要 为 当今 的 全球 危机 负 更 大 的 责任 ， 但 美国 可能 会 比 大多数 国家 以 更 良好 的 势态 走出 困境 。</span><br><span class="line"></span><br><span class="line"># norm.en</span><br><span class="line">&quot;We can&#39;t waste time,&quot; he says.</span><br><span class="line">Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.</span><br><span class="line">Second, Zoellick should ask why the Bank spends only 2.5% of its budget on the &quot;knowledge bank&quot; research function that it trumpets so proudly in its external relations materials, while it spends three times that amount on maintaining its executive board.</span><br><span class="line"></span><br><span class="line"># norm.tok.en</span><br><span class="line">&quot; We can &amp;apos;t waste time , &quot; he says .</span><br><span class="line">Of course , the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall .</span><br><span class="line">Second , Zoellick should ask why the Bank spends only 2.5 % of its budget on the &quot; knowledge bank &quot; research function that it trumpets so proudly in its external relations materials , while it spends three times that amount on maintaining its executive board .</span><br></pre></td></tr></table></figure><h3 id="2-2-6-truecase"><a href="#2-2-6-truecase" class="headerlink" title="2.2.6 truecase"></a>2.2.6 truecase</h3><p>对上述处理后的英文文件(norm.tok.en)进行大小写转换处理(对于句中的每个英文单词，尤其是<strong>句首单词</strong>，在数据中<strong>学习</strong>最适合它们的大小写形式)，使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$&#123;TRAIN_TC&#125; --model $&#123;model_dir&#125;&#x2F;truecase-model.en --corpus $&#123;data_dir&#125;&#x2F;norm.tok.en</span><br><span class="line">$&#123;TC&#125; --model $&#123;model_dir&#125;&#x2F;truecase-model.en &lt; $&#123;data_dir&#125;&#x2F;norm.tok.en &gt; $&#123;data_dir&#125;&#x2F;norm.tok.true.en</span><br></pre></td></tr></table></figure><p>处理后的文件在目录中如下格式存放：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        └── norm.tok.true.en</span><br><span class="line">├── models</span><br><span class="line">    └── v15news</span><br><span class="line">        └── truecase-model.en</span><br></pre></td></tr></table></figure><p>效果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># norm.tok.en</span><br><span class="line">PARIS - As the economic crisis deepens and widens , the world has been searching for historical analogies to help us understand what has been happening .</span><br><span class="line">At the start of the crisis , many people likened it to 1982 or 1973 , which was reassuring , because both dates refer to classical cyclical downturns .</span><br><span class="line">When the TTIP was first proposed , Europe seemed to recognize its value .</span><br><span class="line">Europe is being cautious in the name of avoiding debt and defending the euro , whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms .</span><br><span class="line"></span><br><span class="line"># norm.tok.true.en</span><br><span class="line">Paris - As the economic crisis deepens and widens , the world has been searching for historical analogies to help us understand what has been happening .</span><br><span class="line">at the start of the crisis , many people likened it to 1982 or 1973 , which was reassuring , because both dates refer to classical cyclical downturns .</span><br><span class="line">when the TTIP was first proposed , Europe seemed to recognize its value .</span><br><span class="line">Europe is being cautious in the name of avoiding debt and defending the euro , whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms .</span><br></pre></td></tr></table></figure><h3 id="2-2-7-bpe"><a href="#2-2-7-bpe" class="headerlink" title="2.2.7 bpe"></a>2.2.7 bpe</h3><p>对上述处理后的双语文件(norm.tok.true.en, norm.seg.tok.zh)进行子词处理(可以理解为更细粒度的分词)，使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python $&#123;BPEROOT&#125;&#x2F;learn_joint_bpe_and_vocab.py --input $&#123;data_dir&#125;&#x2F;norm.tok.true.en  -s 32000 -o $&#123;model_dir&#125;&#x2F;bpecode.en --write-vocabulary $&#123;model_dir&#125;&#x2F;voc.en</span><br><span class="line">python $&#123;BPEROOT&#125;&#x2F;apply_bpe.py -c $&#123;model_dir&#125;&#x2F;bpecode.en --vocabulary $&#123;model_dir&#125;&#x2F;voc.en &lt; $&#123;data_dir&#125;&#x2F;norm.tok.true.en &gt; $&#123;data_dir&#125;&#x2F;norm.tok.true.bpe.en</span><br><span class="line"></span><br><span class="line">python $&#123;BPEROOT&#125;&#x2F;learn_joint_bpe_and_vocab.py --input $&#123;data_dir&#125;&#x2F;norm.seg.tok.zh  -s 32000 -o $&#123;model_dir&#125;&#x2F;bpecode.zh --write-vocabulary $&#123;model_dir&#125;&#x2F;voc.zh</span><br><span class="line">python $&#123;BPEROOT&#125;&#x2F;apply_bpe.py -c $&#123;model_dir&#125;&#x2F;bpecode.zh --vocabulary $&#123;model_dir&#125;&#x2F;voc.zh &lt; $&#123;data_dir&#125;&#x2F;norm.seg.tok.zh &gt; $&#123;data_dir&#125;&#x2F;norm.seg.tok.bpe.zh</span><br></pre></td></tr></table></figure><p>处理后的文件在目录中如下格式存放：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── norm.seg.tok.bpe.zh</span><br><span class="line">        └── norm.tok.true.bpe.en</span><br><span class="line">├── models</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── voc.zh</span><br><span class="line">        ├── voc.en</span><br><span class="line">        ├── bpecode.zh</span><br><span class="line">        └── bpecode.en</span><br></pre></td></tr></table></figure><p>效果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># norm.seg.tok.zh</span><br><span class="line">从 一流 的 麻省理工学院 的 媒体 实验室 到 哈佛大学 的 数学 和 经济系 ， 亚洲 人 - 尤其 是 中国 和 印度人 - 到处 都 是 ， 犹如 公元前 一 世纪 在 雅典 的 罗马 人 一样 ： 他们 对 那里 学到 太 多 东西 的 人们 充满 了 敬佩 ， 而 他们 将 在 今后 几十年 打败 他们 学习 的 对象 。</span><br><span class="line">这 不仅 加大 了 预防 危机 的 难度 - - 尤其 因为 它 为 参与者 提供 了 钻空子 和 逃避责任 的 机会 - - 还 使得 人们 越来越 难以 采取措施 来 应对 危机 。</span><br><span class="line">它们 将 通胀 目标 设定 在 2 % 左右 - - 这 意味着 当 波涛汹涌 时 他们 根本 没有 多少 施展 空间 。</span><br><span class="line"></span><br><span class="line"># norm.seg.tok.bpe.zh</span><br><span class="line">从 一流 的 麻省理工学院 的 媒体 实验室 到 哈佛大学 的 数学 和 经济@@ 系 ， 亚洲 人 - 尤其 是 中国 和 印度人 - 到处 都 是 ， 犹如 公元前 一 世纪 在 雅典 的 罗马 人 一样 ： 他们 对 那里 学到 太 多 东西 的 人们 充满 了 敬佩 ， 而 他们 将 在 今后 几十年 打败 他们 学习 的 对象 。</span><br><span class="line">这 不仅 加大 了 预防 危机 的 难度 - - 尤其 因为 它 为 参与者 提供 了 钻@@ 空子 和 逃避@@ 责任 的 机会 - - 还 使得 人们 越来越 难以 采取措施 来 应对 危机 。</span><br><span class="line">它们 将 通胀 目标 设定 在 2 % 左右 - - 这 意味着 当 波@@ 涛@@ 汹涌 时 他们 根本 没有 多少 施展 空间 。</span><br><span class="line"></span><br><span class="line"># norm.tok.true.en</span><br><span class="line">indeed , on the surface it seems to be its perfect antithesis : the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism .</span><br><span class="line">as a visiting professor at Harvard and MIT , I am getting a good preview of what the world could look like when the crisis finally passes .</span><br><span class="line">one senses something like the making of an American-Asian dominated universe .</span><br><span class="line"></span><br><span class="line"># norm.tok.true.bpe.en</span><br><span class="line">indeed , on the surface it seems to be its perfect anti@@ thesis : the collapse of a wall symboli@@ zing oppression and artificial divisions versus the collapse of a seemingly inde@@ struc@@ tible and reassuring institution of financial capitalism .</span><br><span class="line">as a visiting professor at Harvard and MIT , I am getting a good pre@@ view of what the world could look like when the crisis finally passes .</span><br><span class="line">one senses something like the making of an American-@@ Asian dominated universe .</span><br></pre></td></tr></table></figure><blockquote><p><strong>后注：</strong><br>需要注意的是，我为了方便，步骤上失去了一些正确性。正确的做法应该是在<strong>训练集</strong>中学习bpe模型，再将bpe模型应用到<strong>测试集</strong>和<strong>验证集</strong>中。而我是直接在全部数据中学bpe模型了。</p></blockquote><h3 id="2-2-8-clean"><a href="#2-2-8-clean" class="headerlink" title="2.2.8 clean"></a>2.2.8 clean</h3><p>对上述处理后的双语文件(norm.tok.true.bpe.en, norm.seg.tok.bpe.zh)进行过滤(可以过滤<strong>最小长度</strong>和<strong>最大长度</strong>之间的句对，这样能够有效过滤空白行。还可以过滤<strong>长度比</strong>不合理的句对)，使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mv $&#123;data_dir&#125;&#x2F;norm.seg.tok.bpe.zh $&#123;data_dir&#125;&#x2F;toclean.zh</span><br><span class="line">mv $&#123;data_dir&#125;&#x2F;norm.tok.true.bpe.en $&#123;data_dir&#125;&#x2F;toclean.en </span><br><span class="line">$&#123;CLEAN&#125; $&#123;data_dir&#125;&#x2F;toclean zh en $&#123;data_dir&#125;&#x2F;clean 1 256</span><br></pre></td></tr></table></figure><p>处理后的文件在目录中如下格式存放：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── clean.zh</span><br><span class="line">        └── clean.en</span><br></pre></td></tr></table></figure><p>效果如下(每行最开始标出了<strong>行号</strong>):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># norm.tok.true.bpe.en</span><br><span class="line">30 we can only hope that , in the end , the consequences of 2009 similarly prove to be far less dramatic than we now - intuitively and in our historical refle@@ xes - feel them to be .</span><br><span class="line">31</span><br><span class="line">32 one Hund@@ red Years of Ine@@ p@@ titude</span><br><span class="line"></span><br><span class="line"># clean.en</span><br><span class="line">30 we can only hope that , in the end , the consequences of 2009 similarly prove to be far less dramatic than we now - intuitively and in our historical refle@@ xes - feel them to be .</span><br><span class="line">31 one Hund@@ red Years of Ine@@ p@@ titude</span><br><span class="line">32 Berlin - The global financial and economic crisis that began in 2008 was the greatest economic stre@@ ss-@@ test since the Great Depression , and the greatest challenge to social and political systems since World War II .</span><br><span class="line"></span><br><span class="line"># norm.seg.tok.bpe.zh</span><br><span class="line">30 我们 只能 希望 2009 年 的 危机 同样 地 最后 被 证明 是 远远 低于 我们 现在 以 直觉 和 历史 回顾 的 方式 � � 感觉 到 的 那么 剧烈 。</span><br><span class="line">31 </span><br><span class="line">32 百年 愚@@ 顽</span><br><span class="line"></span><br><span class="line"># clean.zh</span><br><span class="line">30 我们 只能 希望 2009 年 的 危机 同样 地 最后 被 证明 是 远远 低于 我们 现在 以 直觉 和 历史 回顾 的 方式 � � 感觉 到 的 那么 剧烈 。</span><br><span class="line">31 百年 愚@@ 顽</span><br><span class="line">32 柏林 - - 2008 年 爆发 的 全球 金融 和 经济危机 是 自大 萧条 以来 最 严峻 的 一次 经济 压力 测试 ， 也 是 自 二战 以来 社会 和 政治 制度 所 面临 的 最 严重 挑战 。</span><br></pre></td></tr></table></figure><h3 id="2-2-9-split"><a href="#2-2-9-split" class="headerlink" title="2.2.9 split"></a>2.2.9 split</h3><p>最后，双语文件(clean.zh, clean.en)都需要按比例划分出训练集、测试集、开发集(所以共6个文件，为方便区分，直接以 ‘train.en’, ‘valid.zh’ 这样的格式命名)，附自己写的脚本(~/nmt/utils/split.py)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">Usage:</span><br><span class="line">python split.py src_fpath tgt_fpath new_data_dir</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">def split(src_fpath, tgt_fpath, nsrc&#x3D;&#39;zh&#39;, ntgt&#x3D;&#39;en&#39;, ratio&#x3D;(0.9, 0.05, 0.05), new_data_dir&#x3D;&#39;&#39;):</span><br><span class="line">  src_fp &#x3D; open(src_fpath, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  tgt_fp &#x3D; open(tgt_fpath, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  </span><br><span class="line">  src_train, src_test, src_val &#x3D; open(new_data_dir + &#39;train.&#39; + nsrc, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), \</span><br><span class="line">    open(new_data_dir + &#39;test.&#39; + nsrc, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), open(new_data_dir + &#39;valid.&#39; + nsrc, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  tgt_train, tgt_test, tgt_val &#x3D; open(new_data_dir + &#39;train.&#39; + ntgt, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), \</span><br><span class="line">    open(new_data_dir + &#39;test.&#39; + ntgt, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), open(new_data_dir + &#39;valid.&#39; + ntgt, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  </span><br><span class="line">  src, tgt &#x3D; src_fp.readlines(), tgt_fp.readlines()</span><br><span class="line">  for s, t in zip(src, tgt):</span><br><span class="line">      rand &#x3D; random.random()</span><br><span class="line">      if 0 &lt; rand &lt;&#x3D; ratio[0]:</span><br><span class="line">        src_train.write(s)</span><br><span class="line">        tgt_train.write(t)</span><br><span class="line">      elif ratio[0] &lt; rand &lt;&#x3D; ratio[0] + ratio[1]:</span><br><span class="line">        src_test.write(s)</span><br><span class="line">        tgt_test.write(t)</span><br><span class="line">      else:</span><br><span class="line">        src_val.write(s)</span><br><span class="line">        tgt_val.write(t)</span><br><span class="line">  </span><br><span class="line">  src_fp.close()</span><br><span class="line">  tgt_fp.close()</span><br><span class="line">  src_train.close()</span><br><span class="line">  src_test.close()</span><br><span class="line">  src_val.close()</span><br><span class="line">  tgt_train.close()</span><br><span class="line">  tgt_test.close()</span><br><span class="line">  tgt_val.close()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:      </span><br><span class="line">    split(src_fpath&#x3D;sys.argv[1], tgt_fpath&#x3D;sys.argv[2], nsrc&#x3D;&#39;zh&#39;, ntgt&#x3D;&#39;en&#39;, ratio&#x3D;(0.95, 0.025, 0.025), new_data_dir&#x3D;sys.argv[3])</span><br></pre></td></tr></table></figure><p>使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python $&#123;utils&#125;&#x2F;split.py $&#123;data_dir&#125;&#x2F;clean.zh $&#123;data_dir&#125;&#x2F;clean.en $&#123;data_dir&#125;&#x2F;</span><br></pre></td></tr></table></figure><p>最后，data/v15news目录中有如下数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── test.en</span><br><span class="line">        ├── test.zh</span><br><span class="line">        ├── train.en</span><br><span class="line">        ├── train.zh</span><br><span class="line">        ├── valid.en</span><br><span class="line">        └── valid.zh</span><br></pre></td></tr></table></figure><h1 id="3-训练过程"><a href="#3-训练过程" class="headerlink" title="3 训练过程"></a>3 训练过程</h1><h2 id="3-1-生成词表及二进制文件"><a href="#3-1-生成词表及二进制文件" class="headerlink" title="3.1 生成词表及二进制文件"></a>3.1 生成词表及二进制文件</h2><p>首先用预处理后的六个文件(train.zh, valid.en等)，使用<code>fairseq-preprocess</code>命令生成<strong>词表</strong>和<strong>训练用的二进制文件</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fairseq-preprocess --source-lang $&#123;src&#125; --target-lang $&#123;tgt&#125; \</span><br><span class="line">    --trainpref $&#123;data_dir&#125;&#x2F;train --validpref $&#123;data_dir&#125;&#x2F;valid --testpref $&#123;data_dir&#125;&#x2F;test \</span><br><span class="line">    --destdir $&#123;data_dir&#125;&#x2F;data-bin</span><br></pre></td></tr></table></figure><p>生成的文件都保存在data-bin目录中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        └── data-bin</span><br><span class="line">            ├── dict.zh</span><br><span class="line">            ├── dict.en</span><br><span class="line">            ├── preprocess.log</span><br><span class="line">            ├── train.zh-en.zh.idx</span><br><span class="line">            ...</span><br><span class="line">            └── valid.zh-en.en.bin</span><br></pre></td></tr></table></figure><p>需要提醒的是：训练阶段使用的是<strong>训练集</strong>和<strong>验证集</strong>，解码阶段使用的是<strong>测试集</strong></p><h2 id="3-2-训练"><a href="#3-2-训练" class="headerlink" title="3.2 训练"></a>3.2 训练</h2><p>使用<code>fairseq-train</code>命令进行训练，其中有很多可以自由设置的超参数，比如选择使用什么模型，模型的参数等。其中，<code>--save-dir</code> 这个参数是指每一个epoch结束后模型保存的位置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0,1,2,3 nohup fairseq-train $&#123;data_dir&#125;&#x2F;data-bin --arch transformer \</span><br><span class="line">--source-lang $&#123;src&#125; --target-lang $&#123;tgt&#125;  \</span><br><span class="line">    --optimizer adam  --lr 0.001 --adam-betas &#39;(0.9, 0.98)&#39; \</span><br><span class="line">    --lr-scheduler inverse_sqrt --max-tokens 4096  --dropout 0.3 \</span><br><span class="line">    --criterion label_smoothed_cross_entropy  --label-smoothing 0.1 \</span><br><span class="line">    --max-update 200000  --warmup-updates 4000 --warmup-init-lr &#39;1e-07&#39; \</span><br><span class="line">    --keep-last-epochs 10 --num-workers 8 \</span><br><span class="line">--save-dir $&#123;model_dir&#125;&#x2F;checkpoints &amp;</span><br></pre></td></tr></table></figure><p>我自己训练时是在3块GTX TITAN X卡上跑了6个小时，共跑了49个epoch，但是在第22个epoch的时候已经收敛(只需要看验证集上的ppl的变化即可)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoch 020 | valid on &#39;valid&#39; subset | loss 4.366 | nll_loss 2.652 | ppl 6.29 | wps 50387.3 | wpb 8026 | bsz 299.8 | num_updates 14400 | best_loss 4.366</span><br><span class="line">epoch 021 | valid on &#39;valid&#39; subset | loss 4.36 | nll_loss 2.647 | ppl 6.27 | wps 51992.7 | wpb 8026 | bsz 299.8 | num_updates 15120 | best_loss 4.36</span><br><span class="line">epoch 022 | valid on &#39;valid&#39; subset | loss 4.361 | nll_loss 2.644 | ppl 6.25 | wps 49009.9 | wpb 8026 | bsz 299.8 | num_updates 15840 | best_loss 4.36</span><br><span class="line">epoch 023 | valid on &#39;valid&#39; subset | loss 4.369 | nll_loss 2.65 | ppl 6.28 | wps 51878.9 | wpb 8026 | bsz 299.8 | num_updates 16560 | best_loss 4.36</span><br><span class="line">epoch 023 | valid on &#39;valid&#39; subset | loss 4.369 | nll_loss 2.65 | ppl 6.28 | wps 51878.9 | wpb 8026 | bsz 299.8 | num_updates 16560 | best_loss 4.36</span><br></pre></td></tr></table></figure><p>由于<code>--keep-last-epochs</code>这个参数我设为10，所以我最后10个epoch的模型都保存在以下目录中。此外，还会额外保存效果最好的模型(即第22个epoch)和最后一个模型(即第49个epoch，可以用于下一次训练)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">├── models</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        └── checkpoints</span><br><span class="line">            ├── checkpoint40.pt</span><br><span class="line">            ...</span><br><span class="line">            ├── checkpoint49.pt</span><br><span class="line">            ├── checkpoint_best.pt</span><br><span class="line">            └── checkpoint_last.pt</span><br></pre></td></tr></table></figure><h2 id="3-3-解码"><a href="#3-3-解码" class="headerlink" title="3.3 解码"></a>3.3 解码</h2><p>fairseq中支持两种解码命令<code>generate</code>和<code>interactive</code>。</p><p>其区别很简单，<code>generate</code>使用<strong>二进制文件</strong>，这个二进制文件是在<code>fairseq-preprocess</code>过程生成的，当时提供了一个<code>testpref</code>参数。也就是说测试集的src和tgt都是已获得的，这种场景符合自己在公开的数据集上做实验（如WMT14en-de），需要在论文中报告测试集结果。</p><p>而<code>interactive</code>用于文本文件，也就是说不需要二进制文件，在<code>fairseq-preprocess</code>中也就不需要提供<code>testpref</code>参数。这种场景符合在比赛中，比赛方只提供测试集中的src部分，需要自己来解码得到tgt，并最终提交。</p><h3 id="3-3-1-生成式解码"><a href="#3-3-1-生成式解码" class="headerlink" title="3.3.1 生成式解码"></a>3.3.1 生成式解码</h3><p>使用<code>fairseq-generate</code>命令进行生成式解码(<strong>用于预处理后的二进制文件</strong>)，可以自行选择是否添加<code>--remove-bpe</code>参数，使得在生成时就去掉bpe符号(@@)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fairseq-generate $&#123;data_dir&#125;&#x2F;data-bin \</span><br><span class="line">    --path $&#123;model_dir&#125;&#x2F;checkpoints&#x2F;checkpoint_best.pt \</span><br><span class="line">    --batch-size 128 --beam 8 &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt</span><br></pre></td></tr></table></figure><p>选取一部分结果展示如下 (<strong>S</strong>: 源句子，<strong>T</strong>: 目标句子，<strong>H/D</strong>: 预测的句子及其生成概率的log，句子质量越好，其生成概率越接近1，其log越接近0。<strong>P</strong>: 每一个词的生成概率的log。其中，H=\frac{\sum P}{n}<em>H</em>=<em>n</em>∑<em>P</em>)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">S-537西班牙 的 人权 困境</span><br><span class="line">T-537Spain &amp;apos;s Human-Rights Dilemma</span><br><span class="line">H-537-0.16863664984703064Spain &amp;apos;s Human Rights Quandary</span><br><span class="line">D-537-0.16863664984703064Spain &amp;apos;s Human Rights Quandary</span><br><span class="line">P-537-0.0973 -0.1385 -0.1464 -0.0123 -0.4252 -0.4299 -0.0110 -0.0884</span><br><span class="line"></span><br><span class="line">S-5516这是 不可 接受 的 。</span><br><span class="line">T-5516that is unacceptable .</span><br><span class="line">H-5516-0.35840675234794617this is unacceptable .</span><br><span class="line">D-5516-0.35840675234794617this is unacceptable .</span><br><span class="line">P-5516-0.7625 -0.5517 -0.2005 -0.1513 -0.1261</span><br><span class="line"></span><br><span class="line">S-676与 最初 版本 的 破产法 相 比较 ， 2006 年 的 法律 是 牢牢 扎根 于 市场经济 的 。</span><br><span class="line">T-676compared with the original bankruptcy code , the 2006 code is firmly rooted in the needs of a market economy .</span><br><span class="line">H-676-0.624997079372406in contrast to the original bankruptcy law , the law of 2006 was firmly rooted in the market economy .</span><br><span class="line">D-676-0.624997079372406in contrast to the original bankruptcy law , the law of 2006 was firmly rooted in the market economy .</span><br><span class="line">P-676-1.4995 -0.9434 -0.1292 -0.3479 -0.9758 -0.6600 -0.9037 -0.1836 -0.4983 -1.6406 -0.3142 -0.0344 -0.1685 -1.0289 -1.0286 -0.1917 -1.5369 -0.6586 -0.1119 -0.1333 -0.1361</span><br><span class="line"></span><br><span class="line">S-432用 缅因州 共和党 参议员 苏珊 · 柯林斯 （ Susan Collins ） 的话 说 ， 政府 关门 对 其 缅因州 阿卡迪亚 国家 公园 （ Acadia National Park ） 周边 &quot; 所有 小企业 都 造成 了 伤害 &quot; ， &quot; 这是 完全 错误 的 。 &quot; 是 她 首先 提出 了 和解 协议 纲要 并 送交 参议院 。</span><br><span class="line">T-432in the words of Senator Susan Collins , a Republican from Maine who first put together the outlines of a deal and took it to the Senate floor , the shutdown &quot; hurt all the small businesses &quot; around Acadia National Park in her home state , &quot; and that is plain wrong . &quot;</span><br><span class="line">H-432-0.7003933787345886in the words of Susan Collins , a Republican senator from Maine , it would be a mistake to shut down the government &amp;apos;s &quot; all small business &quot; around the Maine National Park , where she proposed a settlement and delivered it to the Senate .</span><br><span class="line">D-432-0.7003933787345886in the words of Susan Collins , a Republican senator from Maine , it would be a mistake to shut down the government &amp;apos;s &quot; all small business &quot; around the Maine National Park , where she proposed a settlement and delivered it to the Senate .</span><br><span class="line">P-432-1.2762 -0.3546 -0.0142 -0.1261 -0.0058 -0.7617 -0.1695 -0.2992 -0.0777 -0.3016 -0.4818 -0.0061 -0.0308 -0.3509 -2.5533 -1.5254 -0.2761 -1.1667 -0.6169 -0.6285 -1.2463 -0.0973 -1.4414 -0.3324 -0.2302 -0.3312 -0.6847 -1.0005 -0.1812 -2.9048 -0.3072 -1.8045 -0.0473 -0.8421 -0.4715 -0.6841 -1.1902 -1.6192 -0.3370 -2.3317 -0.3701 -0.2508 -3.0284 -0.2336 -1.1318 -0.3904 -0.1124 -0.0262 -0.2203 -0.1480</span><br></pre></td></tr></table></figure><h3 id="3-3-2-交互式解码"><a href="#3-3-2-交互式解码" class="headerlink" title="3.3.2 交互式解码"></a>3.3.2 交互式解码</h3><p>使用<code>fairseq-interactive</code>命令进行交互式解码(<strong>用于文本文件</strong>)。注意其<code>input</code>参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!fairseq-interactive $&#123;data_dir&#125;&#x2F;data-bin \</span><br><span class="line">    --input $&#123;data_dir&#125;&#x2F;test.zh \</span><br><span class="line">    --path $&#123;model_dir&#125;&#x2F;checkpoints&#x2F;checkpoint_best.pt \</span><br><span class="line">    --batch-size 1 --beam 8 --remove-bpe &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt</span><br></pre></td></tr></table></figure><h2 id="3-4-后处理及评价"><a href="#3-4-后处理及评价" class="headerlink" title="3.4 后处理及评价"></a>3.4 后处理及评价</h2><h3 id="3-4-1-抽取译文"><a href="#3-4-1-抽取译文" class="headerlink" title="3.4.1 抽取译文"></a>3.4.1 抽取译文</h3><p>由于解码生成的文件包含大量无关信息，所以需要把<strong>译文</strong>和<strong>正确答案</strong>单独抽取出来，其中predict是译文，answer是正确答案：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grep ^H $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt | cut -f3- &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.bpe.en</span><br><span class="line">grep ^T $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt | cut -f2- &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.bpe.en</span><br></pre></td></tr></table></figure><p>效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># predict.tok.true.bpe.en</span><br><span class="line">the subsidy .</span><br><span class="line">this is unacceptable .</span><br><span class="line">there is even worse .</span><br><span class="line">this must change .</span><br><span class="line"></span><br><span class="line"># answer.tok.true.bpe.en</span><br><span class="line">removal of subsidies .</span><br><span class="line">that is unacceptable .</span><br><span class="line">it gets worse .</span><br><span class="line">this must change .</span><br></pre></td></tr></table></figure><h3 id="3-4-1-去除bpe符号"><a href="#3-4-1-去除bpe符号" class="headerlink" title="3.4.1 去除bpe符号"></a>3.4.1 去除bpe符号</h3><p>有两种方法可以去除bpe符号，第一种是在解码时添加<code>--remove-bpe</code>参数，第二种是使用<code>sed</code>指令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -r &#39;s&#x2F;(@@ )| (@@ ?$)&#x2F;&#x2F;g&#39; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.bpe.en  &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.en</span><br><span class="line">sed -r &#39;s&#x2F;(@@ )| (@@ ?$)&#x2F;&#x2F;g&#39; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.bpe.en  &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.en</span><br></pre></td></tr></table></figure><p>效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># answer.tok.true.bpe.en</span><br><span class="line">a World of Under@@ investment</span><br><span class="line">that needs to change .</span><br><span class="line">revolts of the Righ@@ teous</span><br><span class="line">Russia &amp;apos;s Economic Imperi@@ alism</span><br><span class="line">shock and Pan@@ ic</span><br><span class="line"></span><br><span class="line"># answer.tok.true.en</span><br><span class="line">a World of Underinvestment</span><br><span class="line">that needs to change .</span><br><span class="line">revolts of the Righteous</span><br><span class="line">Russia &amp;apos;s Economic Imperialism</span><br><span class="line">shock and Panic</span><br></pre></td></tr></table></figure><h3 id="3-4-2-detruecase"><a href="#3-4-2-detruecase" class="headerlink" title="3.4.2 detruecase"></a>3.4.2 detruecase</h3><p>需要使用detruecase.perl将文件中的大小写恢复正常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$&#123;DETC&#125; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.en &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.en</span><br><span class="line">$&#123;DETC&#125; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.en &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.en</span><br></pre></td></tr></table></figure><p>效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># predict.tok.true.en</span><br><span class="line">the subsidy .</span><br><span class="line">this is unacceptable .</span><br><span class="line">there is even worse .</span><br><span class="line">this must change .</span><br><span class="line"></span><br><span class="line"># predict.tok.en</span><br><span class="line">The subsidy .</span><br><span class="line">This is unacceptable .</span><br><span class="line">There is even worse .</span><br><span class="line">This must change .</span><br></pre></td></tr></table></figure><h3 id="3-4-3-评价"><a href="#3-4-3-评价" class="headerlink" title="3.4.3 评价"></a>3.4.3 评价</h3><p>1.<strong>multi-bleu</strong>：在detokenize前进行评价</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$&#123;MULTI_BLEU&#125; -lc $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.en &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.en</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BLEU &#x3D; 28.81, 61.8&#x2F;35.4&#x2F;22.8&#x2F;15.2 (BP&#x3D;0.976, ratio&#x3D;0.977, hyp_len&#x3D;187605, ref_len&#x3D;192093)</span><br><span class="line">It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.</span><br></pre></td></tr></table></figure><p>2.<strong>sacrebleu</strong>：在detokenize后进行评价。<a href="https://github.com/mjpost/sacreBLEU">link</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Option 1: Pass the reference file as a positional argument to sacreBLEU</span><br><span class="line">sacrebleu ref.detok.txt -i output.detok.txt -m bleu -b -w 4</span><br><span class="line">20.7965</span><br><span class="line"></span><br><span class="line"># Option 2: Redirect the system into STDIN (Compatible with multi-bleu.perl way of doing things)</span><br><span class="line">cat output.detok.txt | sacrebleu ref.detok.txt -m bleu -b -w 4</span><br><span class="line">20.7965</span><br></pre></td></tr></table></figure><p>3.<strong>mteval-v14</strong>：Usage: <code>$0 -r &lt;ref_file&gt; -s &lt;src_file&gt; -t &lt;tst_file&gt;</code></p><h3 id="3-4-4-detokenize"><a href="#3-4-4-detokenize" class="headerlink" title="3.4.4 detokenize"></a>3.4.4 detokenize</h3><p>最后一步，是使用detokenize.perl得到纯预测文本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$&#123;DETOKENIZER&#125; -l en &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.en &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.en</span><br></pre></td></tr></table></figure><p>效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># predict.tok.en</span><br><span class="line">what &amp;apos;s wrong with protectionism ?</span><br><span class="line">The &quot; establishment &quot; and counterinsurgency strategy , introduced by President Barack Obama &amp;apos;s military surge in 2010 , was intended to reverse the war .</span><br><span class="line"></span><br><span class="line"># predict.en</span><br><span class="line">What&#39;s wrong with protectionism?</span><br><span class="line">The &quot;establishment&quot; and counterinsurgency strategy, introduced by President Barack Obama&#39;s military surge in 2010, was intended to reverse the war.</span><br></pre></td></tr></table></figure><blockquote><p>参考：</p><p> <a href="https://hannlp.github.io/2021-01-16-Use-fairseq-to-train-a-Chinese-English-translation-model-from-scratch/">https://hannlp.github.io/2021-01-16-Use-fairseq-to-train-a-Chinese-English-translation-model-from-scratch/</a></p><p><a href="https://zhuanlan.zhihu.com/p/194176917">https://zhuanlan.zhihu.com/p/194176917</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;中英机器翻译的数据预处理流程，暂存一下看到的优秀博文，过段时间改成自用版本。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="research" scheme="http://example.com/categories/research/"/>
    
    <category term="NMT" scheme="http://example.com/categories/research/NMT/"/>
    
    
  </entry>
  
  <entry>
    <title>latex-tips</title>
    <link href="http://example.com/2021/11/27/latex-tips/"/>
    <id>http://example.com/2021/11/27/latex-tips/</id>
    <published>2021-11-27T15:05:54.000Z</published>
    <updated>2021-11-27T15:33:19.133Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>latex/overleaf一些笔记。</p></blockquote><span id="more"></span><h1 id="图片排版"><a href="#图片排版" class="headerlink" title="图片排版"></a>图片排版</h1><h2 id="宏包："><a href="#宏包：" class="headerlink" title="宏包："></a>宏包：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;graphicx&#125; %插图宏包</span><br><span class="line">\usepackage&#123;subfig&#125; %子图包含宏包，较新</span><br><span class="line">\usepackage&#123;subfigure&#125; %子图包含宏包，较旧</span><br></pre></td></tr></table></figure><h2 id="基本命令："><a href="#基本命令：" class="headerlink" title="基本命令："></a>基本命令：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure&#125;[ht]</span><br><span class="line">\centering</span><br><span class="line">\includegraphics[width&#x3D;10cm]&#123;example.png&#125;</span><br><span class="line">\caption&#123;this is a figure&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><p>\centering表示的是里面紧跟的内容都居中；</p><p>\includegrapics[参数1]{参数2}</p><p>参数1：对图片进行一些调整，例如宽高缩放width=10cm,height=8cm,scale=0.4</p><p>参数2：图片名</p><p>\caption{标题}设置图片的一个编号以及为图片添加标题；</p><h2 id="图片位置："><a href="#图片位置：" class="headerlink" title="图片位置："></a>图片位置：</h2><p>\begin{figure}[参数1]</p><p>参数1：对图片在文中的位置进行设置，h 此处（here）t 页顶（top）b 页底（bottom）p 独立一页（page），<strong>H 固定位置</strong>；</p><h2 id="subfig和subfigure的区别："><a href="#subfig和subfigure的区别：" class="headerlink" title="subfig和subfigure的区别："></a>subfig和subfigure的区别：</h2><p>subfig：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure&#125;[tbp]</span><br><span class="line">\centering</span><br><span class="line">\subfloat[Arabic numerals]&#123;\label&#123;fig:a&#125;\includegraphics[width&#x3D;1in]&#123;placeholder&#125;&#125;\quad</span><br><span class="line">\subfloat[Arabic numerals]&#123;\label&#123;fig:b&#125;\includegraphics[width&#x3D;1in]&#123;placeholder&#125;&#125;\\</span><br><span class="line">\caption&#123;Capital Roman numerals.&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><p>subfigure：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;figure&#125; \centering </span><br><span class="line">\subfigure[figure 1 title.] &#123; \label&#123;fig:a&#125; </span><br><span class="line">\includegraphics[width&#x3D;0.8\columnwidth]&#123;fig1.eps&#125; </span><br><span class="line">&#125; </span><br><span class="line">\subfigure[figure 2 title.] &#123; \label&#123;fig:b&#125; </span><br><span class="line">\includegraphics[width&#x3D;0.8\columnwidth]&#123;fig2.eps&#125; </span><br><span class="line">&#125; </span><br><span class="line">\caption&#123; general title. &#125; </span><br><span class="line">\label&#123;fig&#125; </span><br><span class="line">\end&#123;figure&#125; </span><br></pre></td></tr></table></figure><blockquote><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/143529262">https://zhuanlan.zhihu.com/p/143529262</a></p><p><a href="https://blog.csdn.net/yq_forever/article/details/84796802">https://blog.csdn.net/yq_forever/article/details/84796802</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;latex/overleaf一些笔记。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="other" scheme="http://example.com/categories/other/"/>
    
    
  </entry>
  
  <entry>
    <title>线性规划_GLPK工具</title>
    <link href="http://example.com/2021/11/27/LP-GLPK/"/>
    <id>http://example.com/2021/11/27/LP-GLPK/</id>
    <published>2021-11-27T14:16:28.000Z</published>
    <updated>2021-11-27T15:03:09.043Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>线性规划求解工具GLPK在windows环境下的安装和使用样例。</p></blockquote><span id="more"></span><p>记录一个GLPK工具无需VS编译，直接增加系统路径的方法，简单好用。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>1.从link<a href="https://sourceforge.net/projects/winglpk/%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85%E5%8C%85%E3%80%82">https://sourceforge.net/projects/winglpk/下载安装包。</a><br>2.下载好之后将安装包解压，并将其复制到C盘下。<br>3.进入C:\glpk-4.65\w64或者C:\glpk-4.65\w32，根据自己电脑的系统类型选择打开64位的还是32位的。（查看电脑系统类型的操作：控制面板–系统和安全–系统）<br>4.打开控制面板–系统和安全–系统–高级系统设置-环境变量：编辑系统变量中的Path，将路径C:\glpk-4.65\w64加入Path中。<br>5.检验安装是否成功：在C:\glpk-4.65\w64文件夹下，路径中输入cmd打开命令行窗口，输入glpsol回车，显示版本信息则说明安装成功。</p><h1 id="输入文件-input-mod"><a href="#输入文件-input-mod" class="headerlink" title="输入文件:input.mod"></a>输入文件:input.mod</h1><p>var x1;<br>var x2;<br>var x3;<br>//var x4 binary;布尔变量binary，类似的还有整数integer。</p><p>maximize z: 10 * x1 + 8 * x2 + 16 * x3;</p><p>s.t. con1 : 3 * x1 + 3 * x2 + 2 * x3 &lt;= 200;<br>s.t. con2 : 4 * x1 + 3 * x2 + 7 * x3 &lt;= 300;<br>s.t. con3 : -x1 &lt;= 0;<br>s.t. con4 : -x2 &lt;= 0;<br>s.t. con5 : -x3 &lt;= 0;<br>end;</p><h1 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h1><p>glpsol -m input.mod -o output.sol 　<br>-m filename: 指定描述问题的文件<br>-o filename: 指定输出结果保存在哪个文件</p><h1 id="输出-output-sol"><a href="#输出-output-sol" class="headerlink" title="输出:output.sol"></a>输出:output.sol</h1><p>Problem:    input<br>Rows:       6<br>Columns:    3<br>Non-zeros:  12<br>Status:     OPTIMAL<br>Objective:  z = 746.6666667 (MAXimum)</p><p>   No.   Row name   St   Activity     Lower bound   Upper bound    Marginal</p><hr><pre><code> 1 z            B        746.667                              2 con1         NU           200                         200      0.533333  3 con2         NU           300                         300       2.13333  4 con3         NU             0                          -0      0.133333  5 con4         B       -53.3333                          -0  6 con5         B            -20                          -0 </code></pre><p>   No. Column name  St   Activity     Lower bound   Upper bound    Marginal</p><hr><pre><code> 1 x1           B              0                              2 x2           B        53.3333                              3 x3           B             20                             </code></pre><p>Karush-Kuhn-Tucker optimality conditions:</p><p>KKT.PE: max.abs.err = 0.00e+00 on row 0<br>        max.rel.err = 0.00e+00 on row 0<br>        High quality</p><p>KKT.PB: max.abs.err = 0.00e+00 on row 0<br>        max.rel.err = 0.00e+00 on row 0<br>        High quality</p><p>KKT.DE: max.abs.err = 1.78e-15 on column 1<br>        max.rel.err = 8.35e-17 on column 1<br>        High quality</p><p>KKT.DB: max.abs.err = 0.00e+00 on row 0<br>        max.rel.err = 0.00e+00 on row 0<br>        High quality</p><p>End of output</p><blockquote><p>参考：<a href="https://blog.csdn.net/weixin_42848399/article/details/91654118">https://blog.csdn.net/weixin_42848399/article/details/91654118</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;线性规划求解工具GLPK在windows环境下的安装和使用样例。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="other" scheme="http://example.com/categories/other/"/>
    
    
  </entry>
  
  <entry>
    <title>DP动态规划</title>
    <link href="http://example.com/2021/10/15/oj-DP/"/>
    <id>http://example.com/2021/10/15/oj-DP/</id>
    <published>2021-10-15T04:33:12.000Z</published>
    <updated>2021-10-15T07:58:41.473Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>算法课笔记-动态规划<br>语言: C++</p></blockquote><span id="more"></span><h1 id="中文标题"><a href="#中文标题" class="headerlink" title="中文标题"></a>中文标题</h1><p>内容。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;算法课笔记-动态规划&lt;br&gt;语言: C++&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="code" scheme="http://example.com/categories/code/"/>
    
    <category term="oj" scheme="http://example.com/categories/code/oj/"/>
    
    
  </entry>
  
  <entry>
    <title>DC分治</title>
    <link href="http://example.com/2021/10/15/oj-DC/"/>
    <id>http://example.com/2021/10/15/oj-DC/</id>
    <published>2021-10-15T04:28:54.000Z</published>
    <updated>2021-10-15T04:40:45.048Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>算法课笔记-分治<br>语言：C++</p></blockquote><span id="more"></span><h1 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h1><p>内容。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;算法课笔记-分治&lt;br&gt;语言：C++&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="code" scheme="http://example.com/categories/code/"/>
    
    <category term="oj" scheme="http://example.com/categories/code/oj/"/>
    
    
  </entry>
  
  <entry>
    <title>python笔记</title>
    <link href="http://example.com/2021/05/31/python-note/"/>
    <id>http://example.com/2021/05/31/python-note/</id>
    <published>2021-05-31T09:07:14.000Z</published>
    <updated>2021-06-04T09:42:13.687Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>python3的一些知识点</p></blockquote><span id="more"></span><h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><ul><li><p>函数中的默认参数必须指向不变对象，否则在多次调用时会改变默认值（例如None、str，反例如list）。</p></li><li><p>可变参数：允许不定量个参数输入，在参数前加<em>，传入参数类型是tuple。<br>在输入list/tuple进入可变参数的函数时，可以加上</em>直接传入。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def func(*num):</span><br><span class="line">    pass</span><br><span class="line">a &#x3D; (1,2,3)</span><br><span class="line">func(*a)</span><br></pre></td></tr></table></figure></li><li><p>关键字参数：**来表示，用法同可变参数，区别是传入的参数都为<code>x=y</code>格式，传入后变成一个关键字为x，值为y的字典。<br>可以用作用户可选输入的收集上。</p></li><li><p>命名关键字参数：用<em>或者可变参数</em>x做分隔符，规定必须输入的关键字参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def func(x,y,*,z):</span><br><span class="line">    pass</span><br><span class="line">func(1,2,z&#x3D;4)</span><br></pre></td></tr></table></figure><h1 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h1></li><li><p>左闭右开取值，第三个参数表示间隔</p></li></ul><h1 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h1><ul><li>字典的迭代默认是key，迭代value:<code>for value in d.values()</code>，<br>同时迭代key和value:<code>for k, v in d.items()</code></li></ul><h1 id="生成器generator"><a href="#生成器generator" class="headerlink" title="生成器generator"></a>生成器generator</h1><ul><li><p>把生成式列表的[]改为()，每次调用next(x)计算下一个元素的值，或者用for循环迭代</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">g &#x3D; (x*x for x in range(0,10))</span><br><span class="line">a &#x3D; next(g)</span><br><span class="line">for i in g:</span><br><span class="line">print(i)</span><br></pre></td></tr></table></figure></li><li><p>在函数中添加yield语句，每次调用<code>next()</code>，运行到yield返回，下次调用从yield下继续</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def odd():</span><br><span class="line">print(&quot;step 1&quot;)</span><br><span class="line">yield 1</span><br><span class="line">print(&quot;step 2&quot;)</span><br><span class="line">yield 3</span><br><span class="line">print(&quot;step 3&quot;)</span><br><span class="line">yield 5</span><br><span class="line">o &#x3D; odd()</span><br><span class="line">a &#x3D; next(o) #step 2\n3</span><br></pre></td></tr></table></figure><ul><li>这里要注意for循环调用函数生成器时，函数内的print内容无法输出，可以通过<code>StopIteration</code>错误的value获取。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for i in o:</span><br><span class="line">print(i) #1 3 5</span><br><span class="line">while True:</span><br><span class="line">try:</span><br><span class="line">x &#x3D; next(o)</span><br><span class="line">print(x)</span><br><span class="line">except StopIteration as e:</span><br><span class="line">print(e.value)</span><br><span class="line">break</span><br></pre></td></tr></table></figure></li></ul><h1 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h1><ul><li>判断对象是不是<code>Iterable</code>对象</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from collections.abc import Iterable</span><br><span class="line">isinstance([], Iterable) # True</span><br></pre></td></tr></table></figure><ul><li>可以被<code>next()</code>不断调用并返回下一个值的对象称为迭代器<code>Iterator</code>对象，生成器都是<code>Iterator</code>对象，但是<code>Iterable</code>对象不全是<code>Iterator</code>对象</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from collections.abc import Iterator</span><br><span class="line">isinstance([],Iterator) # False</span><br></pre></td></tr></table></figure><ul><li><code>Iterable</code>对象可以通过使用<code>iter()</code>函数变成<code>Iterator</code>对象，两者的区别是后者是惰性的，可以是无限长度的序列流，无法提前知道长度，前者是有限的，长度可知的。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">isinstance(iter([]), Iterator) # True</span><br></pre></td></tr></table></figure></li></ul><h1 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h1><p>函数式编程的一个特点：允许把函数本身作为参数传入另一个函数，还允许返回一个函数。</p><h2 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h2><ul><li>接收另一个函数作为参数的函数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def add(x, y, f):</span><br><span class="line">return f(x) + f(y)</span><br></pre></td></tr></table></figure><ul><li>map()：接收两个参数，一个函数，一个<code>Iterable</code>对象，对每个对象实施函数，返回结果是一个<code>Iterator</code>对象。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def f(x):</span><br><span class="line">return x*x</span><br><span class="line">r &#x3D; map(f, [1,2,3,4,5,6])</span><br></pre></td></tr></table></figure><ul><li>reduce()：接收两个参数，一个函数和一个序列，功能是把结果继续和序列的下一个元素做累积计算。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#str2int功能实现</span><br><span class="line">from functools import reduce</span><br><span class="line">def fn(x, y):</span><br><span class="line">return x * 10 + y</span><br><span class="line">def char2num(s):</span><br><span class="line">digits &#x3D; &#123;&#39;0&#39;: 0, &#39;1&#39;: 1, &#39;2&#39;: 2, &#39;3&#39;: 3, &#39;4&#39;: 4, &#39;5&#39;: 5, &#39;6&#39;: 6, &#39;7&#39;: 7, &#39;8&#39;: 8, &#39;9&#39;: 9&#125;</span><br><span class="line">return digits[s]</span><br><span class="line">reduce(fn, map(char2num, &#39;13579&#39;)) # 13579</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#利用map和reduce编写一个str2float函数，把字符串&#39;123.456&#39;转换成浮点数123.456</span><br><span class="line">def str2float(s):</span><br><span class="line">    def ten(x, y):</span><br><span class="line">        return x*10+y</span><br><span class="line">    def st(x):</span><br><span class="line">        D &#x3D; &#123;&#39;0&#39;: 0, &#39;1&#39;: 1, &#39;2&#39;: 2, &#39;3&#39;: 3, &#39;4&#39;: 4, &#39;5&#39;: 5, &#39;6&#39;: 6, &#39;7&#39;: 7, &#39;8&#39;: 8, &#39;9&#39;: 9&#125;</span><br><span class="line">        return D[x]</span><br><span class="line">    a, b &#x3D; s.split(&#39;.&#39;)</span><br><span class="line">    num &#x3D; len(b)</span><br><span class="line">    ans1 &#x3D; reduce(ten, list(map(st, a)))</span><br><span class="line">    ans2 &#x3D; reduce(ten, list(map(st, b)))</span><br><span class="line">    ans &#x3D; ans1 +ans2&#x2F;(10**num)</span><br><span class="line">    return ans</span><br><span class="line"></span><br><span class="line">print(&#39;str2float(\&#39;123.456\&#39;) &#x3D;&#39;, str2float(&#39;123.456&#39;))</span><br><span class="line">if abs(str2float(&#39;123.456&#39;) - 123.456) &lt; 0.00001:</span><br><span class="line">    print(&#39;测试成功!&#39;)</span><br><span class="line">else:</span><br><span class="line">    print(&#39;测试失败!&#39;)</span><br></pre></td></tr></table></figure><ul><li>filter():接收两个参数，一个函数，一个序列，根据函数的True/False来决定序列是否保留，返回值是一个<code>Iterator</code>对象<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def not_empty(s):</span><br><span class="line">    return s and s.strip()</span><br><span class="line">x &#x3D; list(filter(not_empty, [&#39;A&#39;, &#39;&#39;, &#39;B&#39;, None, &#39;C&#39;, &#39;  &#39;])) # [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;python3的一些知识点&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="code" scheme="http://example.com/categories/code/"/>
    
    <category term="python" scheme="http://example.com/categories/code/python/"/>
    
    
  </entry>
  
  <entry>
    <title>pytorch笔记</title>
    <link href="http://example.com/2021/05/28/pytorch-note/"/>
    <id>http://example.com/2021/05/28/pytorch-note/</id>
    <published>2021-05-28T08:25:31.000Z</published>
    <updated>2021-05-28T10:06:55.471Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>pytorch-动手学深度学习2021笔记</p></blockquote><span id="more"></span><h1 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h1><h2 id="访问元素"><a href="#访问元素" class="headerlink" title="访问元素"></a>访问元素</h2><p><code>[1,:]</code>其中:表示选择范围，只有:表示全选，与数字结合表示范围。</p><p><code>[::3,::2]</code>其中::表示跳选，每隔x个选择一个。</p><p>张量:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; torch.arange(12) #生成</span><br><span class="line">x.shape #形状</span><br><span class="line">x.numel() #元素总数</span><br><span class="line">x &#x3D; x.reshape(3,4) #变换</span><br><span class="line">x &#x3D; torch.zeros((1,2,3)) #全零</span><br><span class="line">x &#x3D; torch.ones((2,3,4)) #全一</span><br><span class="line">x &#x3D; torch.tensor(list) #根据list生成tensor</span><br><span class="line">+ - * &#x2F; ** #对元素进行运算</span><br><span class="line">torch.exp(x) #指数运算</span><br><span class="line">torch.cat((x,y),dim&#x3D;0) #在第零维拼接，零维是最外侧</span><br><span class="line">x &#x3D;&#x3D; y #构建二元张量，是元素比较的结果</span><br><span class="line">x.sum() #求和，结果是一个元素的tensor</span><br><span class="line">x[0:2,:]&#x3D;12 #区域赋值</span><br><span class="line">x[:]&#x3D;x+y &#x2F; x+&#x3D;y #减少内存开销，前后不变</span><br><span class="line">y &#x3D; x.numpy() #tensor转ndarray</span><br><span class="line">x &#x3D; torch.tensor(y) #ndarray转tensor</span><br><span class="line">x.item() &#x2F; float(x) &#x2F; int(x) #0维张量转标量</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;pytorch-动手学深度学习2021笔记&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="code" scheme="http://example.com/categories/code/"/>
    
    <category term="pytorch" scheme="http://example.com/categories/code/pytorch/"/>
    
    
  </entry>
  
  <entry>
    <title>fairseq踩坑记录</title>
    <link href="http://example.com/2021/05/27/fairseq-tips1/"/>
    <id>http://example.com/2021/05/27/fairseq-tips1/</id>
    <published>2021-05-27T07:42:39.000Z</published>
    <updated>2021-05-27T08:45:45.621Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录一些fairseq中遇到的bug和解决方案。<br>fairseq版本: 0.9.0</p></blockquote><span id="more"></span><h1 id="torch-max返回值"><a href="#torch-max返回值" class="headerlink" title="torch.max返回值"></a>torch.max返回值</h1><p>报错：<code>TypeError: expected Tensor as element 0 in argument 0, but got torch.return_types.max</code></p><p>原因是torch.max(a, dim=)返回值是tuple类型，第一个元素是值，第二个元素是索引，因此只需要返回值的tensor时，torch.max(a, dim=).values即可</p><h1 id="导致维度变化的命令"><a href="#导致维度变化的命令" class="headerlink" title="导致维度变化的命令"></a>导致维度变化的命令</h1><p>torch.max=tf.reduce_sum，都会使维度减一</p><p>a = a[:-1:] 也会使维度减一</p><p>cat维度不变，stack使维度加一，都是tensor拼接的功能</p><h1 id="cpu和gpu上变量的类型不匹配问题"><a href="#cpu和gpu上变量的类型不匹配问题" class="headerlink" title="cpu和gpu上变量的类型不匹配问题"></a>cpu和gpu上变量的类型不匹配问题</h1><p>报错：RuntimeError: expected device cpu but got device cuda:0</p><p>可能出现错误的位置：</p><ul><li><p>等号左边和右边类型不一样</p></li><li><p>运算符左右两端类型不同，例：+ - * /</p></li><li><p>同一个函数内，传入参数的类型不同，例matmul等</p></li></ul><p>把tensor转移到相同的设备上解决问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-&gt;cuda : data.cuda()</span><br><span class="line">-&gt;cpu: data.cpu()</span><br><span class="line">-&gt;numpy：</span><br><span class="line">cuda类型不能直接转numpy 须先转成cpu类型，data.cpu().numpy()</span><br><span class="line">在cuda下训练中的数据不能直接转换为numpy，data.cpu().detach().numpy()</span><br></pre></td></tr></table></figure><blockquote><p>参考: <a href="https://blog.csdn.net/qq_41368074/article/details/105942534">https://blog.csdn.net/qq_41368074/article/details/105942534</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;记录一些fairseq中遇到的bug和解决方案。&lt;br&gt;fairseq版本: 0.9.0&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="code" scheme="http://example.com/categories/code/"/>
    
    <category term="pytorch" scheme="http://example.com/categories/code/pytorch/"/>
    
    <category term="fairseq" scheme="http://example.com/categories/code/pytorch/fairseq/"/>
    
    
  </entry>
  
  <entry>
    <title>hexo-next框架附加功能</title>
    <link href="http://example.com/2021/05/27/hexo-next-tips/"/>
    <id>http://example.com/2021/05/27/hexo-next-tips/</id>
    <published>2021-05-27T06:17:10.000Z</published>
    <updated>2021-05-28T05:10:34.487Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>hexo+next框架博客的一些附加功能记录。<br>hexo: 5.4.0<br>next: 7.8.0</p></blockquote><span id="more"></span><h1 id="版本信息："><a href="#版本信息：" class="headerlink" title="版本信息："></a>版本信息：</h1><p>hexo和next版本信息可在相应文件夹的package.json中搜索version查看。</p><h1 id="版权声明"><a href="#版权声明" class="headerlink" title="版权声明:"></a>版权声明:</h1><p>在.\themes\next\layout_macro\目录下，新建my-copyright.swig文件，内容为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if page.copyright %&#125;</span><br><span class="line">&lt;div class&#x3D;&quot;my_post_copyright&quot;&gt;</span><br><span class="line">  &lt;script src&#x3D;&quot;&#x2F;&#x2F;cdn.bootcss.com&#x2F;clipboard.js&#x2F;1.5.10&#x2F;clipboard.min.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- JS库 sweetalert 可修改路径 --&gt;</span><br><span class="line">  &lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;http:&#x2F;&#x2F;jslibs.wuxubj.cn&#x2F;sweetalert_mini&#x2F;jquery-1.7.1.min.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">  &lt;script src&#x3D;&quot;http:&#x2F;&#x2F;jslibs.wuxubj.cn&#x2F;sweetalert_mini&#x2F;sweetalert.min.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">  &lt;link rel&#x3D;&quot;stylesheet&quot; type&#x3D;&quot;text&#x2F;css&quot; href&#x3D;&quot;http:&#x2F;&#x2F;jslibs.wuxubj.cn&#x2F;sweetalert_mini&#x2F;sweetalert.mini.css&quot;&gt;</span><br><span class="line"></span><br><span class="line">  &lt;p&gt;&lt;span&gt;本文标题:&lt;&#x2F;span&gt;&#123;&#123; page.title &#125;&#125;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;本文作者:&lt;&#x2F;span&gt;xxx&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;本文链接:&lt;&#x2F;span&gt;&lt;a href&#x3D;&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot; title&#x3D;&quot;&#123;&#123; page.title &#125;&#125;&quot;&gt;&#123;&#123; page.permalink &#125;&#125;&lt;&#x2F;a&gt;</span><br><span class="line">    &lt;span class&#x3D;&quot;copy-path&quot;  title&#x3D;&quot;点击复制文章链接&quot;&gt;&lt;i class&#x3D;&quot;fa fa-clipboard&quot; data-clipboard-text&#x3D;&quot;&#123;&#123; page.permalink &#125;&#125;&quot;  aria-label&#x3D;&quot;复制成功！&quot;&gt;&lt;&#x2F;i&gt;&lt;&#x2F;span&gt;</span><br><span class="line">  &lt;&#x2F;p&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;版权声明:&lt;&#x2F;span&gt;本博客所有文章除特别声明外，均采用&lt;a rel&#x3D;&quot;license&quot; href&#x3D;&quot;https:&#x2F;&#x2F;creativecommons.org&#x2F;licenses&#x2F;by-nc-nd&#x2F;4.0&#x2F;&quot; target&#x3D;&quot;_blank&quot; title&#x3D;&quot;Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)&quot;&gt;CC BY-NC-ND&lt;&#x2F;a&gt;许可协议。转载请注明出处！&lt;&#x2F;p&gt;  </span><br><span class="line">&lt;&#x2F;div&gt;</span><br><span class="line">&lt;script&gt; </span><br><span class="line">    var clipboard &#x3D; new Clipboard(&#39;.fa-clipboard&#39;);</span><br><span class="line">    clipboard.on(&#39;success&#39;, $(function()&#123;</span><br><span class="line">      $(&quot;.fa-clipboard&quot;).click(function()&#123;</span><br><span class="line">        swal(&#123;   </span><br><span class="line">          title: &quot;&quot;,   </span><br><span class="line">          text: &#39;复制成功&#39;,   </span><br><span class="line">          html: false,</span><br><span class="line">          timer: 500,   </span><br><span class="line">          showConfirmButton: false</span><br><span class="line">        &#125;);</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;));  </span><br><span class="line">&lt;&#x2F;script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>打开.\themes\next\layout_macro\post.swig文件，在</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;#####################&#125;</span><br><span class="line">&#123;### END POST BODY ###&#125;</span><br><span class="line">&#123;#####################&#125;</span><br></pre></td></tr></table></figure><p>之后添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--添加版权信息--&gt;</span><br><span class="line">&lt;div&gt;</span><br><span class="line">    &#123;% if not is_index %&#125;</span><br><span class="line">    &#123;% include &#39;my-copyright.swig&#39; %&#125;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure><p>在.\themes\next\source\css_common\components\post\目录下，新建my-post-copyright.styl文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">.my_post_copyright &#123;</span><br><span class="line">  width: 85%;</span><br><span class="line">  max-width: 45em;</span><br><span class="line">  margin: 2.8em auto 0;</span><br><span class="line">  padding: 0.5em 1.0em;</span><br><span class="line">  border: 1px solid #d3d3d3;</span><br><span class="line">  font-size: 0.93rem;</span><br><span class="line">  line-height: 1.6em;</span><br><span class="line">  word-break: break-all;</span><br><span class="line">  background: rgba(255,255,255,0.4);</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright p&#123;margin:0;&#125;</span><br><span class="line">.my_post_copyright span &#123;</span><br><span class="line">  display: inline-block;</span><br><span class="line">  width: 5.2em;</span><br><span class="line">  color: #333333; &#x2F;&#x2F; title color</span><br><span class="line">  font-weight: bold;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .raw &#123;</span><br><span class="line">  margin-left: 1em;</span><br><span class="line">  width: 5em;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright a &#123;</span><br><span class="line">  color: #808080;</span><br><span class="line">  border-bottom:0;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright a:hover &#123;</span><br><span class="line">  color: #0593d3; &#x2F;&#x2F; link color</span><br><span class="line">  text-decoration: underline;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright:hover .fa-clipboard &#123;</span><br><span class="line">  color: #000;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .post-url:hover &#123;</span><br><span class="line">  font-weight: normal;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .copy-path &#123;</span><br><span class="line">  margin-left: 1em;</span><br><span class="line">  width: 1em;</span><br><span class="line">  +mobile()&#123;display:none;&#125;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .copy-path:hover &#123;</span><br><span class="line">  color: #808080;</span><br><span class="line">  cursor: pointer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打开.\themes\next\source\css_common\components\post\post.styl文件，在最后添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@import &quot;my-post-copyright&quot;</span><br></pre></td></tr></table></figure><p>打开.\scaffolds\post.md文件，设置新文件开启版权声明</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">copyright: true #开启</span><br><span class="line">---</span><br></pre></td></tr></table></figure><blockquote><p>参考：<a href="https://blog.csdn.net/u011236348/article/details/88169271">https://blog.csdn.net/u011236348/article/details/88169271</a></p></blockquote><h1 id="访客-阅读量统计"><a href="#访客-阅读量统计" class="headerlink" title="访客/阅读量统计"></a>访客/阅读量统计</h1><p>打开.\themes\next_config.yml文件，<br>搜索busuanzi_count关键字，把enable设置为true，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Show Views &#x2F; Visitors of the website &#x2F; page with busuanzi.</span><br><span class="line"># Get more information on http:&#x2F;&#x2F;ibruce.info&#x2F;2015&#x2F;04&#x2F;04&#x2F;busuanzi</span><br><span class="line">busuanzi_count:</span><br><span class="line">  enable: true</span><br><span class="line">  total_visitors: true #访客数</span><br><span class="line">  total_visitors_icon: fa fa-user</span><br><span class="line">  total_views: true #访问数</span><br><span class="line">  total_views_icon: fa fa-eye</span><br><span class="line">  post_views: true #文章阅读量</span><br><span class="line">  post_views_icon: fa fa-eye</span><br></pre></td></tr></table></figure><p>同一文件，搜索footer关键字，在其底下添加counter，设值为true。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup. If not defined, current year will be used.</span><br><span class="line">  #since: 2015</span><br><span class="line">  # count for visit</span><br><span class="line">  counter: true</span><br></pre></td></tr></table></figure><p>打开.\themes\next\layout_partials\footer.swig文件，添加代码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if theme.footer.counter %&#125;</span><br><span class="line">    &lt;script async src&#x3D;&quot;&#x2F;&#x2F;dn-lbstatics.qbox.me&#x2F;busuanzi&#x2F;2.3&#x2F;busuanzi.pure.mini.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>访客、访问次数在网站主页底部，文章阅读量在文章开头。</p><blockquote><p>参考：<a href="https://blog.csdn.net/baidu_34310405/article/details/102665373">https://blog.csdn.net/baidu_34310405/article/details/102665373</a></p></blockquote><h1 id="博客中插入自己编写的html页面"><a href="#博客中插入自己编写的html页面" class="headerlink" title="博客中插入自己编写的html页面"></a>博客中插入自己编写的html页面</h1><p>看到next官方readme中LEAFERx博客sentence页面很好看，也动手搞一个类似的。利用jQuery的全屏滚动插件fullPage.js实现，网页部分主要参考链接中的代码。</p><p>将自己的html页面插入博客中，首先新建一个页面<code>hexo new page &quot;schedule&quot;</code>，在生成的文件夹source/schedule/下复制网页的css/js/html文件。</p><p>修改根目录下的<code>_config.yml</code>文件，跳过自定义页面的渲染过程，若文件夹下有子文件夹，需要改为<code>schedule/**</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">skip_render:</span><br><span class="line"> - &quot;schedule&#x2F;*&quot;</span><br></pre></td></tr></table></figure><blockquote><p>参考：<a href="https://www.dowebok.com/77.html">https://www.dowebok.com/77.html</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;hexo+next框架博客的一些附加功能记录。&lt;br&gt;hexo: 5.4.0&lt;br&gt;next: 7.8.0&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="other" scheme="http://example.com/categories/other/"/>
    
    <category term="hexo" scheme="http://example.com/categories/other/hexo/"/>
    
    <category term="next" scheme="http://example.com/categories/other/hexo/next/"/>
    
    
  </entry>
  
  <entry>
    <title>tf-tensor变换</title>
    <link href="http://example.com/2021/05/27/tf-tensor-conversion/"/>
    <id>http://example.com/2021/05/27/tf-tensor-conversion/</id>
    <published>2021-05-27T05:53:10.000Z</published>
    <updated>2021-05-27T08:39:45.786Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>tensorflow中tensor变换的函数汇总。</p></blockquote><span id="more"></span><p><code>tf.stack</code><br>list-&gt;tensor<br>（tf.convert_to_tensor）</p><p><code>tf.unstack</code><br>tensor-&gt;list</p><p><code>tf.expand_dims</code><br>扩展维度</p><p><code>reduce_sum/max/mean</code><br>减少维度</p><p><code>tf.reshape</code><br>改变各维度大小（可增加减少维度）</p><p><code>tf.transpose</code><br>调换维度顺序</p><p><code>tf.tile</code><br>不改变维度的复制</p><p><code>tf.concat</code><br>不改变维度的拼接</p><p><code>tf.broadcast_to</code><br>广播</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;tensorflow中tensor变换的函数汇总。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="code" scheme="http://example.com/categories/code/"/>
    
    <category term="tensorflow" scheme="http://example.com/categories/code/tensorflow/"/>
    
    
  </entry>
  
  <entry>
    <title>github博客常用命令</title>
    <link href="http://example.com/2021/05/27/blog-instructions/"/>
    <id>http://example.com/2021/05/27/blog-instructions/</id>
    <published>2021-05-27T04:00:45.000Z</published>
    <updated>2021-05-27T08:43:25.186Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>汇总一些博客常用命令和备份博客的方法。</p></blockquote><span id="more"></span><h1 id="新建："><a href="#新建：" class="headerlink" title="新建："></a>新建：</h1><p><code>hexo init [folder]</code><br>新建网站，有folder就新建一个名为folder的文件夹并将网站存在该文件夹下，没有folder就将网站存在当前目录下。</p><p><code>hexo new [layout] “title”</code><br>新建文章，如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替，文章标题用双引号括起来。</p><h1 id="部署更改"><a href="#部署更改" class="headerlink" title="部署更改:"></a>部署更改:</h1><p><code>hexo clean</code><br>清除缓存文件(db.json)和已生成的静态文件(public)，当更改不生效时可以使用该命令。</p><p><code>hexo g / hexo generate</code><br>生成静态文件，之后可启动服务器或者部署网站。</p><p><code>hexo s / hexo server</code><br>启动服务器，可访问<a href="http://localhost:4000/%E6%9F%A5%E7%9C%8B%E3%80%82">http://localhost:4000/查看。</a></p><p><code>hexo d / hexo deploy</code><br>部署网站，可访问网站链接查看。</p><h1 id="上传文件："><a href="#上传文件：" class="headerlink" title="上传文件："></a>上传文件：</h1><p>在github上code下载处复制<a href="https://github.com/xxx.git">https://github.com/xxx.git</a> 的链接，<br>在本地git bash中输入<code>git clone https://github.com/xxx.git</code> 克隆仓库到本地，<br>将需要上传的文件复制到本地生成的文件夹中，输入命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add . #将文件夹下的所有文件都添加进来</span><br><span class="line">git commit  -m  &quot;first commit&quot;  #“first commit”可替换成任意需要的注释信息</span><br><span class="line">git push -u origin main  #将本地仓库push到github上面，2020年10月1日之后，github新创建的仓库默认分支都将使用main，而不是之前的master，要注意。</span><br></pre></td></tr></table></figure><h1 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h1><p>采用同上传文件的方式来备份网站源文件，在github上建立一个private仓库，将网站的文件夹整体移动到克隆到本地的备份文件夹中，每隔一段时间上传最新的文件来备份网站。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit  -m  &quot;backup&quot;</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;汇总一些博客常用命令和备份博客的方法。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="other" scheme="http://example.com/categories/other/"/>
    
    <category term="github" scheme="http://example.com/categories/other/github/"/>
    
    
  </entry>
  
  <entry>
    <title>fairseq代码位置记录</title>
    <link href="http://example.com/2021/05/21/fairseq-tips/"/>
    <id>http://example.com/2021/05/21/fairseq-tips/</id>
    <published>2021-05-21T10:59:52.000Z</published>
    <updated>2021-05-27T08:42:15.500Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录一些fairseq中功能实现的代码位置。<br>fairseq版本: 0.9.0</p></blockquote><span id="more"></span><h1 id="ensemble集成"><a href="#ensemble集成" class="headerlink" title="ensemble集成"></a>ensemble集成</h1><p>在fairseq/sequence_generator.py中760行，ensemble在beam search之前。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;记录一些fairseq中功能实现的代码位置。&lt;br&gt;fairseq版本: 0.9.0&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="code" scheme="http://example.com/categories/code/"/>
    
    <category term="pytorch" scheme="http://example.com/categories/code/pytorch/"/>
    
    <category term="fairseq" scheme="http://example.com/categories/code/pytorch/fairseq/"/>
    
    
  </entry>
  
</feed>

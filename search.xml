<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>线性规划_GLPK工具</title>
    <url>/2021/11/27/LP-GLPK/</url>
    <content><![CDATA[<blockquote>
<p>线性规划求解工具GLPK在windows环境下的安装和使用样例。</p>
</blockquote>
<span id="more"></span>

<p>记录一个GLPK工具无需VS编译，直接增加系统路径的方法，简单好用。</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>1.从link<a href="https://sourceforge.net/projects/winglpk/%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85%E5%8C%85%E3%80%82">https://sourceforge.net/projects/winglpk/下载安装包。</a><br>2.下载好之后将安装包解压，并将其复制到C盘下。<br>3.进入C:\glpk-4.65\w64或者C:\glpk-4.65\w32，根据自己电脑的系统类型选择打开64位的还是32位的。（查看电脑系统类型的操作：控制面板–系统和安全–系统）<br>4.打开控制面板–系统和安全–系统–高级系统设置-环境变量：编辑系统变量中的Path，将路径C:\glpk-4.65\w64加入Path中。<br>5.检验安装是否成功：在C:\glpk-4.65\w64文件夹下，路径中输入cmd打开命令行窗口，输入glpsol回车，显示版本信息则说明安装成功。</p>
<h1 id="输入文件-input-mod"><a href="#输入文件-input-mod" class="headerlink" title="输入文件:input.mod"></a>输入文件:input.mod</h1><p>var x1;<br>var x2;<br>var x3;<br>//var x4 binary;布尔变量binary，类似的还有整数integer。</p>
<p>maximize z: 10 * x1 + 8 * x2 + 16 * x3;</p>
<p>s.t. con1 : 3 * x1 + 3 * x2 + 2 * x3 &lt;= 200;<br>s.t. con2 : 4 * x1 + 3 * x2 + 7 * x3 &lt;= 300;<br>s.t. con3 : -x1 &lt;= 0;<br>s.t. con4 : -x2 &lt;= 0;<br>s.t. con5 : -x3 &lt;= 0;<br>end;</p>
<h1 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h1><p>glpsol -m input.mod -o output.sol 　<br>-m filename: 指定描述问题的文件<br>-o filename: 指定输出结果保存在哪个文件</p>
<h1 id="输出-output-sol"><a href="#输出-output-sol" class="headerlink" title="输出:output.sol"></a>输出:output.sol</h1><p>Problem:    input<br>Rows:       6<br>Columns:    3<br>Non-zeros:  12<br>Status:     OPTIMAL<br>Objective:  z = 746.6666667 (MAXimum)</p>
<p>   No.   Row name   St   Activity     Lower bound   Upper bound    Marginal</p>
<hr>
<pre><code> 1 z            B        746.667                             
 2 con1         NU           200                         200      0.533333 
 3 con2         NU           300                         300       2.13333 
 4 con3         NU             0                          -0      0.133333 
 5 con4         B       -53.3333                          -0 
 6 con5         B            -20                          -0 
</code></pre>
<p>   No. Column name  St   Activity     Lower bound   Upper bound    Marginal</p>
<hr>
<pre><code> 1 x1           B              0                             
 2 x2           B        53.3333                             
 3 x3           B             20                             
</code></pre>
<p>Karush-Kuhn-Tucker optimality conditions:</p>
<p>KKT.PE: max.abs.err = 0.00e+00 on row 0<br>        max.rel.err = 0.00e+00 on row 0<br>        High quality</p>
<p>KKT.PB: max.abs.err = 0.00e+00 on row 0<br>        max.rel.err = 0.00e+00 on row 0<br>        High quality</p>
<p>KKT.DE: max.abs.err = 1.78e-15 on column 1<br>        max.rel.err = 8.35e-17 on column 1<br>        High quality</p>
<p>KKT.DB: max.abs.err = 0.00e+00 on row 0<br>        max.rel.err = 0.00e+00 on row 0<br>        High quality</p>
<p>End of output</p>
<blockquote>
<p>参考：<a href="https://blog.csdn.net/weixin_42848399/article/details/91654118">https://blog.csdn.net/weixin_42848399/article/details/91654118</a></p>
</blockquote>
]]></content>
      <categories>
        <category>other</category>
      </categories>
  </entry>
  <entry>
    <title>github博客常用命令</title>
    <url>/2021/05/27/blog-instructions/</url>
    <content><![CDATA[<blockquote>
<p>汇总一些博客常用命令和备份博客的方法。</p>
</blockquote>
<span id="more"></span>

<h1 id="新建："><a href="#新建：" class="headerlink" title="新建："></a>新建：</h1><p><code>hexo init [folder]</code><br>新建网站，有folder就新建一个名为folder的文件夹并将网站存在该文件夹下，没有folder就将网站存在当前目录下。</p>
<p><code>hexo new [layout] “title”</code><br>新建文章，如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替，文章标题用双引号括起来。</p>
<h1 id="部署更改"><a href="#部署更改" class="headerlink" title="部署更改:"></a>部署更改:</h1><p><code>hexo clean</code><br>清除缓存文件(db.json)和已生成的静态文件(public)，当更改不生效时可以使用该命令。</p>
<p><code>hexo g / hexo generate</code><br>生成静态文件，之后可启动服务器或者部署网站。</p>
<p><code>hexo s / hexo server</code><br>启动服务器，可访问<a href="http://localhost:4000/%E6%9F%A5%E7%9C%8B%E3%80%82">http://localhost:4000/查看。</a></p>
<p><code>hexo d / hexo deploy</code><br>部署网站，可访问网站链接查看。</p>
<h1 id="上传文件："><a href="#上传文件：" class="headerlink" title="上传文件："></a>上传文件：</h1><p>在github上code下载处复制<a href="https://github.com/xxx.git">https://github.com/xxx.git</a> 的链接，<br>在本地git bash中输入<code>git clone https://github.com/xxx.git</code> 克隆仓库到本地，<br>将需要上传的文件复制到本地生成的文件夹中，输入命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add . #将文件夹下的所有文件都添加进来</span><br><span class="line">git commit  -m  &quot;first commit&quot;  #“first commit”可替换成任意需要的注释信息</span><br><span class="line">git push -u origin main  #将本地仓库push到github上面，2020年10月1日之后，github新创建的仓库默认分支都将使用main，而不是之前的master，要注意。</span><br></pre></td></tr></table></figure>

<h1 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h1><p>采用同上传文件的方式来备份网站源文件，在github上建立一个private仓库，将网站的文件夹整体移动到克隆到本地的备份文件夹中，每隔一段时间上传最新的文件来备份网站。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit  -m  &quot;backup&quot;</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>other</category>
        <category>github</category>
      </categories>
  </entry>
  <entry>
    <title>linux环境搭建</title>
    <url>/2022/03/19/environment/</url>
    <content><![CDATA[<blockquote>
<p>linux上环境搭建记录（gpu）。</p>
</blockquote>
<span id="more"></span>

<h1 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h1><p>anaconda安装</p>
<p>pytorch（cuda版本）安装</p>
<p>fairseq安装</p>
<p>huggingface安装</p>
<p>requirements.txt生成</p>
<h1 id="1-anaconda安装"><a href="#1-anaconda安装" class="headerlink" title="1 anaconda安装"></a>1 anaconda安装</h1><p>在<a href="https://www.anaconda.com/products/individual">anaconda官网</a>上找到符合系统和配置的版本，复制下载链接，使用wget命令安装：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>

<p>执行安装程序：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bash Anaconda3-2021.11-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>

<p>中间过程选择yes，将conda加入环境变量，结束后输入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda -V</span><br></pre></td></tr></table></figure>

<p>如出现版本号说明安装成功。</p>
<p>创建虚拟环境，为配置特定环境做准备：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create -n envname python&#x3D;3.x</span><br></pre></td></tr></table></figure>



<p><em>tips</em>：</p>
<p>查看linux的系统构架的命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">~$</span><span class="bash"> arch</span></span><br></pre></td></tr></table></figure>



<h1 id="2-cuda安装"><a href="#2-cuda安装" class="headerlink" title="2 cuda安装"></a>2 cuda安装</h1><p><strong>注：</strong>在进行3步骤安装cuda版本的pytorch时，一直没有成功，一开始以为是cuda的问题，找到了这个解决方案，但是后续发现是第三步命令有问题，在第三步可以解决，所以可以不进行这一步。</p>
<p>0 查看当前显卡可安装的cuda版本：</p>
<p>可以查看当前NVIDIA驱动的版本，还可以查询与此驱动相匹配的Cuda版本，虽是匹配，但是Cuda的版本可以略低于此时驱动匹配的Cuda版本，因此，我们可以安装版本高一点的驱动，来兼容不同版本的Cuda。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>

<p>为没有super权限的用户安装cuda的步骤如下：</p>
<p>1 首先在<a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA Toolkit Archive | NVIDIA Developer</a>下载对应的安装包：</p>
<p>linux x86_64 ubuntu18.04 runfile(建议用此文件格式） </p>
<p>2 对安装包属性进行修改为可执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod 755 cuda_11.3.0_465.19.01_linux.run</span><br></pre></td></tr></table></figure>

<p>2 进行安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh cuda_11.3.0_465.19.01_linux.run</span><br></pre></td></tr></table></figure>

<p>3 安装进程</p>
<p>accept，在CUDAinstall界面只选中“CUDA Toolkit 10.1”（前面带X默认选中）。选中“CUDA Toolkit 10.1”，按A进入，选中“Change Toolkit Install Path”，enter进入，将路径设置为非root路径，例如”/data0/xxx/local/cuda10.1/“，“finish”退出，“Done”退出.</p>
<p>4 安装成功：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&#x3D; Summary &#x3D;</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">Driver:   Not Selected</span><br><span class="line">Toolkit:  Installed in &#x2F;data&#x2F;yyfxu&#x2F;10-cuda&#x2F;cuda-11.3&#x2F;</span><br><span class="line">Samples:  Not Selected</span><br><span class="line"></span><br><span class="line">Please make sure that</span><br><span class="line"> -   PATH includes &#x2F;data&#x2F;yyfxu&#x2F;10-cuda&#x2F;cuda-11.3&#x2F;bin</span><br><span class="line"> -   LD_LIBRARY_PATH includes &#x2F;data&#x2F;yyfxu&#x2F;10-cuda&#x2F;cuda-11.3&#x2F;lib64, or, add &#x2F;data&#x2F;yyfxu&#x2F;10-cuda&#x2F;cuda-11.3&#x2F;lib64 to &#x2F;etc&#x2F;ld.so.conf and run ldconfig as root</span><br><span class="line"></span><br><span class="line">To uninstall the CUDA Toolkit, run cuda-uninstaller in &#x2F;data&#x2F;yyfxu&#x2F;10-cuda&#x2F;cuda-11.3&#x2F;bin</span><br><span class="line">***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 465.00 is required for CUDA 11.3 functionality to work.</span><br><span class="line">To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file:</span><br><span class="line">    sudo &lt;CudaInstaller&gt;.run --silent --driver</span><br><span class="line"></span><br><span class="line">Logfile is &#x2F;tmp&#x2F;cuda-installer.log</span><br></pre></td></tr></table></figure>

<p>5 修改环境变量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure>

<p>跳转到最后，添加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export CUDA_HOME&#x3D;&#x2F;data0&#x2F;xuyangbin&#x2F;local&#x2F;cuda10.1$&#123;CUDA_HOME:+:$&#123;CUDA_HOME&#125;&#125; </span><br><span class="line">export PATH&#x3D;&#x2F;data0&#x2F;xuyangbin&#x2F;local&#x2F;cuda10.1&#x2F;bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;&#x2F;data0&#x2F;xuyangbin&#x2F;local&#x2F;cuda10.1&#x2F;lib64&#x2F;$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">export LIBRARY_PATH&#x3D;&#x2F;data0&#x2F;xuyangbin&#x2F;local&#x2F;cuda10.1&#x2F;lib64$&#123;LIBRARY_PATH:+:$&#123;LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure>

<p>生效：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure>

<p>确认成功（在super没有安装nvcc的情况下无法显示）：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nvcc --version</span><br><span class="line"><span class="meta">#</span><span class="bash">cat /usr/<span class="built_in">local</span>/cuda/version.txt <span class="comment">#也可以用这条命令</span></span></span><br></pre></td></tr></table></figure>

<p>这两条命令是查看当前Cuda的版本，即实际安装的Cuda版本。</p>
<blockquote>
<p>参考：</p>
<p><a href="https://blog.csdn.net/qq_46277596/article/details/122332496">https://blog.csdn.net/qq_46277596/article/details/122332496</a></p>
<p><a href="https://blog.csdn.net/m0_67401545/article/details/126434430">(1条消息) Linux查看当前Cuda（CUDA Toolkit ）版本_普通网友的博客-CSDN博客_cudatoolkit版本怎么看</a></p>
</blockquote>
<h1 id="3-pytorch安装"><a href="#3-pytorch安装" class="headerlink" title="3 pytorch安装"></a>3 pytorch安装</h1><p>在<a href="https://pytorch.org/get-started/locally/">pytorch官网</a>根据系统和版本复制安装命令，如果首页没有所需版本可在“You can also <a href="https://pytorch.org/get-started/previous-versions">install previous versions of PyTorch</a>“处查找。</p>
<p><strong>注：</strong>在最近一次安装环境的过程中，发现conda install会安装成cpu版本，用pip install命令可以。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> CUDA 11.3</span></span><br><span class="line">pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113</span><br></pre></td></tr></table></figure>

<p>测试是否安装成功：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">flag = torch.cuda.is_available()</span><br><span class="line">print(flag)</span><br><span class="line">ngpu= 1</span><br><span class="line">device = torch.device(&quot;cuda:0&quot; if (torch.cuda.is_available() and ngpu &gt; 0) else &quot;cpu&quot;)</span><br><span class="line">print(device)</span><br><span class="line">print(torch.cuda.get_device_name(0))</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">True</span><br><span class="line">cuda:0</span><br><span class="line">NVIDIA GeForce RTX 3090</span><br></pre></td></tr></table></figure>



<h1 id="4-huggingface安装"><a href="#4-huggingface安装" class="headerlink" title="4 huggingface安装"></a>4 huggingface安装</h1><p>此处选择源码方式安装huggingface便于自行修改，其它安装方式在<a href="https://huggingface.co/docs/transformers/installation">官网</a>中有介绍。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers.git</span><br><span class="line">cd transformers</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure>

<p><em>tips</em>：</p>
<p><code>pip install -e</code>的作用是添加非官方的模块，先下载源码，进入源码目录，并执行该命令可解决<code>No module named &#39;XXX&#39;</code>的报错，它进行的操作有：</p>
<ul>
<li>安装site-packages/PackageName.egg-link文件</li>
<li>添加路径 site-packages/easy-install.pth</li>
</ul>
<h1 id="5-清华源加速"><a href="#5-清华源加速" class="headerlink" title="5 清华源加速"></a>5 清华源加速</h1><p>pip速度慢或出现超时错误时，考虑换成清华源下载包。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple xxx</span><br></pre></td></tr></table></figure>



<h1 id="6-requirements-txt生成"><a href="#6-requirements-txt生成" class="headerlink" title="6 requirements.txt生成"></a>6 requirements.txt生成</h1><p>5.1 生成当前环境下的所有依赖：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure>

<p>5.2 生成指定项目下的所有依赖：</p>
<p><em>tips：根据import，即项目引入了哪些包才会写进requirements.txt中，相比前一种方法更简洁，但可能包含的包不全。</em></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install pipreqs</span><br><span class="line">pipreqs ./ [--encoding=utf-8]</span><br></pre></td></tr></table></figure>

<p>5.3 安装requirements.txt要求的所有模块:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install -r requirements.txt [-i HTTPS://mirrors.aliyun.com/pypi/simple/]</span><br></pre></td></tr></table></figure>



<h1 id="7-环境复制"><a href="#7-环境复制" class="headerlink" title="7 环境复制"></a>7 环境复制</h1><h2 id="从本机到本机的环境复制"><a href="#从本机到本机的环境复制" class="headerlink" title="从本机到本机的环境复制"></a>从本机到本机的环境复制</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n new_env  --clone  exist_env</span><br></pre></td></tr></table></figure>

<h2 id="从一台服务器到另一台服务器的环境复制"><a href="#从一台服务器到另一台服务器的环境复制" class="headerlink" title="从一台服务器到另一台服务器的环境复制"></a>从一台服务器到另一台服务器的环境复制</h2><p><strong>第一种方法</strong>（自己尝试的时候报错未解决）：</p>
<p>在本机上先激活环境，并导出配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda activate exist_env</span><br><span class="line">conda env export &gt; environment.yml</span><br></pre></td></tr></table></figure>

<p>在目标服务器上按前面的步骤安装好anaconda后，复制配置文件到目标服务器目录中，在base环境下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda env create -f environment.yml</span><br></pre></td></tr></table></figure>

<p><strong>第二种方法</strong>（可行）：</p>
<p>先克隆一份原始环境在本地，激活环境，并下载conda-pack</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n new_env  --clone  exist_env</span><br><span class="line">conda activate new_env</span><br><span class="line">conda install -c conda-forge conda-pack </span><br></pre></td></tr></table></figure>

<p>打包环境，注意可能有editable-packages无法打包导致报错，需要忽略（忽略的部分需要重新安装，怎样列出当前环境下的editable-packages暂不清楚，需要自己注意记一下）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda pack -n new_env -o new_env.tar.gz --ignore-editable-packages</span><br></pre></td></tr></table></figure>

<p>讲打包好的tar.gz文件传到目标服务器上，在<code>anaconda3/envs</code>下创建一个新环境名称的文件夹，把环境打包文件解压到这个文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd anaconda3/envs</span><br><span class="line">mkdir new_env</span><br><span class="line">tar -xzvf new_env.tar.gz -C anaconda3/envs/new_env</span><br></pre></td></tr></table></figure>

<p>查看结果，发现环境已经复制成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda info -e</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>other</category>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title>fairseq代码位置记录</title>
    <url>/2021/05/21/fairseq-tips/</url>
    <content><![CDATA[<blockquote>
<p>记录一些fairseq中功能实现的代码位置。<br>fairseq版本: 0.9.0</p>
</blockquote>
<span id="more"></span>

<h1 id="ensemble集成"><a href="#ensemble集成" class="headerlink" title="ensemble集成"></a>ensemble集成</h1><p>在fairseq/sequence_generator.py中760行，ensemble在beam search之前。</p>
]]></content>
      <categories>
        <category>code</category>
        <category>pytorch</category>
        <category>fairseq</category>
      </categories>
  </entry>
  <entry>
    <title>fairseq踩坑记录</title>
    <url>/2021/05/27/fairseq-tips1/</url>
    <content><![CDATA[<blockquote>
<p>记录一些fairseq中遇到的bug和解决方案。<br>fairseq版本: 0.9.0</p>
</blockquote>
<span id="more"></span>

<h1 id="torch-max返回值"><a href="#torch-max返回值" class="headerlink" title="torch.max返回值"></a>torch.max返回值</h1><p>报错：<code>TypeError: expected Tensor as element 0 in argument 0, but got torch.return_types.max</code></p>
<p>原因是torch.max(a, dim=)返回值是tuple类型，第一个元素是值，第二个元素是索引，因此只需要返回值的tensor时，torch.max(a, dim=).values即可</p>
<h1 id="导致维度变化的命令"><a href="#导致维度变化的命令" class="headerlink" title="导致维度变化的命令"></a>导致维度变化的命令</h1><p>torch.max=tf.reduce_sum，都会使维度减一</p>
<p>a = a[:-1:] 也会使维度减一</p>
<p>cat维度不变，stack使维度加一，都是tensor拼接的功能</p>
<h1 id="cpu和gpu上变量的类型不匹配问题"><a href="#cpu和gpu上变量的类型不匹配问题" class="headerlink" title="cpu和gpu上变量的类型不匹配问题"></a>cpu和gpu上变量的类型不匹配问题</h1><p>报错：RuntimeError: expected device cpu but got device cuda:0</p>
<p>可能出现错误的位置：</p>
<ul>
<li><p>等号左边和右边类型不一样</p>
</li>
<li><p>运算符左右两端类型不同，例：+ - * /</p>
</li>
<li><p>同一个函数内，传入参数的类型不同，例matmul等</p>
</li>
</ul>
<p>把tensor转移到相同的设备上解决问题</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-&gt;cuda : data.cuda()</span><br><span class="line">-&gt;cpu: data.cpu()</span><br><span class="line">-&gt;numpy：</span><br><span class="line">cuda类型不能直接转numpy 须先转成cpu类型，data.cpu().numpy()</span><br><span class="line">在cuda下训练中的数据不能直接转换为numpy，data.cpu().detach().numpy()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>参考: <a href="https://blog.csdn.net/qq_41368074/article/details/105942534">https://blog.csdn.net/qq_41368074/article/details/105942534</a></p>
</blockquote>
]]></content>
      <categories>
        <category>code</category>
        <category>pytorch</category>
        <category>fairseq</category>
      </categories>
  </entry>
  <entry>
    <title>git版本控制</title>
    <url>/2022/07/13/git-note/</url>
    <content><![CDATA[<blockquote>
<p>git在linux系统和windows系统中进行版本控制的学习记录。</p>
<p>想要完成两个目标：</p>
<p>一是在个人电脑和实验室电脑都能写博客并同步备份到github；</p>
<p>二是将linux服务器的代码备份到本地计算机中一份，防止丢失，暂未解决。</p>
<span id="more"></span>
</blockquote>
<h1 id="Git安装"><a href="#Git安装" class="headerlink" title="Git安装"></a>Git安装</h1><p>在windows系统中，可以通过官网直接下载，在文件夹空白处右击打开<code>git bash</code>，其余操作和linux中相同，下面统一描述。</p>
<p>在linux系统中直接在命令行中进行安装：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install git</span><br></pre></td></tr></table></figure>

<p>检查是否已经安装：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git</span><br></pre></td></tr></table></figure>

<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><h2 id="创建版本库："><a href="#创建版本库：" class="headerlink" title="创建版本库："></a>创建版本库：</h2><p>将一个目录变为git可管理的仓库，创建后会在文件夹下多一个<code>.git</code>文件，用于跟踪管理版本库，是隐藏文件夹。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir learngit</span><br><span class="line">cd learngit</span><br><span class="line">git init</span><br></pre></td></tr></table></figure>

<h2 id="添加文件到版本库"><a href="#添加文件到版本库" class="headerlink" title="添加文件到版本库"></a>添加文件到版本库</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git add readme.md</span><br></pre></td></tr></table></figure>

<h2 id="将文件提交到仓库"><a href="#将文件提交到仓库" class="headerlink" title="将文件提交到仓库"></a>将文件提交到仓库</h2><p>引号中是对本次提交的描述，可以多次添加文件后进行统一提交。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git commit -m &quot;add a readme file&quot;</span><br></pre></td></tr></table></figure>

<h2 id="提交修改相关操作"><a href="#提交修改相关操作" class="headerlink" title="提交修改相关操作"></a>提交修改相关操作</h2><p><code>git status</code>可以查看哪些文件被修改了还没有被提交。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure>

<p>在文件被<code>git add</code>命令添加之前，<code>git status</code>命令的输出，注意是有add提示的：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">	modified:   readme.txt</span><br><span class="line"></span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br></pre></td></tr></table></figure>

<p>在文件被<code>git add</code>命令添加之后，<code>git status</code>命令的输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">	modified:   readme.txt</span><br></pre></td></tr></table></figure>

<p>在文件被<code>git commit</code>命令提交之后，<code>git status</code>命令的输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">On branch master</span><br><span class="line">nothing to commit, working tree clean</span><br></pre></td></tr></table></figure>

<p><code>git diff</code>可以查看文件的具体的更改</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git diff readme.md</span><br></pre></td></tr></table></figure>

<h2 id="版本回退相关操作"><a href="#版本回退相关操作" class="headerlink" title="版本回退相关操作"></a>版本回退相关操作</h2><p>查看版本控制的历史记录，从近到远，可选项可以让每条记录单行呈现：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git log [--pretty=oneline]</span><br></pre></td></tr></table></figure>

<p>在git中，<code>HEAD</code>表示当前版本，<code>HEAD^</code>表示上一个版本，<code>HEAD~100</code>表示往上第100个版本。</p>
<p>回退一个版本：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git reset --hard HEAD^</span><br></pre></td></tr></table></figure>

<p>这时，log中已经没有回退之前的版本了，如果想要恢复，需要找到回退之前版本的commit id（不需要写全，不产生歧义就行），可以通过以下命令记录来找到：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git reflog</span><br></pre></td></tr></table></figure>

<h2 id="撤销修改"><a href="#撤销修改" class="headerlink" title="撤销修改"></a>撤销修改</h2><p>已经add但没有commit，需要将暂存区的修改撤销到工作区</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git reset HEAD &lt;file&gt;</span><br></pre></td></tr></table></figure>

<p>将没有add的工作区修改撤销</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout -- &lt;file&gt;</span><br></pre></td></tr></table></figure>

<p>删除文件也是一样，当用rm操作后，工作区和版本库不一样，若确实需要删除，则</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git rm &lt;file&gt;</span><br><span class="line">git commit -m &quot;remove file&quot;</span><br></pre></td></tr></table></figure>

<p>若删除错了，则可以从版本库中恢复</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git checkout -- &lt;file&gt;</span><br></pre></td></tr></table></figure>

<p><code>git checkout</code>是用版本库中的版本代替工作区，因此两种情况都可以用作撤回。</p>
<p>总结一下，<code>git checkout</code>是针对工作区的修改；<code>git reset</code>和<code>git remove</code>是针对版本库的修改。</p>
<h1 id="远程仓库github"><a href="#远程仓库github" class="headerlink" title="远程仓库github"></a>远程仓库github</h1><h2 id="创建ssh密钥"><a href="#创建ssh密钥" class="headerlink" title="创建ssh密钥"></a>创建ssh密钥</h2><p>在linux的用户根目录下，windows的<code>C:\Users\username</code>下，查看有没有<code>.ssh</code>目录，和这个目录下有没有<code>id_rsa</code>和<code>id_rsa.pub</code>这两个文件，如果已经有了，可直接跳到下一步。如果没有则创建SSH Key：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;youremail@example.com&quot;</span><br></pre></td></tr></table></figure>

<p>登陆GitHub，打开“Account settings”，“SSH Keys”页面，点“Add SSH Key”，填上任意Title，在Key文本框里粘贴<code>id_rsa.pub</code>文件的内容，点“Add Key”，你就应该看到已经添加的Key。</p>
<h2 id="有远程仓库时如何克隆到本地"><a href="#有远程仓库时如何克隆到本地" class="headerlink" title="有远程仓库时如何克隆到本地"></a>有远程仓库时如何克隆到本地</h2><p>先创建远程仓库可以直接生成readme文件，在远程库准备好后，通过<code>git clone</code>克隆到本地</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone git@github.com:michaelliao/gitskills.git</span><br></pre></td></tr></table></figure>

<p>注：<code>michaelliao/gitskills</code>要改成自己的用户名和仓库名，这里如果报错<code>The authenticity of host can’t be established.</code>的话，可能是文件夹内少了一个known_hosts文件，可以yes回车解决。</p>
<h2 id="先有本地仓库时如何关联远程仓库"><a href="#先有本地仓库时如何关联远程仓库" class="headerlink" title="先有本地仓库时如何关联远程仓库"></a>先有本地仓库时如何关联远程仓库</h2><p>首先创建一个github空仓库，然后在本地仓库下运行：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git remote add origin git@github.com:michaelliao/learngit.git</span><br></pre></td></tr></table></figure>

<p><code>michaelliao/learngit</code>用自己的用户名和仓库名，<code>origin</code>是git默认的远程仓库名，也可以改成别的</p>
<p>查看远程库信息：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure>

<h2 id="将本地仓库推送到远程仓库"><a href="#将本地仓库推送到远程仓库" class="headerlink" title="将本地仓库推送到远程仓库"></a>将本地仓库推送到远程仓库</h2><p>将<code>main</code>分支推送到<code>origin</code>远程仓库，第一次推送<code>master</code>分支时，加上了<code>-u</code>参数，Git不但会把本地的<code>master</code>分支内容推送的远程新的<code>master</code>分支，还会把本地的<code>master</code>分支和远程的<code>master</code>分支关联起来，在以后的推送或者拉取时就可以简化命令。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure>

<p>以后的推送可以用命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git push origin master</span><br></pre></td></tr></table></figure>

<h2 id="解除关联"><a href="#解除关联" class="headerlink" title="解除关联"></a>解除关联</h2><p>先查看远程库的信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure>

<p>解除关联</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git remote rm origin</span><br></pre></td></tr></table></figure>

<h2 id="github开源库"><a href="#github开源库" class="headerlink" title="github开源库"></a>github开源库</h2><p>在github中的开源代码，可以先Fork克隆到自己的仓库，然后在自己的账号下clone到本地修改，再向自己的仓库推送，如果希望发布到原始的github仓库，可以发起pull request，需要对方接受。</p>
<h1 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h1><p>每次提交都是git时间线上的一个节点，<code>master</code>指向最新的提交，<code>HEAD</code>指向当前分支的最新提交点，当创建新的分支时，也就是创建了一个新的指针<code>dev</code>指向<code>master</code>相同的指针，将<code>HEAD</code>指向<code>dev</code>表示当前分支在<code>dev</code>上，合并删除和创建都是指针操作，因此速度很快。</p>
<h2 id="分支操作"><a href="#分支操作" class="headerlink" title="分支操作"></a>分支操作</h2><p>创建和切换到<code>dev</code>分支，可以用<code>git checkout</code>命令，<code>-b</code>表示创建并切换；</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git checkout -b dev</span><br></pre></td></tr></table></figure>

<p>也可以用专门的命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git switch -c dev</span><br></pre></td></tr></table></figure>

<p>也可分两步用两条命令实现：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git branch dev</span><br><span class="line">git checkout dev</span><br></pre></td></tr></table></figure>

<p>查看当前分支，会列出多有分支，当前分支前有<code>*</code>号：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git branch</span><br></pre></td></tr></table></figure>

<p>切换回<code>master</code>分支：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git checkout master</span><br></pre></td></tr></table></figure>

<p>也可以用专门的命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git switch master</span><br></pre></td></tr></table></figure>

<p>将指定分支<code>dev</code>分支的工作成果合并到当前分支<code>master</code>分支上：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git merge dev</span><br></pre></td></tr></table></figure>

<p>当出现冲突时，需要手动解决冲突再提交，用<code>git log --graph</code>可以查看分支合并图。</p>
<p>删除<code>dev</code>分支</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git branch -d dev  </span><br></pre></td></tr></table></figure>

<h2 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h2><p>首先从远程克隆仓库到本地，默认情况只能看到master分支</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone git@github.com:michaelliao/learngit.git</span><br></pre></td></tr></table></figure>

<p>假设我们需要在<code>dev</code>分支上开发，则需要创建本地的<code>dev</code>分支</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout -b dev origin origin、dev</span><br></pre></td></tr></table></figure>

<p>在此基础上进行修改和提交并推送</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git push origin dev</span><br></pre></td></tr></table></figure>

<p>如果推送失败，则因为远程分支比你的本地更新，需要先用<code>git pull</code>试图合并</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git pull</span><br></pre></td></tr></table></figure>

<p>如果<code>pull</code>失败，是因为没有指定本地<code>dev</code>分支与远程<code>origin/dev</code>分支的链接，根据提示，设置<code>dev</code>和<code>origin/dev</code>的链接，并再次<code>pull</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git branch --set-upstream-to=origin/dev dev</span><br><span class="line">git pull</span><br></pre></td></tr></table></figure>

<p>如果再次提交和推送，显示冲突，则手动解决冲突，并在本地提交；</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git push origin dev</span><br></pre></td></tr></table></figure>



<h1 id="总结一下"><a href="#总结一下" class="headerlink" title="总结一下"></a>总结一下</h1><p>对第一个需求，Github允许添加多个key，可以将每台电脑的key都添加到github中，即可实现多个设备对同一个库的同步更新，每次clone下来并进行修改提交推送即可。</p>
<p>对第二个需求，暂未解决外网访问的问题。</p>
<blockquote>
<p>参考：<a href="https://www.liaoxuefeng.com/wiki/896043488029600/">https://www.liaoxuefeng.com/wiki/896043488029600/</a></p>
</blockquote>
]]></content>
      <categories>
        <category>other</category>
        <category>git</category>
      </categories>
  </entry>
  <entry>
    <title>latex-tips</title>
    <url>/2021/11/27/latex-tips/</url>
    <content><![CDATA[<blockquote>
<p>latex/overleaf一些笔记。</p>
</blockquote>
<span id="more"></span>

<h1 id="图片排版"><a href="#图片排版" class="headerlink" title="图片排版"></a>图片排版</h1><h2 id="宏包："><a href="#宏包：" class="headerlink" title="宏包："></a>宏包：</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\usepackage&#123;graphicx&#125; %插图宏包</span><br><span class="line">\usepackage&#123;subfig&#125; %子图包含宏包，较新</span><br><span class="line">\usepackage&#123;subfigure&#125; %子图包含宏包，较旧</span><br></pre></td></tr></table></figure>

<h2 id="基本命令："><a href="#基本命令：" class="headerlink" title="基本命令："></a>基本命令：</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\begin&#123;figure&#125;[ht]</span><br><span class="line">\centering</span><br><span class="line">\includegraphics[width&#x3D;10cm]&#123;example.png&#125;</span><br><span class="line">\caption&#123;this is a figure&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure>

<p>\centering表示的是里面紧跟的内容都居中；</p>
<p>\includegrapics[参数1]{参数2}</p>
<p>参数1：对图片进行一些调整，例如宽高缩放width=10cm,height=8cm,scale=0.4</p>
<p>参数2：图片名</p>
<p>\caption{标题}设置图片的一个编号以及为图片添加标题；</p>
<h2 id="图片位置："><a href="#图片位置：" class="headerlink" title="图片位置："></a>图片位置：</h2><p>\begin{figure}[参数1]</p>
<p>参数1：对图片在文中的位置进行设置，h 此处（here）t 页顶（top）b 页底（bottom）p 独立一页（page），<strong>H 固定位置</strong>；</p>
<h2 id="subfig和subfigure的区别："><a href="#subfig和subfigure的区别：" class="headerlink" title="subfig和subfigure的区别："></a>subfig和subfigure的区别：</h2><p>subfig：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\begin&#123;figure&#125;[tbp]</span><br><span class="line">	\centering</span><br><span class="line">	\subfloat[Arabic numerals]&#123;\label&#123;fig:a&#125;\includegraphics[width&#x3D;1in]&#123;placeholder&#125;&#125;\quad</span><br><span class="line">	\subfloat[Arabic numerals]&#123;\label&#123;fig:b&#125;\includegraphics[width&#x3D;1in]&#123;placeholder&#125;&#125;\\	</span><br><span class="line">	\caption&#123;Capital Roman numerals.&#125;</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure>

<p>subfigure：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\begin&#123;figure&#125; \centering </span><br><span class="line">\subfigure[figure 1 title.] &#123; \label&#123;fig:a&#125; </span><br><span class="line">\includegraphics[width&#x3D;0.8\columnwidth]&#123;fig1.eps&#125; </span><br><span class="line">&#125; </span><br><span class="line">\subfigure[figure 2 title.] &#123; \label&#123;fig:b&#125; </span><br><span class="line">\includegraphics[width&#x3D;0.8\columnwidth]&#123;fig2.eps&#125; </span><br><span class="line">&#125; </span><br><span class="line">\caption&#123; general title. &#125; </span><br><span class="line">\label&#123;fig&#125; </span><br><span class="line">\end&#123;figure&#125; </span><br></pre></td></tr></table></figure>


<blockquote>
<p>参考：</p>
<p><a href="https://zhuanlan.zhihu.com/p/143529262">https://zhuanlan.zhihu.com/p/143529262</a></p>
<p><a href="https://blog.csdn.net/yq_forever/article/details/84796802">https://blog.csdn.net/yq_forever/article/details/84796802</a></p>
</blockquote>
]]></content>
      <categories>
        <category>other</category>
      </categories>
  </entry>
  <entry>
    <title>hexo-next框架附加功能</title>
    <url>/2021/05/27/hexo-next-tips/</url>
    <content><![CDATA[<blockquote>
<p>hexo+next框架博客的一些附加功能记录。<br>hexo: 5.4.0<br>next: 7.8.0</p>
</blockquote>
<span id="more"></span>

<h1 id="版本信息："><a href="#版本信息：" class="headerlink" title="版本信息："></a>版本信息：</h1><p>hexo和next版本信息可在相应文件夹的package.json中搜索version查看。</p>
<h1 id="版权声明"><a href="#版权声明" class="headerlink" title="版权声明:"></a>版权声明:</h1><p>在.\themes\next\layout_macro\目录下，新建my-copyright.swig文件，内容为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% if page.copyright %&#125;</span><br><span class="line">&lt;div class&#x3D;&quot;my_post_copyright&quot;&gt;</span><br><span class="line">  &lt;script src&#x3D;&quot;&#x2F;&#x2F;cdn.bootcss.com&#x2F;clipboard.js&#x2F;1.5.10&#x2F;clipboard.min.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- JS库 sweetalert 可修改路径 --&gt;</span><br><span class="line">  &lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;http:&#x2F;&#x2F;jslibs.wuxubj.cn&#x2F;sweetalert_mini&#x2F;jquery-1.7.1.min.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">  &lt;script src&#x3D;&quot;http:&#x2F;&#x2F;jslibs.wuxubj.cn&#x2F;sweetalert_mini&#x2F;sweetalert.min.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">  &lt;link rel&#x3D;&quot;stylesheet&quot; type&#x3D;&quot;text&#x2F;css&quot; href&#x3D;&quot;http:&#x2F;&#x2F;jslibs.wuxubj.cn&#x2F;sweetalert_mini&#x2F;sweetalert.mini.css&quot;&gt;</span><br><span class="line"></span><br><span class="line">  &lt;p&gt;&lt;span&gt;本文标题:&lt;&#x2F;span&gt;&#123;&#123; page.title &#125;&#125;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;本文作者:&lt;&#x2F;span&gt;xxx&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;本文链接:&lt;&#x2F;span&gt;&lt;a href&#x3D;&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot; title&#x3D;&quot;&#123;&#123; page.title &#125;&#125;&quot;&gt;&#123;&#123; page.permalink &#125;&#125;&lt;&#x2F;a&gt;</span><br><span class="line">    &lt;span class&#x3D;&quot;copy-path&quot;  title&#x3D;&quot;点击复制文章链接&quot;&gt;&lt;i class&#x3D;&quot;fa fa-clipboard&quot; data-clipboard-text&#x3D;&quot;&#123;&#123; page.permalink &#125;&#125;&quot;  aria-label&#x3D;&quot;复制成功！&quot;&gt;&lt;&#x2F;i&gt;&lt;&#x2F;span&gt;</span><br><span class="line">  &lt;&#x2F;p&gt;</span><br><span class="line">  &lt;p&gt;&lt;span&gt;版权声明:&lt;&#x2F;span&gt;本博客所有文章除特别声明外，均采用&lt;a rel&#x3D;&quot;license&quot; href&#x3D;&quot;https:&#x2F;&#x2F;creativecommons.org&#x2F;licenses&#x2F;by-nc-nd&#x2F;4.0&#x2F;&quot; target&#x3D;&quot;_blank&quot; title&#x3D;&quot;Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)&quot;&gt;CC BY-NC-ND&lt;&#x2F;a&gt;许可协议。转载请注明出处！&lt;&#x2F;p&gt;  </span><br><span class="line">&lt;&#x2F;div&gt;</span><br><span class="line">&lt;script&gt; </span><br><span class="line">    var clipboard &#x3D; new Clipboard(&#39;.fa-clipboard&#39;);</span><br><span class="line">    clipboard.on(&#39;success&#39;, $(function()&#123;</span><br><span class="line">      $(&quot;.fa-clipboard&quot;).click(function()&#123;</span><br><span class="line">        swal(&#123;   </span><br><span class="line">          title: &quot;&quot;,   </span><br><span class="line">          text: &#39;复制成功&#39;,   </span><br><span class="line">          html: false,</span><br><span class="line">          timer: 500,   </span><br><span class="line">          showConfirmButton: false</span><br><span class="line">        &#125;);</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;));  </span><br><span class="line">&lt;&#x2F;script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>

<p>打开.\themes\next\layout_macro\post.swig文件，在</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;#####################&#125;</span><br><span class="line">&#123;### END POST BODY ###&#125;</span><br><span class="line">&#123;#####################&#125;</span><br></pre></td></tr></table></figure>
<p>之后添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!--添加版权信息--&gt;</span><br><span class="line">&lt;div&gt;</span><br><span class="line">    &#123;% if not is_index %&#125;</span><br><span class="line">    &#123;% include &#39;my-copyright.swig&#39; %&#125;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure>

<p>在.\themes\next\source\css_common\components\post\目录下，新建my-post-copyright.styl文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.my_post_copyright &#123;</span><br><span class="line">  width: 85%;</span><br><span class="line">  max-width: 45em;</span><br><span class="line">  margin: 2.8em auto 0;</span><br><span class="line">  padding: 0.5em 1.0em;</span><br><span class="line">  border: 1px solid #d3d3d3;</span><br><span class="line">  font-size: 0.93rem;</span><br><span class="line">  line-height: 1.6em;</span><br><span class="line">  word-break: break-all;</span><br><span class="line">  background: rgba(255,255,255,0.4);</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright p&#123;margin:0;&#125;</span><br><span class="line">.my_post_copyright span &#123;</span><br><span class="line">  display: inline-block;</span><br><span class="line">  width: 5.2em;</span><br><span class="line">  color: #333333; &#x2F;&#x2F; title color</span><br><span class="line">  font-weight: bold;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .raw &#123;</span><br><span class="line">  margin-left: 1em;</span><br><span class="line">  width: 5em;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright a &#123;</span><br><span class="line">  color: #808080;</span><br><span class="line">  border-bottom:0;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright a:hover &#123;</span><br><span class="line">  color: #0593d3; &#x2F;&#x2F; link color</span><br><span class="line">  text-decoration: underline;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright:hover .fa-clipboard &#123;</span><br><span class="line">  color: #000;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .post-url:hover &#123;</span><br><span class="line">  font-weight: normal;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .copy-path &#123;</span><br><span class="line">  margin-left: 1em;</span><br><span class="line">  width: 1em;</span><br><span class="line">  +mobile()&#123;display:none;&#125;</span><br><span class="line">&#125;</span><br><span class="line">.my_post_copyright .copy-path:hover &#123;</span><br><span class="line">  color: #808080;</span><br><span class="line">  cursor: pointer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>打开.\themes\next\source\css_common\components\post\post.styl文件，在最后添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@import &quot;my-post-copyright&quot;</span><br></pre></td></tr></table></figure>

<p>打开.\scaffolds\post.md文件，设置新文件开启版权声明</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">copyright: true #开启</span><br><span class="line">---</span><br></pre></td></tr></table></figure>

<blockquote>
<p>参考：<a href="https://blog.csdn.net/u011236348/article/details/88169271">https://blog.csdn.net/u011236348/article/details/88169271</a></p>
</blockquote>
<h1 id="访客-阅读量统计"><a href="#访客-阅读量统计" class="headerlink" title="访客/阅读量统计"></a>访客/阅读量统计</h1><p>打开.\themes\next_config.yml文件，<br>搜索busuanzi_count关键字，把enable设置为true，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Show Views &#x2F; Visitors of the website &#x2F; page with busuanzi.</span><br><span class="line"># Get more information on http:&#x2F;&#x2F;ibruce.info&#x2F;2015&#x2F;04&#x2F;04&#x2F;busuanzi</span><br><span class="line">busuanzi_count:</span><br><span class="line">  enable: true</span><br><span class="line">  total_visitors: true #访客数</span><br><span class="line">  total_visitors_icon: fa fa-user</span><br><span class="line">  total_views: true #访问数</span><br><span class="line">  total_views_icon: fa fa-eye</span><br><span class="line">  post_views: true #文章阅读量</span><br><span class="line">  post_views_icon: fa fa-eye</span><br></pre></td></tr></table></figure>
<p>同一文件，搜索footer关键字，在其底下添加counter，设值为true。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup. If not defined, current year will be used.</span><br><span class="line">  #since: 2015</span><br><span class="line">  # count for visit</span><br><span class="line">  counter: true</span><br></pre></td></tr></table></figure>

<p>打开.\themes\next\layout_partials\footer.swig文件，添加代码。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% if theme.footer.counter %&#125;</span><br><span class="line">    &lt;script async src&#x3D;&quot;&#x2F;&#x2F;dn-lbstatics.qbox.me&#x2F;busuanzi&#x2F;2.3&#x2F;busuanzi.pure.mini.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>

<p>访客、访问次数在网站主页底部，文章阅读量在文章开头。</p>
<blockquote>
<p>参考：<a href="https://blog.csdn.net/baidu_34310405/article/details/102665373">https://blog.csdn.net/baidu_34310405/article/details/102665373</a></p>
</blockquote>
<h1 id="博客中插入自己编写的html页面"><a href="#博客中插入自己编写的html页面" class="headerlink" title="博客中插入自己编写的html页面"></a>博客中插入自己编写的html页面</h1><p>看到next官方readme中LEAFERx博客sentence页面很好看，也动手搞一个类似的。利用jQuery的全屏滚动插件fullPage.js实现，网页部分主要参考链接中的代码。</p>
<p>将自己的html页面插入博客中，首先新建一个页面<code>hexo new page &quot;schedule&quot;</code>，在生成的文件夹source/schedule/下复制网页的css/js/html文件。</p>
<p>修改根目录下的<code>_config.yml</code>文件，跳过自定义页面的渲染过程，若文件夹下有子文件夹，需要改为<code>schedule/**</code>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">skip_render:</span><br><span class="line"> - &quot;schedule&#x2F;*&quot;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>参考：<a href="https://www.dowebok.com/77.html">https://www.dowebok.com/77.html</a></p>
</blockquote>
]]></content>
      <categories>
        <category>other</category>
        <category>hexo</category>
        <category>next</category>
      </categories>
  </entry>
  <entry>
    <title>linux-tips</title>
    <url>/2021/11/28/linux-tips/</url>
    <content><![CDATA[<blockquote>
<p>linux服务器下的一些常用操作。</p>
</blockquote>
<span id="more"></span>

<h1 id="0-在线手册"><a href="#0-在线手册" class="headerlink" title="0 在线手册"></a>0 在线手册</h1><p>1.<a href="https://www.linuxcool.com/">Linux命令大全（手册）</a></p>
<p>2.<a href="https://www.cnblogs.com/jingmoxukong/p/7867397.html">一篇文章让你彻底掌握shell语言</a></p>
<p>3.<a href="https://www.runoob.com/linux/linux-command-manual.html">Linux命令大全（RUNOOB.COM）</a></p>
<h1 id="1-资源占用情况"><a href="#1-资源占用情况" class="headerlink" title="1 资源占用情况"></a>1 资源占用情况</h1><h2 id="1-1-GPU显卡占用"><a href="#1-1-GPU显卡占用" class="headerlink" title="1.1 GPU显卡占用"></a>1.1 GPU显卡占用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>

<p>显示如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Sun Nov 28 13:47:48 2021       </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 430.64       Driver Version: 430.64       CUDA Version: 10.1     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  Tesla V100-PCIE...  Off  | 00000000:04:00.0 Off |                    0 |</span><br><span class="line">| N/A   60C    P0    73W / 250W |  16487MiB / 32510MiB |     39%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   1  Tesla V100-PCIE...  Off  | 00000000:06:00.0 Off |                    0 |</span><br><span class="line">| N/A   40C    P0    27W / 250W |      0MiB / 32510MiB |      0%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   2  Tesla V100S-PCI...  Off  | 00000000:0C:00.0 Off |                    0 |</span><br><span class="line">| N/A   75C    P0   166W / 250W |  17412MiB / 32510MiB |     27%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   3  Tesla V100S-PCI...  Off  | 00000000:0E:00.0 Off |                    0 |</span><br><span class="line">| N/A   72C    P0   140W / 250W |  15442MiB / 32510MiB |     28%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                       GPU Memory |</span><br><span class="line">|  GPU       PID   Type   Process name                             Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|    0     10685      C   ...g/anaconda3/envs/torch/bin/python 16475MiB |</span><br><span class="line">|    2     11578      C   ...g/anaconda3/envs/torch/bin/python 17401MiB |</span><br><span class="line">|    3     11579      C   ...g/anaconda3/envs/torch/bin/python 15431MiB |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="1-2-查看进程"><a href="#1-2-查看进程" class="headerlink" title="1.2 查看进程"></a>1.2 查看进程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">top</span><br></pre></td></tr></table></figure>

<p>显示如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND </span><br><span class="line">   10010      20  10  10.15g  3.63g   200200   4.0  1.5   0:30:31 top</span><br></pre></td></tr></table></figure>

<p>查看某一用户的所有进程（top下）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">u username</span><br></pre></td></tr></table></figure>

<p>展开具体命令（top下）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c</span><br></pre></td></tr></table></figure>

<p>退出top命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ctrl+c</span><br></pre></td></tr></table></figure>

<p>查看某一进程的详细信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps aux | grep PID</span><br></pre></td></tr></table></figure>

<p>结束进程：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kill PID</span><br></pre></td></tr></table></figure>

<h1 id="2-文件路径"><a href="#2-文件路径" class="headerlink" title="2 文件路径"></a>2 文件路径</h1><p>1.当前路径：<code>$pwd</code></p>
<p>2.当前目录：<code>.</code>，上级目录：<code>..</code></p>
<p>3.根目录：<code>/</code></p>
<p>4.用户的home路径：<code>~</code>，<code>~user</code>表示用户名为user（在/etc/passwd中存在的用户名）的home路径</p>
<h1 id="3-从网页下载数据"><a href="#3-从网页下载数据" class="headerlink" title="3 从网页下载数据"></a>3 从网页下载数据</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget 数据链接</span><br></pre></td></tr></table></figure>

<h1 id="4-防误删（回收站）"><a href="#4-防误删（回收站）" class="headerlink" title="4 防误删（回收站）"></a>4 防误删（回收站）</h1><p>目录结构：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\.bashrc</span><br><span class="line">\00-trash</span><br><span class="line">	-trash.sh</span><br><span class="line">	-.trash</span><br><span class="line">		-.bashrc</span><br></pre></td></tr></table></figure>

<h2 id="4-1-移入回收站"><a href="#4-1-移入回收站" class="headerlink" title="4.1 移入回收站"></a>4.1 移入回收站</h2><p>将<code>rm</code>命令删除的文件移入<code>.trash</code>文件中，并用时间命名防止重复。</p>
<p>使用时在<code>.bashrc</code>文件中加入以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># trash for rm command</span><br><span class="line">alias rm&#x3D;&#x2F;user&#x2F;00-trash&#x2F;trash.sh</span><br></pre></td></tr></table></figure>

<p>执行<code>source .bashrc</code>使其生效。</p>
<p><code>trash.sh</code>如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">#一个简易回收站</span><br><span class="line">#https:&#x2F;&#x2F;github.com&#x2F;LIU-LIU-LIU&#x2F;Recycle_bin</span><br><span class="line"></span><br><span class="line">TarshDir&#x3D;&quot;&#96;echo ~&#96;&#x2F;00-trash&#x2F;.trash&quot;</span><br><span class="line">FileNamePrefix&#x3D;&#96;date +%y-%m-%d-%H-%M-%S&#96;</span><br><span class="line"></span><br><span class="line">error()&#123;</span><br><span class="line">echo -e &quot;\033[31m 错误!\n移动至回收站不需要任何命令选项。格式:rm [文件]  \033[0m&quot;</span><br><span class="line">exit 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tarsh()&#123;</span><br><span class="line">case $1 in</span><br><span class="line">-*|.&#x2F;)</span><br><span class="line">        error</span><br><span class="line">;;</span><br><span class="line">&#x2F;)</span><br><span class="line">        echo -e &quot;\033[31m 此操作风险太高,已禁止! \033[0m&quot;</span><br><span class="line">        exit 1</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">        mkdir -p &quot;$TarshDir&quot;&#x2F;&quot;$FileNamePrefix&quot;</span><br><span class="line">        &#x2F;bin&#x2F;mv -i &quot;$@&quot; &quot;&quot;$TarshDir&quot;&#x2F;&quot;$FileNamePrefix&quot;&#x2F;&quot;</span><br><span class="line">        if [ &quot;$?&quot; &#x3D;&#x3D; &quot;0&quot; ] ; then</span><br><span class="line">                echo &quot;已将&quot;$@&quot;移动至回收站(&quot;$TarshDir&quot;&#x2F;&quot;$FileNamePrefix&quot;&#x2F;)&quot;</span><br><span class="line">        else</span><br><span class="line">                echo -e &quot;\033[31m 移动至回收站失败。\033[0m&quot;</span><br><span class="line">                &#x2F;bin&#x2F;rm -rf &quot;$TarshDir&quot;&#x2F;&quot;$FileNamePrefix&quot;</span><br><span class="line">        fi</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$1&quot; ];then</span><br><span class="line">        error</span><br><span class="line">else</span><br><span class="line">        tarsh $*</span><br><span class="line">fi</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="4-2-清空回收站"><a href="#4-2-清空回收站" class="headerlink" title="4.2 清空回收站"></a>4.2 清空回收站</h1><p>在<code>.trash</code>文件夹中，写一个<code>.bashrc</code>文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alias rm&#x3D;&quot;rm&quot;</span><br></pre></td></tr></table></figure>

<p>执行<code>source .bashrc</code>文件恢复原始<code>rm</code>命令功能，清空回收站。</p>
<h1 id="5-解压文件"><a href="#5-解压文件" class="headerlink" title="5 解压文件"></a>5 解压文件</h1><h2 id="5-1-zip文件"><a href="#5-1-zip文件" class="headerlink" title="5.1 zip文件"></a>5.1 zip文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">unzip filename</span><br></pre></td></tr></table></figure>

<h2 id="5-2-tar-gz文件"><a href="#5-2-tar-gz文件" class="headerlink" title="5.2 tar.gz文件"></a>5.2 tar.gz文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxvf filename</span><br></pre></td></tr></table></figure>

<h1 id="6-执行进程相关"><a href="#6-执行进程相关" class="headerlink" title="6 执行进程相关"></a>6 执行进程相关</h1><h2 id="6-1-指定进程使用的GPU"><a href="#6-1-指定进程使用的GPU" class="headerlink" title="6.1 指定进程使用的GPU"></a>6.1 指定进程使用的GPU</h2><p>在终端执行时指定：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0,1，当设为-1时代表不使用任何gpu</span><br></pre></td></tr></table></figure>

<p>在sh脚本文件中指定:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#export CUDA_VISIBLE_DEVICES&#x3D;0,1</span><br></pre></td></tr></table></figure>

<p>在python程序中指定：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import os; os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] &#x3D; &quot;0&quot;</span><br></pre></td></tr></table></figure>

<h2 id="6-2-后台运行不挂断"><a href="#6-2-后台运行不挂断" class="headerlink" title="6.2 后台运行不挂断"></a>6.2 后台运行不挂断</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup python -u train.py &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>该命令会将输出重定向到nohup.log文件中。</p>
<h1 id="7-磁盘空间"><a href="#7-磁盘空间" class="headerlink" title="7 磁盘空间"></a>7 磁盘空间</h1><p>查看整个服务器的空间:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df -h</span><br></pre></td></tr></table></figure>

<p>查看当前目录下每一级目录的空间：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">du -h --max-depth&#x3D;1</span><br></pre></td></tr></table></figure>

<h1 id="8-文本内容抽取和对比"><a href="#8-文本内容抽取和对比" class="headerlink" title="8 文本内容抽取和对比"></a>8 文本内容抽取和对比</h1><h1 id="8-1-文本比对"><a href="#8-1-文本比对" class="headerlink" title="8.1 文本比对"></a>8.1 文本比对</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vimdiff file1 file2</span><br></pre></td></tr></table></figure>

<h2 id="8-2-内容抽取：cut"><a href="#8-2-内容抽取：cut" class="headerlink" title="8.2 内容抽取：cut"></a>8.2 内容抽取：cut</h2><p>对每行抽取需要的部分，常用参数如下：</p>
<ol>
<li><code>-d</code>：指定分隔符，默认为“TAB”</li>
<li><code>-f</code>：抽取指定部分</li>
<li><code>N-</code>：从第N个部分到结尾</li>
<li><code>N-M</code>：从第N个部分到第M个（包括M）部分</li>
<li><code>-M</code>：从第1个部分到第M个（包括M）部分</li>
</ol>
<p><strong>举例：</strong> 在文件ldc_test.result中抽取出以<code>-T</code>开头的句子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-S: 决议 要求 埃塞俄比亚 立即 采取 具体 步骤 , 使 厄 埃 边界 委员会 能 在 没有 先决条件 的 情况 下 迅速 标@@ 定 边界 ; 要求 厄立特里亚 不再 拖延 , 不 设 先决条件 地 取消 对 埃@@ 厄 特派 团 的 行动 和 作业 的 所有 限制 .</span><br><span class="line">-T: The resolution requires Ethiopia to immediately take concrete steps to allow the Erit@@ rea - Ethiopia Boundary Commission to speedily demarc@@ ate the border without any preconditions ; and requires Erit@@ rea to cancel all of its restrictions on UN@@ ME@@ E &#39;s actions and operations without any further delay and without setting any preconditions .</span><br><span class="line">-P: The resolution asked Ethiopia to take specific steps immediately to enable the Erit@@ rean border committee to rapidly set its boundary without a precondition ; Erit@@ rea would no longer delay or set a precedent for the removal of all restrictions on the actions and operations of the Erit@@ rean special missions .</span><br><span class="line"></span><br><span class="line">-S: 有关 部门 应 强化 低 保@@ 户 在 享受 低 保 时 须 履行 的 义务 : 如 及时 通报 家庭 人员 及 收入 变化 情况 , 汇报 就业 情况 , 接受 定期 复@@ 审 等 , 而 有关 部门 则 应 加大 监督 检查 的 力度 .</span><br><span class="line">-T: The relevant department should stress the obligations that welfare recipients must carry out while enjoying the welfare : for example , promptly notifying the changes in the family members and incomes , reporting the status of employment , accepting regular reviews , etc. On the other hand , the relevant department should step up monitoring and inspection .</span><br><span class="line">-P: The relevant departments should strengthen the obligation of low - bonded households to carry out such tasks as helping low - income families to enjoy low - income insured : if timely reporting of changes in the income and changes in the income and reporting on employment , and receiving regular reviews , the relevant departments should intensify supervision and inspection .</span><br></pre></td></tr></table></figure>

<p>使用命令：<code>grep ^-T ldc_test.result | cut -f2- -d&quot; &quot; &gt; new.result</code>。其中，<code>-d&quot; &quot;</code>将空格设为分隔符，<code>-f2-</code>在用空格分开后的部分中，从第二个部分起，抽取后面所有部分。效果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The resolution requires Ethiopia to immediately take concrete steps to allow the Erit@@ rea - Ethiopia Boundary Commission to speedily demarc@@ ate the border without any preconditions ; and requires Erit@@ rea to cancel all of its restrictions on UN@@ ME@@ E &#39;s actions and operations without any further delay and without setting any preconditions .</span><br><span class="line">The relevant department should stress the obligations that welfare recipients must carry out while </span><br></pre></td></tr></table></figure>

<h1 id="9-统计目录下的文件数量"><a href="#9-统计目录下的文件数量" class="headerlink" title="9 统计目录下的文件数量"></a>9 统计目录下的文件数量</h1><p>1.统计当前目录下的一般文件的数量（不包括子文件夹，也不包括子文件夹内的文件）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ls -l |grep &quot;^-&quot;|wc -l</span><br></pre></td></tr></table></figure>

<p>2.统计当前目录下的子文件夹数量（不包括一般文件，也不包括子文件夹内的文件）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ls -l |grep &quot;^ｄ&quot;|wc -l</span><br></pre></td></tr></table></figure>

<p>3.统计当前目录下的一般文件数量（包括子文件夹内的一般文件，但不包括子文件夹，也不包括子文件夹内的子文件夹）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ls -lR|grep &quot;^-&quot;|wc -l </span><br></pre></td></tr></table></figure>

<p>解释：</p>
<p><code>ls-l</code>命令输出当前目录下的文件信息（包含目录、链接、设备文件等），每一行对应一个文件。</p>
<p><code>ls -lR</code>命令输出当前目录下的包括子目录的文件信息。</p>
<p><code>grep &quot;^-&quot;</code>过滤<code>ls</code>的输出信息，只保留一般文件。</p>
<p><code>grep &quot;^d&quot;</code>过滤<code>ls</code>的输出信息，只保留子文件夹。</p>
<p><code>wc -l</code>命令统计输出信息的行数。</p>
<h1 id="10-查看系统构架"><a href="#10-查看系统构架" class="headerlink" title="10 查看系统构架"></a>10 查看系统构架</h1><p>方法1：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">~$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux alt 5.4.0-104-generic #118-Ubuntu SMP Wed Mar 2 19:02:41 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure>

<p>方法2：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">~$</span><span class="bash"> arch</span></span><br><span class="line">x86_64</span><br></pre></td></tr></table></figure>

<blockquote>
<p>参考：</p>
<p><a href="https://hannlp.github.io/2021-01-15-Linux-&amp;-Shell-Usage-Record/">https://hannlp.github.io/2021-01-15-Linux-&amp;-Shell-Usage-Record/</a></p>
<p><a href="https://github.com/LIU-LIU-LIU/Recycle_bin">https://github.com/LIU-LIU-LIU/Recycle_bin</a></p>
<p><a href="https://www.cnblogs.com/dylancao/p/9012790.html">https://www.cnblogs.com/dylancao/p/9012790.html</a></p>
</blockquote>
]]></content>
      <categories>
        <category>other</category>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title>pytorch笔记</title>
    <url>/2021/05/28/pytorch-note/</url>
    <content><![CDATA[<blockquote>
<p>pytorch-学习笔记</p>
</blockquote>
<span id="more"></span>

<h1 id="0-pytorch的组件"><a href="#0-pytorch的组件" class="headerlink" title="0 pytorch的组件"></a>0 pytorch的组件</h1><table>
<thead>
<tr>
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://pytorch.org/docs/stable/torch.html"><strong>torch</strong></a></td>
<td>在GPU上的类似Numpy的张量库，一些基本操作</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/autograd.html"><strong>torch.autograd</strong></a></td>
<td>支持torch中张量的微分操作</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/jit.html"><strong>torch.jit</strong></a></td>
<td>创建可序列化和可优化的编译堆栈(TorchScript)</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/nn.html"><strong>torch.nn</strong></a></td>
<td>与autograd深度集成的神经网络库</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/multiprocessing.html"><strong>torch.multiprocessing</strong></a></td>
<td>对数据加载Hogwild训练起效的多进程</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/data.html"><strong>torch.utils</strong></a></td>
<td>数据加载和其他实用程序功能</td>
</tr>
</tbody></table>
<h1 id="1-pytorch手册"><a href="#1-pytorch手册" class="headerlink" title="1 pytorch手册"></a>1 pytorch手册</h1><h2 id="1-1-官方备忘录"><a href="#1-1-官方备忘录" class="headerlink" title="1.1 官方备忘录"></a><a href="https://pytorch.org/tutorials/beginner/ptcheat.html">1.1 官方备忘录</a></h2><p><strong>import:</strong></p>
<p>一般的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch                                        <span class="comment"># root package</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader    <span class="comment"># dataset representation and loading</span></span><br></pre></td></tr></table></figure>

<p>神经网络API：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd         <span class="comment"># computation graph</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor                  <span class="comment"># tensor node in the computation graph</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn                     <span class="comment"># neural networks</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F           <span class="comment"># layers, activations and more</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim               <span class="comment"># optimizers e.g. gradient descent, ADAM, etc.</span></span><br><span class="line"><span class="keyword">from</span> torch.jit <span class="keyword">import</span> script, trace       <span class="comment"># hybrid frontend decorator and tracing jit</span></span><br></pre></td></tr></table></figure>

<p><strong>Tensor:</strong></p>
<p>创建：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(*size)              <span class="comment"># tensor with independent N(0,1) entries</span></span><br><span class="line">x = torch.[ones|zeros](*size)       <span class="comment"># tensor with all 1&#x27;s [or 0&#x27;s]</span></span><br><span class="line">x = torch.tensor(L)                 <span class="comment"># create tensor from [nested] list or ndarray L</span></span><br><span class="line">y = x.clone()                       <span class="comment"># clone of x</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():               <span class="comment"># code wrap that stops autograd from tracking tensor history</span></span><br><span class="line">requires_grad=<span class="literal">True</span>                  <span class="comment"># arg, when set to True, tracks computation</span></span><br><span class="line">                                    <span class="comment"># history for future derivative calculations</span></span><br></pre></td></tr></table></figure>

<p>维度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.size()                                  <span class="comment"># return tuple-like object of dimensions</span></span><br><span class="line">x = torch.cat(tensor_seq, dim=<span class="number">0</span>)          <span class="comment"># concatenates tensors along dim</span></span><br><span class="line">y = x.view(a,b,...)                       <span class="comment"># reshapes x into size (a,b,...)</span></span><br><span class="line">y = x.view(-<span class="number">1</span>,a)                          <span class="comment"># reshapes x into size (b,a) for some b</span></span><br><span class="line">y = x.transpose(a,b)                      <span class="comment"># swaps dimensions a and b</span></span><br><span class="line">y = x.permute(*dims)                      <span class="comment"># permutes dimensions</span></span><br><span class="line">y = x.unsqueeze(dim)                      <span class="comment"># tensor with added axis</span></span><br><span class="line">y = x.unsqueeze(dim=<span class="number">2</span>)                    <span class="comment"># (a,b,c) tensor -&gt; (a,b,1,c) tensor</span></span><br><span class="line">y = x.squeeze()                           <span class="comment"># removes all dimensions of size 1 (a,1,b,1) -&gt; (a,b)</span></span><br><span class="line">y = x.squeeze(dim=<span class="number">1</span>)                      <span class="comment"># removes specified dimension of size 1 (a,1,b,1) -&gt; (a,b,1)</span></span><br></pre></td></tr></table></figure>



<h1 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h1><h2 id="访问元素"><a href="#访问元素" class="headerlink" title="访问元素"></a>访问元素</h2><p><code>[1,:]</code>其中:表示选择范围，只有:表示全选，与数字结合表示范围。</p>
<p><code>[::3,::2]</code>其中::表示跳选，每隔x个选择一个。</p>
<p>张量:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; torch.arange(12) #生成</span><br><span class="line">x.shape #形状</span><br><span class="line">x.numel() #元素总数</span><br><span class="line">x &#x3D; x.reshape(3,4) #变换</span><br><span class="line">x &#x3D; torch.zeros((1,2,3)) #全零</span><br><span class="line">x &#x3D; torch.ones((2,3,4)) #全一</span><br><span class="line">x &#x3D; torch.tensor(list) #根据list生成tensor</span><br><span class="line">+ - * &#x2F; ** #对元素进行运算</span><br><span class="line">torch.exp(x) #指数运算</span><br><span class="line">torch.cat((x,y),dim&#x3D;0) #在第零维拼接，零维是最外侧</span><br><span class="line">x &#x3D;&#x3D; y #构建二元张量，是元素比较的结果</span><br><span class="line">x.sum() #求和，结果是一个元素的tensor</span><br><span class="line">x[0:2,:]&#x3D;12 #区域赋值</span><br><span class="line">x[:]&#x3D;x+y &#x2F; x+&#x3D;y #减少内存开销，前后不变</span><br><span class="line">y &#x3D; x.numpy() #tensor转ndarray</span><br><span class="line">x &#x3D; torch.tensor(y) #ndarray转tensor</span><br><span class="line">x.item() &#x2F; float(x) &#x2F; int(x) #0维张量转标量</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>code</category>
        <category>pytorch</category>
      </categories>
  </entry>
  <entry>
    <title>python笔记</title>
    <url>/2021/05/31/python-note/</url>
    <content><![CDATA[<blockquote>
<p>python3的一些知识点</p>
</blockquote>
<span id="more"></span>

<h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><ul>
<li><p>函数中的默认参数必须指向不变对象，否则在多次调用时会改变默认值（例如None、str，反例如list）。</p>
</li>
<li><p>可变参数：允许不定量个参数输入，在参数前加<em>，传入参数类型是tuple。<br>在输入list/tuple进入可变参数的函数时，可以加上</em>直接传入。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def func(*num):</span><br><span class="line">    pass</span><br><span class="line">a &#x3D; (1,2,3)</span><br><span class="line">func(*a)</span><br></pre></td></tr></table></figure></li>
<li><p>关键字参数：**来表示，用法同可变参数，区别是传入的参数都为<code>x=y</code>格式，传入后变成一个关键字为x，值为y的字典。<br>可以用作用户可选输入的收集上。</p>
</li>
<li><p>命名关键字参数：用<em>或者可变参数</em>x做分隔符，规定必须输入的关键字参数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def func(x,y,*,z):</span><br><span class="line">    pass</span><br><span class="line">func(1,2,z&#x3D;4)</span><br></pre></td></tr></table></figure>
<h1 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h1></li>
<li><p>左闭右开取值，第三个参数表示间隔</p>
</li>
</ul>
<h1 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h1><ul>
<li>字典的迭代默认是key，迭代value:<code>for value in d.values()</code>，<br>同时迭代key和value:<code>for k, v in d.items()</code></li>
</ul>
<h1 id="生成器generator"><a href="#生成器generator" class="headerlink" title="生成器generator"></a>生成器generator</h1><ul>
<li><p>把生成式列表的[]改为()，每次调用next(x)计算下一个元素的值，或者用for循环迭代</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">g &#x3D; (x*x for x in range(0,10))</span><br><span class="line">a &#x3D; next(g)</span><br><span class="line">for i in g:</span><br><span class="line">	print(i)</span><br></pre></td></tr></table></figure></li>
<li><p>在函数中添加yield语句，每次调用<code>next()</code>，运行到yield返回，下次调用从yield下继续</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def odd():</span><br><span class="line">	print(&quot;step 1&quot;)</span><br><span class="line">	yield 1</span><br><span class="line">	print(&quot;step 2&quot;)</span><br><span class="line">	yield 3</span><br><span class="line">	print(&quot;step 3&quot;)</span><br><span class="line">	yield 5</span><br><span class="line">o &#x3D; odd()</span><br><span class="line">a &#x3D; next(o) #step 2\n3</span><br></pre></td></tr></table></figure>

<ul>
<li>这里要注意for循环调用函数生成器时，函数内的print内容无法输出，可以通过<code>StopIteration</code>错误的value获取。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for i in o:</span><br><span class="line">	print(i) #1 3 5</span><br><span class="line">while True:</span><br><span class="line">	try:</span><br><span class="line">		x &#x3D; next(o)</span><br><span class="line">		print(x)</span><br><span class="line">	except StopIteration as e:</span><br><span class="line">		print(e.value)</span><br><span class="line">		break</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h1><ul>
<li>判断对象是不是<code>Iterable</code>对象</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from collections.abc import Iterable</span><br><span class="line">isinstance([], Iterable) # True</span><br></pre></td></tr></table></figure>

<ul>
<li>可以被<code>next()</code>不断调用并返回下一个值的对象称为迭代器<code>Iterator</code>对象，生成器都是<code>Iterator</code>对象，但是<code>Iterable</code>对象不全是<code>Iterator</code>对象</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from collections.abc import Iterator</span><br><span class="line">isinstance([],Iterator) # False</span><br></pre></td></tr></table></figure>

<ul>
<li><code>Iterable</code>对象可以通过使用<code>iter()</code>函数变成<code>Iterator</code>对象，两者的区别是后者是惰性的，可以是无限长度的序列流，无法提前知道长度，前者是有限的，长度可知的。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">isinstance(iter([]), Iterator) # True</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h1><p>函数式编程的一个特点：允许把函数本身作为参数传入另一个函数，还允许返回一个函数。</p>
<h2 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h2><ul>
<li>接收另一个函数作为参数的函数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def add(x, y, f):</span><br><span class="line">	return f(x) + f(y)</span><br></pre></td></tr></table></figure>

<ul>
<li>map()：接收两个参数，一个函数，一个<code>Iterable</code>对象，对每个对象实施函数，返回结果是一个<code>Iterator</code>对象。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def f(x):</span><br><span class="line">	return x*x</span><br><span class="line">r &#x3D; map(f, [1,2,3,4,5,6])</span><br></pre></td></tr></table></figure>

<ul>
<li>reduce()：接收两个参数，一个函数和一个序列，功能是把结果继续和序列的下一个元素做累积计算。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#str2int功能实现</span><br><span class="line">from functools import reduce</span><br><span class="line">def fn(x, y):</span><br><span class="line">	return x * 10 + y</span><br><span class="line">def char2num(s):</span><br><span class="line">	digits &#x3D; &#123;&#39;0&#39;: 0, &#39;1&#39;: 1, &#39;2&#39;: 2, &#39;3&#39;: 3, &#39;4&#39;: 4, &#39;5&#39;: 5, &#39;6&#39;: 6, &#39;7&#39;: 7, &#39;8&#39;: 8, &#39;9&#39;: 9&#125;</span><br><span class="line">	return digits[s]</span><br><span class="line">reduce(fn, map(char2num, &#39;13579&#39;)) # 13579</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#利用map和reduce编写一个str2float函数，把字符串&#39;123.456&#39;转换成浮点数123.456</span><br><span class="line">def str2float(s):</span><br><span class="line">    def ten(x, y):</span><br><span class="line">        return x*10+y</span><br><span class="line">    def st(x):</span><br><span class="line">        D &#x3D; &#123;&#39;0&#39;: 0, &#39;1&#39;: 1, &#39;2&#39;: 2, &#39;3&#39;: 3, &#39;4&#39;: 4, &#39;5&#39;: 5, &#39;6&#39;: 6, &#39;7&#39;: 7, &#39;8&#39;: 8, &#39;9&#39;: 9&#125;</span><br><span class="line">        return D[x]</span><br><span class="line">    a, b &#x3D; s.split(&#39;.&#39;)</span><br><span class="line">    num &#x3D; len(b)</span><br><span class="line">    ans1 &#x3D; reduce(ten, list(map(st, a)))</span><br><span class="line">    ans2 &#x3D; reduce(ten, list(map(st, b)))</span><br><span class="line">    ans &#x3D; ans1 +ans2&#x2F;(10**num)</span><br><span class="line">    return ans</span><br><span class="line">	</span><br><span class="line">print(&#39;str2float(\&#39;123.456\&#39;) &#x3D;&#39;, str2float(&#39;123.456&#39;))</span><br><span class="line">if abs(str2float(&#39;123.456&#39;) - 123.456) &lt; 0.00001:</span><br><span class="line">    print(&#39;测试成功!&#39;)</span><br><span class="line">else:</span><br><span class="line">    print(&#39;测试失败!&#39;)</span><br></pre></td></tr></table></figure>

<ul>
<li>filter():接收两个参数，一个函数，一个序列，根据函数的True/False来决定序列是否保留，返回值是一个<code>Iterator</code>对象<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def not_empty(s):</span><br><span class="line">    return s and s.strip()</span><br><span class="line">x &#x3D; list(filter(not_empty, [&#39;A&#39;, &#39;&#39;, &#39;B&#39;, None, &#39;C&#39;, &#39;  &#39;])) # [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>code</category>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>DC分治</title>
    <url>/2021/10/15/oj-DC/</url>
    <content><![CDATA[<blockquote>
<p>算法课笔记-分治<br>语言：C++</p>
</blockquote>
<span id="more"></span>

<h1 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h1><p>内容。</p>
]]></content>
      <categories>
        <category>code</category>
        <category>oj</category>
      </categories>
  </entry>
  <entry>
    <title>DP动态规划</title>
    <url>/2021/10/15/oj-DP/</url>
    <content><![CDATA[<blockquote>
<p>算法课笔记-动态规划<br>语言: C++</p>
</blockquote>
<span id="more"></span>

<h1 id="中文标题"><a href="#中文标题" class="headerlink" title="中文标题"></a>中文标题</h1><p>内容。</p>
]]></content>
      <categories>
        <category>code</category>
        <category>oj</category>
      </categories>
  </entry>
  <entry>
    <title>shell脚本食用指南</title>
    <url>/2022/08/08/shell-tips/</url>
    <content><![CDATA[<blockquote>
<p>努力学习fairseq的一天，先学到了许多shell脚本知识，整理一下吧！</p>
</blockquote>
<span id="more"></span>

<h1 id="1-shell脚本中set-e的作用"><a href="#1-shell脚本中set-e的作用" class="headerlink" title="1 shell脚本中set -e的作用"></a>1 shell脚本中set -e的作用</h1><p>linux系统的说明：”Exit immediately if a simple command exits with a non-zero status.”</p>
<p>在”set -e”之后出现的代码，一旦出现了返回值非零，整个脚本就会立即退出，那么就可以避免一些脚本的危险操作。</p>
<h1 id="2-sh脚本执行报错"><a href="#2-sh脚本执行报错" class="headerlink" title="2 sh脚本执行报错"></a>2 sh脚本执行报错</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Syntax error: &quot;(&quot; unexpected</span><br></pre></td></tr></table></figure>

<p>报错原因：</p>
<p>在常见的linux发行版中，sh一般指向bash，但是有的系统debian、ubuntu上的sh默认指向dash，只支持基本的shell功能。</p>
<p>解决方式：用以下两种方式之一执行脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bash test.sh</span><br><span class="line">./test.sh</span><br></pre></td></tr></table></figure>

<h1 id="3-执行-sh文件总是提示permission-denied"><a href="#3-执行-sh文件总是提示permission-denied" class="headerlink" title="3 执行.sh文件总是提示permission denied"></a>3 执行.sh文件总是提示permission denied</h1><p>一种常见可能是，写代码的时候加入了不必要的空格或者字符，sh文件代码之间不需要空格，去掉一般可以解决。</p>
<h1 id="4-shell脚本中的特殊字符"><a href="#4-shell脚本中的特殊字符" class="headerlink" title="4 shell脚本中的特殊字符"></a>4 shell脚本中的特殊字符</h1><h2 id="4-1-”-“相关："><a href="#4-1-”-“相关：" class="headerlink" title="4.1 ”$“相关："></a>4.1 ”$“相关：</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> 取一个变量的值，例如<span class="variable">$VAR</span> <span class="variable">$&#123;VAR&#125;</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"><span class="comment"># 代表传入参数的个数</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">@ 代表传入参数的列表</span></span><br><span class="line"><span class="meta">$</span><span class="bash">0 代表脚本本身</span></span><br><span class="line"><span class="meta">$</span><span class="bash">1 代表传入的第一个参数，<span class="variable">$2</span>，<span class="variable">$3</span>...以此类推</span></span><br><span class="line"><span class="meta">$</span><span class="bash">* 以字符串方式显示所有传入的参数</span></span><br><span class="line"><span class="meta">$</span><span class="bash">$ 脚本运行的进程ID</span></span><br><span class="line"><span class="meta">$</span><span class="bash">? 显示最后命令的退出状况，0表示没有错误</span></span><br><span class="line"><span class="meta">$</span><span class="bash">! 后台运行的最后一个进程的 ID 号</span></span><br><span class="line"><span class="meta">$</span><span class="bash">- 显示 Shell 使用的当前选项</span></span><br><span class="line"><span class="meta">$</span><span class="bash">&#123;<span class="comment">#0&#125; 显示执行shell本身文件名的长度</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">&#123;<span class="comment">#1&#125; 显示执行shell传入的第一个参数的长度</span></span></span><br></pre></td></tr></table></figure>

<p>例子：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">VAR=&quot;Hello World!&quot;</span><br><span class="line">echo &quot;\$VAR : $VAR&quot;</span><br><span class="line"></span><br><span class="line">echo &quot;\$# argc: $#&quot;</span><br><span class="line">echo &quot;\$@ argv[]: $@&quot;</span><br><span class="line">echo &quot;\$0 argv[0]: $0&quot;</span><br><span class="line">echo &quot;\$1 argv[1]: $1&quot;</span><br><span class="line">echo &quot;\$2 argv[2]: $2&quot;</span><br><span class="line">echo &quot;\$* argv_str: $*&quot;</span><br><span class="line">echo &quot;\$$ pid: $$&quot;</span><br><span class="line">echo &quot;\$? retcode: $?&quot;</span><br><span class="line">echo &quot;\$! retcode: $!&quot;</span><br><span class="line">echo &quot;\$- retcode: $-&quot;</span><br><span class="line">echo &quot;\$&#123;#0&#125; stringlength: $&#123;#0&#125;&quot;</span><br><span class="line">echo &quot;\$&#123;#1&#125; stringlength: $&#123;#1&#125;&quot;</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">```powershell</span><br><span class="line"><span class="meta">$</span><span class="bash">VAR : Hello World!</span></span><br><span class="line"><span class="meta">$</span><span class="bash"><span class="comment"># argc: 2</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">@ argv[]: a 1</span></span><br><span class="line"><span class="meta">$</span><span class="bash">0 argv[0]: test.sh</span></span><br><span class="line"><span class="meta">$</span><span class="bash">1 argv[1]: a</span></span><br><span class="line"><span class="meta">$</span><span class="bash">2 argv[2]: 1</span></span><br><span class="line"><span class="meta">$</span><span class="bash">* argv_str: a 1</span></span><br><span class="line"><span class="meta">$</span><span class="bash">$ pid: 2648</span></span><br><span class="line"><span class="meta">$</span><span class="bash">? retcode: 0</span></span><br><span class="line"><span class="meta">$</span><span class="bash">! retcode:</span> </span><br><span class="line"><span class="meta">$</span><span class="bash">- retcode: hB</span></span><br><span class="line"><span class="meta">$</span><span class="bash">&#123;<span class="comment">#0&#125; stringlength: 7</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">&#123;<span class="comment">#1&#125; stringlength: 1</span></span></span><br></pre></td></tr></table></figure>



<h2 id="4-2-循环数组变量，”-”和”-“相关："><a href="#4-2-循环数组变量，”-”和”-“相关：" class="headerlink" title="4.2 循环数组变量，”@”和”*“相关："></a>4.2 循环数组变量，”@”和”*“相关：</h2><p><code>“*”</code>当变量加上<code>“”</code> 会当成一串<a href="https://so.csdn.net/so/search?q=%E5%AD%97%E7%AC%A6%E4%B8%B2&spm=1001.2101.3001.7020">字符串</a>处理.<br><code>“@”</code>变量加上<code>“”</code> 依然当做数组处理.<br>在没有加上“” 的情况下两者的效果是等效的.</p>
<p>例子：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">array=(1 2 3)</span><br><span class="line">echo &quot;case 1&quot;</span><br><span class="line">for line in &quot;$&#123;array[@]&#125;&quot;</span><br><span class="line">do</span><br><span class="line">echo $line</span><br><span class="line">done</span><br><span class="line"> </span><br><span class="line">echo &quot;case 2&quot;</span><br><span class="line">for line in &quot;$&#123;array[*]&#125;&quot;</span><br><span class="line">do</span><br><span class="line">echo $line</span><br><span class="line">done</span><br><span class="line"> </span><br><span class="line">echo &quot;case 3&quot;</span><br><span class="line">for line in $&#123;array[*]&#125;</span><br><span class="line">do</span><br><span class="line">echo $line</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &quot;case 4&quot;</span><br><span class="line">for line in $&#123;array[@]&#125;</span><br><span class="line">do</span><br><span class="line">echo $line</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">case 1</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">case 2</span><br><span class="line">1 2 3</span><br><span class="line">case 3</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">case 4</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td></tr></table></figure>



<h2 id="4-3-换行相关："><a href="#4-3-换行相关：" class="headerlink" title="4.3 换行相关："></a>4.3 换行相关：</h2><p>\ 符号表示下一行是上一行内容的连续。</p>
<p>要注意每个换行\后面不能有空格，直接跟回车。</p>
<h2 id="4-4-cat相关："><a href="#4-4-cat相关：" class="headerlink" title="4.4 cat相关："></a>4.4 cat相关：</h2><p>cat：查看文件内容，连接文件，创建一个或多个文件，并将输出重定向到终端或文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat #接受标准输入的内容并在标准输出中显示，ctrl+D退出</span><br><span class="line">cat test.txt #显示test.txt的文本内容</span><br><span class="line">cat -n test.txt #显示test.txt的文本内容和行号</span><br><span class="line">cat -b test.txt #显示test.txt的文本内容和非空行的行号，空行可见</span><br><span class="line">cat -s test.txt #显示test.txt的文本内容和非空行的行号，连续两行空行只留一行</span><br><span class="line">cat -e test.txt #显示test.txt的文本内容，每行末尾显示$字符</span><br><span class="line">cat a b &gt; c #把文件a和b的内容合并到c中，c是新文件</span><br><span class="line">cat a b &gt;&gt; c #把文件a和b的内容附加到c中，c是本来就有的</span><br></pre></td></tr></table></figure>



<h2 id="4-5-grep相关："><a href="#4-5-grep相关：" class="headerlink" title="4.5 grep相关："></a>4.5 grep相关：</h2><p>grep全称是Global Regular Expression Print，表示全局正则表达式版本，主要功能是进行字符串数据的对比，能使用正则表达式搜索文本，并将符合用户需求的字符串打印出来，在数据中查找出一个字符串时，以<strong>整行</strong>为单位来进行数据选取的。</p>
<p>具体查看<a href="https://www.cnblogs.com/w-j-q/p/14864213.html">这篇博客</a>。</p>
<h2 id="4-6-管道命令操作符-："><a href="#4-6-管道命令操作符-：" class="headerlink" title="4.6 管道命令操作符|："></a>4.6 管道命令操作符|：</h2><p>可以理解为把上一个命令的输出传给下一个命令。</p>
<p><img src="https://raw.githubusercontent.com/zhang3550545/image_center/master/image/shell-pipe1.png" alt="image"></p>
<p>例如：与cat、grep、sort 等结合使用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat test.txt | grep o | grep w</span><br></pre></td></tr></table></figure>



<h2 id="4-7-匹配字符串，-相关"><a href="#4-7-匹配字符串，-相关" class="headerlink" title="4.7 匹配字符串，#*,##*,%*,%%*相关"></a>4.7 匹配字符串，#*,##*,%*,%%*相关</h2><p>总结：#提取文件名，%提取路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">VAR=&quot;/a/b/c/d.txt&quot; #以该字符串为例</span><br></pre></td></tr></table></figure>

<p>#*：最小匹配左边，匹配部分被删除，相当于保留带路径的文件名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">VAR=$&#123;VAR#*/&#125;</span><br><span class="line">echo $VAR</span><br><span class="line"><span class="meta">#</span><span class="bash"> a/b/c/d.txt</span></span><br></pre></td></tr></table></figure>

<p>##*：最长匹配左边，匹配部分被删除，相当于保留文件名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">VAR=$&#123;VAR##*/&#125;</span><br><span class="line">echo $VAR</span><br><span class="line"><span class="meta">#</span><span class="bash"> d.txt</span></span><br></pre></td></tr></table></figure>

<p>%*：最小匹配右边，匹配部分被删除，相当于保留当前目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">VAR=$&#123;VAR%/*&#125;</span><br><span class="line">echo $VAR</span><br><span class="line"><span class="meta">#</span><span class="bash"> /a/b/c</span></span><br></pre></td></tr></table></figure>

<p>%%*：最长匹配右边，匹配部分被删除，相当于保留根目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">VAR=$&#123;VAR%%/*&#125;</span><br><span class="line">echo $VAR</span><br><span class="line"><span class="meta">#</span><span class="bash"> （空字符）</span></span><br></pre></td></tr></table></figure>



<h2 id="5-set命令"><a href="#5-set命令" class="headerlink" title="5 set命令"></a>5 set命令</h2><p>set命令可以定义脚本的运行方式，变量的获取方式，脚本的执行过程，脚本的测试。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set -u # 检查脚本内的变量，有未定义的变量则终止脚本</span><br><span class="line">set -e # 脚本中遇到错误则终止运行，不用该命令则下面的命令继续执行</span><br><span class="line">set -x # 显示脚本的执行过程</span><br></pre></td></tr></table></figure>



<blockquote>
<p>参考：</p>
<p><a href="https://blog.csdn.net/V_matrix/article/details/122448124">https://blog.csdn.net/V_matrix/article/details/122448124</a></p>
<p><a href="https://blog.csdn.net/qq_20817327/article/details/120067033">https://blog.csdn.net/qq_20817327/article/details/120067033</a></p>
<p><a href="https://blog.csdn.net/qq_33689414/article/details/79111154?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-79111154-blog-124279883.pc_relevant_multi_platform_whitelistv3&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-79111154-blog-124279883.pc_relevant_multi_platform_whitelistv3&amp;utm_relevant_index=1">https://blog.csdn.net/qq_33689414/article/details/79111154?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-79111154-blog-124279883.pc_relevant_multi_platform_whitelistv3&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-79111154-blog-124279883.pc_relevant_multi_platform_whitelistv3&amp;utm_relevant_index=1</a></p>
<p><a href="https://www.cnblogs.com/w-j-q/p/14864213.html">https://www.cnblogs.com/w-j-q/p/14864213.html</a></p>
<p><a href="https://blog.csdn.net/weixin_50034171/article/details/110202089">https://blog.csdn.net/weixin_50034171/article/details/110202089</a></p>
</blockquote>
]]></content>
      <categories>
        <category>other</category>
      </categories>
  </entry>
  <entry>
    <title>如何进行研究——Marco Tulio Ribeiro</title>
    <url>/2022/08/18/research-tips/</url>
    <content><![CDATA[<blockquote>
<p>对Marco Tulio Ribeiro写的两篇关于研究者如何进行研究的总结</p>
</blockquote>
<span id="more"></span>

<h1 id="1-提出研究思路"><a href="#1-提出研究思路" class="headerlink" title="1 提出研究思路"></a>1 提出研究思路</h1><h2 id="1-1-了解相邻领域"><a href="#1-1-了解相邻领域" class="headerlink" title="1.1 了解相邻领域"></a>1.1 了解相邻领域</h2><p>知道的越多越容易提出想法，在对自己的领域深入了解的同时，应该对自己研究方向的相邻领域有广泛的了解，可以通过课程或者论文的方式，不要陷入细节，而是有一个全局的概念，<strong>例如人们正在解决什么问题</strong>、<strong>哪些技术是最先进的</strong>以及<strong>如何进行评估等</strong>。</p>
<p>安排一些学习时间，对内容可以随机一些，看论文时遇到不熟悉的技术和问题可以学习一下。</p>
<p>与其他合作者交流喜欢的论文，阅读非自己领域的顶会论文，在报告会时多走动交流。</p>
<h2 id="1-2-对重点问题设置过滤器"><a href="#1-2-对重点问题设置过滤器" class="headerlink" title="1.2 对重点问题设置过滤器"></a>1.2 对重点问题设置过滤器</h2><p>当我们有意识的思考某件事情时，会更容易的注意到其他事物与它的关联，因此列出要解决的重要的问题可以帮助研究者更好的注意到获得的新知识是否可以用于该问题。</p>
<h2 id="1-3-调查失败的尝试"><a href="#1-3-调查失败的尝试" class="headerlink" title="1.3 调查失败的尝试"></a>1.3 调查失败的尝试</h2><p>对于研究中尝试的失败的方法，需要花时间来了解失败的原因，可以帮助我们更好的找到新的想法，对于暂时不能解决的可以放到一个清单中，之后再继续思考。</p>
<p>这种学习方法也可以通过对他人工作的反思来进行，可以思考一下已经有的技术如何用在论文中没有提及但是有用的事情上，这种场景一般是失败的，进而可以思考失败的原因。</p>
<h2 id="1-4-使用类比"><a href="#1-4-使用类比" class="headerlink" title="1.4 使用类比"></a>1.4 使用类比</h2><p>尝试在现有工作和未解决的问题之间找到类比，不是简单的套用a技术到b问题，而是探索相似和不同，找到合适的场景或符合类比所需的约束的情况。</p>
<h2 id="1-5-挑战现状"><a href="#1-5-挑战现状" class="headerlink" title="1.5 挑战现状"></a>1.5 挑战现状</h2><p>首先需要理解为什么现状是这样，如果不知道是什么导致了现状，以及现状的合理性，那么挑战现状是白费努力。</p>
<h1 id="2-组织和充实想法"><a href="#2-组织和充实想法" class="headerlink" title="2 组织和充实想法"></a>2 组织和充实想法</h1><p>需要对要解决的问题有清晰的了解和考量，一个不好的问题即使有一个很好的解决方法也意义不大。</p>
<h2 id="2-1-组织潜在项目"><a href="#2-1-组织潜在项目" class="headerlink" title="2.1 组织潜在项目"></a>2.1 组织潜在项目</h2><p>可以按照下面的模板进行组织，以作者的工作checklist为示例说明，：</p>
<p><strong>问题陈述：</strong>对我们要解决的问题的陈述</p>
<p><em>我们想知道我们的模型是否符合业务规则、先验知识和常识。现在，即使是专家也很难写下这些是什么。大多数人甚至没有想过他们为什么需要这样做。</em></p>
<p><strong>为什么困难：</strong>总结一下这个问题为什么重要或为什么我们要关心并解决它。</p>
<p><em>训练/评估数据是静态的，通常具有模型可以利用的偏差。准确性通常不是我们关心的全部，而且测试集几乎从来没有足够大或变化到足以测试某些行为（例如，模型在存在拼写错误时的行为方式，模型对于某些异常值的行为方式等）。</em></p>
<p><strong>已有的解决方案和失败原因：</strong>可能适用于“问题”的方法列表，即使它们不是专门为它设计的。请注意失败的方法并不是不好，只是我们希望它们不能解决我们的问题，即使它们在其他方面非常出色。</p>
<p><em>我列出了一堆方法，并说“这些方法并不能真正测试人类期望的模型行为，只有少数例外。此外，有些需要访问模型内部”。</em></p>
<p><strong>如果我有一个解决方案，是什么样的：</strong>重点不是实际提出解决方案，而是我们期望的解决方案，也就是对结果的预期或希望的形式，可以提出多种就更好了。</p>
<p><em>测试框架，包括不同测试类型的分类</em></p>
<p><em>一种从用户那里引出测试的方法</em></p>
<p><em>使整个过程变得简单的软件</em></p>
<p><strong>我怎样知道是否能解决：</strong>评估的草稿，不用详细</p>
<p><em>许多令人信服的 SOTA 模型未能通过测试的例子，并从中收集到了见解。</em></p>
<p><em>与微软工程师进行的“用户研究”，我们（希望）表明他们在编写测试后在自己的模型中发现了一堆（可修复的）问题。</em></p>
<p><strong>不确定性是什么？什么对于这个工作来说是需要成立的，什么对于这个工作来说是无法成立的：</strong>这是关于确定高度不确定的区域，并确保我们尽可能快地排除会使项目或解决方案无效的事情。</p>
<p><em>如果我是唯一能够进行测试的用户，那将是失败的。</em></p>
<p><em>如果 SOTA 模型仅在特殊测试中失败，那么准确性可能已经足够好，我们不需要这个项目。</em></p>
<p><strong>计划草图：</strong>草图是这里的关键词。我们只需要前几个步骤的合理细节，而第一步通常是某种形式的初步文献回顾。</p>
<p>文献综述（lit review）</p>
<p><em>为 SOTA 情绪分析模型编写测试</em></p>
<p><em>为其他几个模型编写测试（paraphrase，SQuAD）</em></p>
<p><em>尝试将这些组织成一般原则</em></p>
<p><em>开发所需的任何工具</em></p>
<p><em>与微软人员进行案例研究；与研究员的案例研究</em></p>
<p><em>用户研究</em></p>
<p><em>写论文</em></p>
<h2 id="2-2-对模板的评价"><a href="#2-2-对模板的评价" class="headerlink" title="2.2 对模板的评价"></a>2.2 对模板的评价</h2><p>前五点（直到“我怎么知道我解决了它”）让我们了解了项目是什么，如果项目成功，世界会发生什么变化，为什么我们关心这种变化，以及我们如何证明确实发生了一些变化。最后两点主要用于评估项目，并有一个关于如何取得进展的暂定草图（如果无法指定草图是一个坏迹象）。</p>
<p>模板是不断修改的，初期的版本很可能有各种问题，但是有一个模板比没有模板好很多。</p>
<p>模板是面向问题的而不是解决方案的。</p>
<h1 id="3-决定哪些想法值得作为研究项目进行"><a href="#3-决定哪些想法值得作为研究项目进行" class="headerlink" title="3 决定哪些想法值得作为研究项目进行"></a>3 决定哪些想法值得作为研究项目进行</h1><h2 id="3-1-想象不同的未来"><a href="#3-1-想象不同的未来" class="headerlink" title="3.1 想象不同的未来"></a>3.1 想象不同的未来</h2><p><strong>写下一个项目的可能结果</strong>，<strong>以及你对其可能性的估计</strong>（尽量具体）。有些项目在极端情况下（成功或失败）更为尖锐，而另一些项目则更有可能部分成功。即使是粗略的估计也会迫使您考虑当前的资源和约束如何影响成功的概率，以及您可以预期的成功或失败类型，例如“如果我成功，这个项目会产生什么样的影响？”，“我能不能弄清楚几周后我肯定会失败，还是几个月后才知道？”。反过来，这有助于你思考你的风险/回报组合，以及该项目的概况如何符合你的目标，例如，如果你是一名希望很快毕业的研究生，你可能不应该选择一个风险很大、几乎没有部分成功途径的项目。</p>
<p>有一点需要注意：您应该对这些估计有很高的不确定性，并且对过早优化要非常小心。我认为在你当前的最佳估计中加入一些随机性（例如，有时根据你当前的信念和目标做一些没有真正意义的项目）是一个很好的策略，可以避免错过好机会，因为你无法预测它们。注意到我们大多数人不善于估计结果及其概率，我仍然认为在评估项目时进行粗略估计是有用的，即使人们不太重视这些估计。</p>
<h2 id="3-2-确保回报上限很高"><a href="#3-2-确保回报上限很高" class="headerlink" title="3.2 确保回报上限很高"></a>3.2 确保回报上限很高</h2><p>“高上界”并不是一个好项目的充分条件，但“低上界”通常足以拒绝一个项目，无论你如何定义“成功”或“回报”。如果最好的情况是一篇“ok”的论文（即使风险接近于零），那么在几乎任何项目上花费大量时间都是不值得的。我的导师曾经说过，一篇好论文胜过三篇好论文，我认为这种心态真的很有帮助。当然，你不能保证伟大，但如果你追求“足够好”，这通常是你能做到的。</p>
<h2 id="3-3-获得更多信息"><a href="#3-3-获得更多信息" class="headerlink" title="3.3 获得更多信息"></a>3.3 获得更多信息</h2><p>与其做出接受或拒绝项目的不可逆决定，不如收集更多信息，以完善提案并减少不确定性。重要的是在这个阶段将项目作为一种可能性（而不是确定性）来思考和讨论，例如避免说“这是我现在正在做的项目”。</p>
<p>有以下两种方式来获得信息：</p>
<p><strong>与人交流：</strong>把这个项目介绍给几个人，看看他们是否“相信”你的故事、你对其重要性的估计、当前解决方案的缺点等等。我认为持怀疑态度和消极态度的人在这一点上是很棒的，但矛盾的是，我认为你不应该听得太仔细。从一开始，大多数原创想法听起来都不太好。如果你不感到气馁，负面反馈是模板中“不确定性”部分的绝佳素材，为你提供可以检查和防御的伪造假设。此外，这是一个很好的方法来做廉价的文献综述，当人们问你“这不是X在1984年做的吗？”。</p>
<p>向专家询问您不确定的特定问题也可以是一种降低失败风险不确定性的廉价方法。如果这是目标，我通常会尝试抽象地问我不确定的问题（例如，“你将如何解决这个子问题？”在给出项目背景之前——否则，这个人可以更专注于整个项目，而不是在我最想要的地方减少我的不确定性。当然，这里有一个权衡，因为提出更具体的问题会降低你得到意外（有用）建议的可能性。</p>
<p><strong>参加黑客会议：</strong>回顾模板中的“不确定性”部分，并尝试减少不确定性，尤其是在可能导致项目失败的事情周围。</p>
<p><a href="https://fedoraproject.org/wiki/Hack_session">Hack session</a> 是一种非正式的会议方式，起源于<a href="https://fedoraproject.org/wiki/CommOps">CommOps</a>团队。黑客会议是 CommOps 团队成员之间的协作式音频/视频通话。在 hack 会话中，成员处理团队中发生的各种故障单或问题。它们是掌握正在进行的工作并让团队其他成员快速回答问题的好方法。这也是进一步了解团队的好方法。</p>
<p>有一些捷径，例如，如果提议的解决方案是管道A→ B→ C→ D，你对D有不确定性，只需要伪造C，而不是A→ B→ C。</p>
<p>通常也可以使用廉价的下限来减少不确定性。例如使用zero-shot GPT-3、在相关任务上训练过的预训练模型或者编写一个粗糙的基于规则的模型等等。一旦你有了某个步骤的廉价下限，你就可以使用它，看看它能帮你走多远。</p>
<p>黑客会议也可以确保你真的很好地解决问题。即使你手工完成所有步骤（一种很棒的捷径），你也必须检查你提议的任何步骤和“输出”是否现实（“输出”与检查模板中的“为什么困难”部分是否合理特别相关）。</p>
<p>黑客会议的结果可以成为与一群新的人交谈的绝佳素材，因为你通常会得到具体的例子，让人们很容易想象一个项目的“最终结果”。</p>
<blockquote>
<p>参考：</p>
<p><a href="https://medium.com/@marcotcr/coming-up-with-research-ideas-3032682e5852">https://medium.com/@marcotcr/coming-up-with-research-ideas-3032682e5852</a></p>
<p><a href="https://medium.com/@marcotcr/organizing-and-evaluating-research-ideas-e137637b599e">https://medium.com/@marcotcr/organizing-and-evaluating-research-ideas-e137637b599e</a></p>
<p><a href="https://mp.weixin.qq.com/s/nDj_m1Cq1qi3Qo4E--EhpA">https://mp.weixin.qq.com/s/nDj_m1Cq1qi3Qo4E--EhpA</a></p>
</blockquote>
]]></content>
      <categories>
        <category>other</category>
      </categories>
  </entry>
  <entry>
    <title>tf-tensor变换</title>
    <url>/2021/05/27/tf-tensor-conversion/</url>
    <content><![CDATA[<blockquote>
<p>tensorflow中tensor变换的函数汇总。</p>
</blockquote>
<span id="more"></span>

<p><code>tf.stack</code><br>list-&gt;tensor<br>（tf.convert_to_tensor）</p>
<p><code>tf.unstack</code><br>tensor-&gt;list</p>
<p><code>tf.expand_dims</code><br>扩展维度</p>
<p><code>reduce_sum/max/mean</code><br>减少维度</p>
<p><code>tf.reshape</code><br>改变各维度大小（可增加减少维度）</p>
<p><code>tf.transpose</code><br>调换维度顺序</p>
<p><code>tf.tile</code><br>不改变维度的复制</p>
<p><code>tf.concat</code><br>不改变维度的拼接</p>
<p><code>tf.broadcast_to</code><br>广播</p>
]]></content>
      <categories>
        <category>code</category>
        <category>tensorflow</category>
      </categories>
  </entry>
  <entry>
    <title>每周知识碎片3</title>
    <url>/2022/03/31/weekly-220331/</url>
    <content><![CDATA[<blockquote>
<p>三月的最后一天！太阳很好的一天！huggingface🤗！</p>
</blockquote>
<span id="more"></span>

<h1 id="python中的-staticmethod方法"><a href="#python中的-staticmethod方法" class="headerlink" title="python中的@staticmethod方法"></a>python中的@staticmethod方法</h1><p>作用：方便外部函数集成到类中，可以<strong>在不实例化类的情况下直接访问该方法</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>:</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,num</span>):</span></span><br><span class="line">            self.num = num;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">cout_num</span>(<span class="params">self</span>):</span></span><br><span class="line">            <span class="built_in">print</span>(self.num)</span><br><span class="line"><span class="meta">      @staticmethod</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">print_num</span>():</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Hello World&quot;</span>)         </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">      <span class="string">&quot;&quot;&quot;实例化&quot;&quot;&quot;</span></span><br><span class="line">      obj = Test(<span class="number">10</span>)</span><br><span class="line">      <span class="string">&quot;&quot;&quot;实例化成员方法&quot;&quot;&quot;</span></span><br><span class="line">      obj.cout_num()</span><br><span class="line">      <span class="string">&quot;&quot;&quot;直接访问静态方法&quot;&quot;&quot;</span></span><br><span class="line">      Test.print_num()</span><br><span class="line">      <span class="string">&quot;&quot;&quot;实例化 访问静态方法&quot;&quot;&quot;</span></span><br><span class="line">      obj.print_num()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;输出结果&quot;&quot;&quot;</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line">Hello World</span><br><span class="line">Hello World</span><br></pre></td></tr></table></figure>

<p>在读embedding类的代码（<a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/models/WordEmbeddings.py">sentence-transformers/sentence-transformers/models/WordEmbeddings.py</a>）时看到该方法，实例化方法需要传入embedding_weight变量，通过staticmethod可以直接通过txt文件或者模型来创建实例，相当于提供了三种创建方法。</p>
]]></content>
      <categories>
        <category>weekly</category>
      </categories>
  </entry>
  <entry>
    <title>每周知识碎片4</title>
    <url>/2022/08/18/weekly-220809/</url>
    <content><![CDATA[<blockquote>
<p>新进入实验室的一个月！疯狂吸入零碎知识，加油哇！</p>
</blockquote>
<span id="more"></span>

<h1 id="1-shell脚本中set-e的作用"><a href="#1-shell脚本中set-e的作用" class="headerlink" title="1 shell脚本中set -e的作用"></a>1 shell脚本中set -e的作用</h1><p>linux系统的说明：”Exit immediately if a simple command exits with a non-zero status.”</p>
<p>在”set -e”之后出现的代码，一旦出现了返回值非零，整个脚本就会立即退出，那么就可以避免一些脚本的危险操作。</p>
<h1 id="2-python下载包报错"><a href="#2-python下载包报错" class="headerlink" title="2 python下载包报错"></a>2 python下载包报错</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ERROR: Could not find a version that satisfies the requirement difflib (from versions: none)</span><br><span class="line"></span><br><span class="line">ERROR: No matching distribution found for difflib</span><br></pre></td></tr></table></figure>

<p>可能是因为下载的包是python内置包，可以import一下试试。</p>
<h1 id="3-sh脚本执行报错"><a href="#3-sh脚本执行报错" class="headerlink" title="3 sh脚本执行报错"></a>3 sh脚本执行报错</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Syntax error: &quot;(&quot; unexpected</span><br></pre></td></tr></table></figure>

<p>报错原因：</p>
<p>在常见的linux发行版中，sh一般指向bash，但是有的系统debian、ubuntu上的sh默认指向dash，只支持基本的shell功能。</p>
<p>解决方式：用以下两种方式之一执行脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bash test.sh</span><br><span class="line">./test.sh</span><br></pre></td></tr></table></figure>

<h1 id="4-执行-sh文件总是提示permission-denied"><a href="#4-执行-sh文件总是提示permission-denied" class="headerlink" title="4 执行.sh文件总是提示permission denied"></a>4 执行.sh文件总是提示permission denied</h1><p>一种常见可能是，写代码的时候加入了不必要的空格或者字符，sh文件代码之间不需要空格，去掉一般可以解决。</p>
<h1 id="5-3090显卡和pytorch-cuda的适配"><a href="#5-3090显卡和pytorch-cuda的适配" class="headerlink" title="5 3090显卡和pytorch/cuda的适配"></a>5 3090显卡和pytorch/cuda的适配</h1><p>问题：</p>
<p>NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.<br>The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.</p>
<p><strong>RTX 3090</strong>显卡适配11.0以上版本的cuda</p>
<p>解决办法：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure>

<p><a href="https://blog.csdn.net/weixin_41596697/article/details/120455682">参考的博客</a>说不同时安装有依赖问题。</p>
]]></content>
      <categories>
        <category>weekly</category>
      </categories>
  </entry>
  <entry>
    <title>每周知识碎片1</title>
    <url>/2021/11/29/weekly-211129/</url>
    <content><![CDATA[<blockquote>
<p>疯狂写大作业的一周。</p>
</blockquote>
<span id="more"></span>

<h1 id="python-获取路径相关"><a href="#python-获取路径相关" class="headerlink" title="python 获取路径相关"></a>python 获取路径相关</h1><p>获取linux下的用户根路径</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> os</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>os.envrion[<span class="string">&#x27;HOME&#x27;</span>]</span><br><span class="line"><span class="string">&#x27;/home/user&#x27;</span></span><br></pre></td></tr></table></figure>

<p>连接路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import os</span><br><span class="line">&gt;&gt;&gt; os.path.join(os.envrion[&#39;HOME&#39;], &#39;xxx&#39;, &#39;yyy.txt&#39;)</span><br><span class="line">&#39;&#x2F;home&#x2F;user&#x2F;xxx&#x2F;yyy.txt&#39;</span><br></pre></td></tr></table></figure>

<p>glob返回所有匹配的文件路径列表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; from glob import glob</span><br><span class="line">&gt;&gt;&gt; files &#x3D; glob(os.path.join(ROOT, cat, &#39;*.txt&#39;))</span><br><span class="line">[&#39;xxx.txt&#39;, &#39;111.txt&#39;]</span><br></pre></td></tr></table></figure>

<h1 id="错误：AttributeError-module-‘threading’-has-no-attribute-‘RLock’"><a href="#错误：AttributeError-module-‘threading’-has-no-attribute-‘RLock’" class="headerlink" title="错误：AttributeError: module ‘threading’ has no attribute ‘RLock’"></a>错误：AttributeError: module ‘threading’ has no attribute ‘RLock’</h1><p><strong>错误原因</strong>：自己的代码文件与python自带的脚本文件重名，导致import引入包的时候引入错误。</p>
<p><strong>修改办法</strong>：根据错误提示，修改相关文件的文件名，要注意除了提示的包，在引入该包的过程中引入的其他包的错误也可能导致这个错误提示，例如该错误提示为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nohup: ignoring input</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;data/split_data.py&quot;, line 4, in &lt;module&gt;</span><br><span class="line">    import pandas as pd</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/pandas/__init__.py&quot;, line 11, in &lt;module&gt;</span><br><span class="line">    __import__(dependency)</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/numpy/__init__.py&quot;, line 142, in &lt;module&gt;</span><br><span class="line">    from . import core</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/numpy/core/__init__.py&quot;, line 103, in &lt;module&gt;</span><br><span class="line">    from . import _internal</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/numpy/core/_internal.py&quot;, line 11, in &lt;module&gt;</span><br><span class="line">    import platform</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/platform.py&quot;, line 117, in &lt;module&gt;</span><br><span class="line">    import sys, os, re, subprocess</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/subprocess.py&quot;, line 131, in &lt;module&gt;</span><br><span class="line">    import threading</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/threading.py&quot;, line 7, in &lt;module&gt;</span><br><span class="line">    from traceback import format_exc as _format_exc</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/traceback.py&quot;, line 5, in &lt;module&gt;</span><br><span class="line">    import linecache</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/linecache.py&quot;, line 11, in &lt;module&gt;</span><br><span class="line">    import tokenize</span><br><span class="line">  File &quot;/home/user/text-classify/Bert-THUCNews-Classification-master/data/tokenize.py&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    from pytorch_pretrained_bert import BertTokenizer</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/pytorch_pretrained_bert/__init__.py&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    from .tokenization import BertTokenizer, BasicTokenizer, WordpieceTokenizer</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/site-packages/pytorch_pretrained_bert/tokenization.py&quot;, line 20, in &lt;module&gt;</span><br><span class="line">    import logging</span><br><span class="line">  File &quot;/home/user/anaconda3/envs/bert/lib/python3.5/logging/__init__.py&quot;, line 212, in &lt;module&gt;</span><br><span class="line">    _lock = threading.RLock()</span><br><span class="line">AttributeError: module &#x27;threading&#x27; has no attribute &#x27;RLock&#x27;</span><br></pre></td></tr></table></figure>

<p>但并不是<code>threading</code>包有重名问题，而是在<code>import tokenize</code>这步中，本文件夹下有一个<code>tokenize.py</code>文件，可以看到引入的file路径和其他明显不同。</p>
<h1 id="错误：No-module-named-xxx"><a href="#错误：No-module-named-xxx" class="headerlink" title="错误：No module named xxx"></a>错误：No module named xxx</h1><p>引入自己写的代码中的文件时<code>from filename import xxx</code>，出现这个错误，检查下面两点：</p>
<p>1.在被引入(import)的目录下需要有一个<code>__init__.py</code>文件，该文件可以为空。</p>
<p>2.同级目录不可以直接导入，需要加上上一级的文件名，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from folder.filename import xxx</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>weekly</category>
      </categories>
  </entry>
  <entry>
    <title>每周知识碎片2</title>
    <url>/2022/01/20/weekly-220120/</url>
    <content><![CDATA[<blockquote>
<p>力扣力扣力扣！</p>
</blockquote>
<span id="more"></span>

<h1 id="python-初始化全零的二维列表"><a href="#python-初始化全零的二维列表" class="headerlink" title="python 初始化全零的二维列表"></a>python 初始化全零的二维列表</h1><p>初始化一个全零的n*m的二维列表：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list1=[[<span class="number">0</span>]*(m) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br></pre></td></tr></table></figure>

<p>错误的创建方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list2=[[<span class="number">0</span>]*m]*nde</span><br></pre></td></tr></table></figure>

<p>在后续修改值的过程中会出现问题：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list2[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(list2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[2, 0, 0], [2, 0, 0]]</span><br></pre></td></tr></table></figure>

<h1 id="python-初值默认的字典"><a href="#python-初值默认的字典" class="headerlink" title="python 初值默认的字典"></a>python 初值默认的字典</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">dict1 = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">dict2 = defaultdict(<span class="built_in">set</span>)</span><br><span class="line">dict3 = defaultdict(<span class="built_in">str</span>)</span><br><span class="line">dict4 = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dict1[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(dict2[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(dict3[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(dict4[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>在使用dict[n]时，不会出现key不存在的情况，会返回默认值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">set()</span><br><span class="line"></span><br><span class="line">[]</span><br></pre></td></tr></table></figure>

<h1 id="力扣-剑指offer-数组"><a href="#力扣-剑指offer-数组" class="headerlink" title="力扣-剑指offer-数组"></a>力扣-剑指offer-数组</h1><p>Ⅱ007：</p>
<p>排序后有一个顺序的信息可以利用，固定一个值（跳过省略和过大的值），另外两个用两个指针根据大小调整可以更快的查找。</p>
<p>Ⅱ010：</p>
<p>数组元素存在负数时不能用双指针滑窗，因为不满足条件就滑窗的性质不具备了，这是采用前缀和和哈希表。</p>
<p>Ⅱ011：</p>
<p>把0换成-1就和上一道题的和为k的子数组一样了，用字典记录一下最开头的相同前缀和的位置即可。</p>
<p>Ⅱ013：</p>
<p>二维前缀和，需要注意的是二维全零列表的初始化方式（错误的初始化方式会导致在后期赋值的时候出现错误）</p>
<h1 id="如何判断研究工作的价值（李沐）"><a href="#如何判断研究工作的价值（李沐）" class="headerlink" title="如何判断研究工作的价值（李沐）"></a>如何判断研究工作的价值（李沐）</h1><h2 id="一句话：用有新意的方法有效的解决一个研究问题。"><a href="#一句话：用有新意的方法有效的解决一个研究问题。" class="headerlink" title="一句话：用有新意的方法有效的解决一个研究问题。"></a>一句话：用有<strong>新意</strong>的方法<strong>有效</strong>的解决一个<strong>研究</strong>问题。</h2><p><strong>研究</strong>：<br>工程问题：例如内存，精度等问题，对于一个问题可以提出几个解决方案（买内存，标数据），并且在方案没有实验的时候就知道一定可以解决问题，那么属于工程问题。<br>研究问题：比较困难，可以想到的解决方案在实验前不能确定是否可以解决问题。</p>
<p><strong>有效</strong>：</p>
<p>相对于之前的工作，我解决这个问题的有效性有所提升。</p>
<p><strong>新意</strong>：</p>
<p>对所在研究社区里面的绝大多数研究者是有新意的，可能是很久之前的方法，或者在别的领域有研究者在使用的方法。</p>
<p><em>这也可以体现在自己写论文摘要的时候</em>：</p>
<p>我要解决一个什么研究问题，我用的方法新意在哪，最后我的结果如何。</p>
<h2 id="价值-新意度-有效性-问题大小"><a href="#价值-新意度-有效性-问题大小" class="headerlink" title="价值 = 新意度 * 有效性 * 问题大小"></a>价值 = 新意度 * 有效性 * 问题大小</h2><p><strong>问题大小</strong>：</p>
<p>1：对前一个工作不好的地方进行改进</p>
<p>10：对一个领域的某个子任务进行提升</p>
<p>100：提升机器对文字/图片的理解</p>
<p><strong>有效性</strong>：</p>
<p>1：可能多次实验精度提升不明显</p>
<p>10：一个点</p>
<p>100：五个点</p>
<p>效果好，规模做大（成本降低），安全</p>
<p><strong>新意度</strong>：</p>
<p>1：大家都不意外方法，同时看到方法的时候不意外结果</p>
<p>10：在某一个方面用某个技术聪明的解决问题</p>
<p>100：用了一个大家不熟悉的技术</p>
<p><em>在某一个指标上达到较好算一个有价值的工作。</em></p>
<h1 id="怎样读论文（李沐）"><a href="#怎样读论文（李沐）" class="headerlink" title="怎样读论文（李沐）"></a>怎样读论文（李沐）</h1><p><strong>第一遍</strong>：</p>
<p>标题、摘要、结论。可以看一看方法和实验部分重要的图和表。花费十几分钟时间了解到论文是否适合你的研究方向。</p>
<p><strong>第二遍</strong>：</p>
<p>确定论文值得读之后，可以快速的把整个论文过一遍，不需要知道所有的细节，需要了解重要的图和表，知道每一个部分在干什么，圈出相关文献。觉得文章太难，可以读引用的文献。</p>
<p><strong>第三遍</strong>：</p>
<p>要知道每一句话每一段在说什么，在心里复述一下文章提出什么问题，用什么方法来解决这个问题，实验是怎么做的。想一下如果是我来做我要怎么做，作者没有解决的问题我是否有合适的方法可以继续解决。合上文章，回忆每一个部分在讲什么。</p>
]]></content>
      <categories>
        <category>weekly</category>
      </categories>
  </entry>
  <entry>
    <title>data_preprocess</title>
    <url>/2021/11/27/data-preprocess/</url>
    <content><![CDATA[<blockquote>
<p>中英机器翻译的数据预处理流程，暂存一下看到的优秀博文，过段时间改成自用版本。</p>
</blockquote>
<span id="more"></span>

<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文在news-commentary-v15语料上训练了<strong>中英NMT模型</strong>，并将整个流程，包括工具和数据的准备、数据的预处理、训练及解码，以及中途遇到的问题和解决方案记录在此，希望能够给予别人一些帮助。</p>
<h1 id="1-相关工具及目录结构"><a href="#1-相关工具及目录结构" class="headerlink" title="1 相关工具及目录结构"></a>1 相关工具及目录结构</h1><h2 id="1-1-相关工具"><a href="#1-1-相关工具" class="headerlink" title="1.1 相关工具"></a>1.1 相关工具</h2><p>除<strong>jieba</strong>是使用<code>pip install</code>安装外，其他几个工具都是建议直接克隆库到自己的用户目录中，方便使用其脚本(<strong>moses</strong>/<strong>subword-nmt</strong>)，或未来可能要自己拓展其中的模型(<strong>fairseq</strong>)</p>
<ol>
<li><p>Moses (一个SMT工具，在这里只会用到一些预处理脚本，如：tokenisation,  truecasing, cleaning)，安装指令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;moses-smt&#x2F;mosesdecoder.git</span><br></pre></td></tr></table></figure></li>
<li><p>subword-nmt (使用BPE算法生成子词的预处理脚本)，安装指令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;rsennrich&#x2F;subword-nmt.git</span><br></pre></td></tr></table></figure></li>
<li><p>jieba (中文分词组件)，安装指令如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure></li>
<li><p>fairseq (一个基于PyTorch的序列建模工具)，安装指令如下：</p>
<p>fairseq安装参考：<a href="https://zhuanlan.zhihu.com/p/194176917">https://zhuanlan.zhihu.com/p/194176917</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;fairseq</span><br><span class="line">cd fairseq</span><br><span class="line">pip install --editable .&#x2F;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="1-2-目录结构与初始化"><a href="#1-2-目录结构与初始化" class="headerlink" title="1.2 目录结构与初始化"></a>1.2 目录结构与初始化</h2><h3 id="1-2-1-目录结构"><a href="#1-2-1-目录结构" class="headerlink" title="1.2.1 目录结构"></a>1.2.1 目录结构</h3><p>提前组织一个目录结构的好处是可以让后面的一系列操作更加统一、规范化。下表中<code>~</code>代表linux系统中<strong>我的用户目录</strong>, v15news目录名代表此次我使用的数据集名称</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~</span><br><span class="line">├── mosesdecoder</span><br><span class="line">├── subword-nmt</span><br><span class="line">├── fairseq</span><br><span class="line">└── nmt</span><br><span class="line">    ├── data</span><br><span class="line">        └── v15news</span><br><span class="line">            ├── result          # 用于存放翻译结果</span><br><span class="line">            └── data-bin        # 用于存放二进制文件</span><br><span class="line">    ├── models                  # 用于保存过程中的model文件和checkpoint</span><br><span class="line">        └── v15news</span><br><span class="line">            └── checkpoints     # 保存checkpoints</span><br><span class="line">    ├── utils                   # 一些其他工具</span><br><span class="line">        ├── split.py            # 用于划分train,valid,test</span><br><span class="line">        └── cut2.py             # 用于划分src,tgt</span><br><span class="line">    └── scripts                 # 一些脚本</span><br></pre></td></tr></table></figure>

<h3 id="1-2-2-用于初始化的bash文件"><a href="#1-2-2-用于初始化的bash文件" class="headerlink" title="1.2.2 用于初始化的bash文件"></a>1.2.2 用于初始化的bash文件</h3><p>这个文件是在上述目录结构的基础下，定义了一些后面需要用到的变量(主要是<strong>各种脚本的路径</strong>)，包括tokenizer.perl, truecase.perl等，可以在linux中使用bash xx.sh运行，也可以把这些内容直接全部复制到linux命令行中按回车</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line"></span><br><span class="line">src&#x3D;zh</span><br><span class="line">tgt&#x3D;en</span><br><span class="line"></span><br><span class="line">SCRIPTS&#x3D;~&#x2F;mosesdecoder&#x2F;scripts</span><br><span class="line">TOKENIZER&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;tokenizer.perl</span><br><span class="line">DETOKENIZER&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;detokenizer.perl</span><br><span class="line">LC&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;lowercase.perl</span><br><span class="line">TRAIN_TC&#x3D;$&#123;SCRIPTS&#125;&#x2F;recaser&#x2F;train-truecaser.perl</span><br><span class="line">TC&#x3D;$&#123;SCRIPTS&#125;&#x2F;recaser&#x2F;truecase.perl</span><br><span class="line">DETC&#x3D;$&#123;SCRIPTS&#125;&#x2F;recaser&#x2F;detruecase.perl</span><br><span class="line">NORM_PUNC&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;normalize-punctuation.perl</span><br><span class="line">CLEAN&#x3D;$&#123;SCRIPTS&#125;&#x2F;training&#x2F;clean-corpus-n.perl</span><br><span class="line">BPEROOT&#x3D;~&#x2F;subword-nmt&#x2F;subword_nmt</span><br><span class="line">MULTI_BLEU&#x3D;$&#123;SCRIPTS&#125;&#x2F;generic&#x2F;multi-bleu.perl</span><br><span class="line">MTEVAL_V14&#x3D;$&#123;SCRIPTS&#125;&#x2F;generic&#x2F;mteval-v14.pl</span><br><span class="line"></span><br><span class="line">data_dir&#x3D;~&#x2F;nmt&#x2F;data&#x2F;v15news</span><br><span class="line">model_dir&#x3D;~&#x2F;nmt&#x2F;models&#x2F;v15news</span><br><span class="line">utils&#x3D;~&#x2F;nmt&#x2F;utils</span><br></pre></td></tr></table></figure>

<h1 id="2-数据的准备"><a href="#2-数据的准备" class="headerlink" title="2 数据的准备"></a>2 数据的准备</h1><h2 id="2-1-平行语料"><a href="#2-1-平行语料" class="headerlink" title="2.1 平行语料"></a>2.1 平行语料</h2><p>对于有监督神经机器翻译，能够找到的中英平行语料如下：</p>
<ol>
<li><a href="https://github.com/NiuTrans/NiuTrans.SMT/tree/master/sample-data">NEU nlp lab 开源语料</a> (10w，国内政治新闻领域)</li>
<li><a href="http://www.statmt.org/wmt20/translation-task.html">WMT新闻翻译任务News Commentary语料</a> (32w左右，国际新闻领域。其实News Commentary每年都有新闻数据集，但是基本没啥变化，每次在前一年的基础上加几百句，所以这里的链接直接指向最新的WMT20)</li>
<li><a href="https://catalog.ldc.upenn.edu/LDC2010T21">NIST数据集</a> (200w左右，需要购买)</li>
<li><a href="https://conferences.unite.un.org/UNCORPUS/zh">United Nations Parallel Corpus</a> (1500w左右，联合国文件领域)</li>
</ol>
<p>我本人使用过语料1、3，其中3是跟已购买的师兄要的，不向外提供。其实初次训练建议使用语料1，规模小训练快，能够快速体验整个流程。当然，中英还有很多其他语料，见<a href="https://chinesenlp.xyz/#/docs/machine_translation">参考资料1</a>, <a href="https://www.cluebenchmarks.com/dataSet_search.html">2</a></p>
<h2 id="2-2-数据预处理"><a href="#2-2-数据预处理" class="headerlink" title="2.2 数据预处理"></a>2.2 数据预处理</h2><h3 id="2-2-1-数据格式"><a href="#2-2-1-数据格式" class="headerlink" title="2.2.1 数据格式"></a>2.2.1 数据格式</h3><p>在本篇博客中，我准备使用WMT20新闻翻译任务的<strong>news-commentary-v15语料</strong>，放于以下位置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">└── nmt</span><br><span class="line">    ├── data</span><br><span class="line">        └── v15news     </span><br><span class="line">            └── news-commentary-v15.en-zh.tsv</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>格式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1929 or 1989?	1929年还是1989年?</span><br><span class="line">PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.	巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。</span><br><span class="line">At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.	一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="2-2-2-切分"><a href="#2-2-2-切分" class="headerlink" title="2.2.2 切分"></a>2.2.2 切分</h3><p>首先，需要将一个单独的数据文件切分成标准格式，即源语言(raw.zh)、目标语言(raw.en)文件各一个，一行一句，附自己写的脚本(~/nmt/utils/cut2.py)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">Usage: </span><br><span class="line">python cut2.py fpath new_data_dir</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">def cut2(fpath, new_data_dir, nsrc&#x3D;&#39;zh&#39;, ntgt&#x3D;&#39;en&#39;):</span><br><span class="line">    fp &#x3D; open(fpath, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">    src_fp &#x3D; open(new_data_dir + &#39;raw.&#39; + nsrc, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">    tgt_fp &#x3D; open(new_data_dir + &#39;raw.&#39; + ntgt, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">    for line in fp.readlines():</span><br><span class="line">        tgt_line, src_line &#x3D; line.replace(&#39;\n&#39;, &#39;&#39;).split(&#39;\t&#39;)</span><br><span class="line">        src_fp.write(src_line + &#39;\n&#39;)</span><br><span class="line">        tgt_fp.write(tgt_line + &#39;\n&#39;)</span><br><span class="line">    src_fp.close()</span><br><span class="line">    tgt_fp.close()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:      </span><br><span class="line">    cut2(fpath&#x3D;sys.argv[1], new_data_dir&#x3D;sys.argv[2], nsrc&#x3D;&#39;zh&#39;, ntgt&#x3D;&#39;en&#39;)</span><br></pre></td></tr></table></figure>

<p>使用命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python $&#123;utils&#125;&#x2F;cut2.py $&#123;data_dir&#125;&#x2F;news-commentary-v15.en-zh.tsv $&#123;data_dir&#125;&#x2F;</span><br></pre></td></tr></table></figure>

<p><strong>后注：</strong> 在linux里可以直接用cut实现 <code>cut -f 1 fpath &gt; new_data_dir/raw.en | cut -f 2 fpath &gt; new_data_dir/raw.zh</code></p>
<p>切分后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ├── news-commentary-v15.en-zh.tsv</span><br><span class="line">        ├── raw.zh</span><br><span class="line">        └── raw.en</span><br></pre></td></tr></table></figure>

<h3 id="2-2-3-normalize-punctuation-可选"><a href="#2-2-3-normalize-punctuation-可选" class="headerlink" title="2.2.3 normalize-punctuation(可选)"></a>2.2.3 normalize-punctuation(可选)</h3><p>标点符号的标准化，同时对双语文件(raw.en, raw.zh)处理，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">perl $&#123;NORM_PUNC&#125; -l en &lt; $&#123;data_dir&#125;&#x2F;raw.en &gt; $&#123;data_dir&#125;&#x2F;norm.en</span><br><span class="line">perl $&#123;NORM_PUNC&#125; -l zh &lt; $&#123;data_dir&#125;&#x2F;raw.zh &gt; $&#123;data_dir&#125;&#x2F;norm.zh</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ...</span><br><span class="line">        ├── norm.zh</span><br><span class="line">        └── norm.en</span><br></pre></td></tr></table></figure>

<p>效果如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># raw.en</span><br><span class="line">“We can’t waste time,” he says.</span><br><span class="line">Yet, according to the political economist Moeletsi Mbeki, at his core, “Zuma is a conservative.”</span><br><span class="line"></span><br><span class="line"># norm.en</span><br><span class="line">&quot;We can&#39;t waste time,&quot; he says.</span><br><span class="line">Yet, according to the political economist Moeletsi Mbeki, at his core, &quot;Zuma is a conservative.&quot;</span><br></pre></td></tr></table></figure>

<h3 id="2-2-4-中文分词"><a href="#2-2-4-中文分词" class="headerlink" title="2.2.4 中文分词"></a>2.2.4 中文分词</h3><p>对标点符号标准化后的中文文件(norm.zh)进行分词处理，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m jieba -d &quot; &quot; $&#123;data_dir&#125;&#x2F;norm.zh &gt; $&#123;data_dir&#125;&#x2F;norm.seg.zh</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ...</span><br><span class="line">        └── norm.seg.zh</span><br></pre></td></tr></table></figure>

<p>效果如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># norm.zh</span><br><span class="line">1929年还是1989年?</span><br><span class="line">巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。</span><br><span class="line">一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。</span><br><span class="line"></span><br><span class="line"># norm.seg.zh</span><br><span class="line">1929 年 还是 1989 年 ?</span><br><span class="line">巴黎 - 随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 。</span><br><span class="line">一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 情况 ， 这样 得 类比 是 令人 宽心 的 ， 因为 这 两段 时期 意味着 典型 的 周期性 衰退 。</span><br></pre></td></tr></table></figure>

<h3 id="2-2-5-tokenize"><a href="#2-2-5-tokenize" class="headerlink" title="2.2.5 tokenize"></a>2.2.5 tokenize</h3><p>对上述处理后的双语文件(norm.en, norm.seg.zh)进行标记化处理，有很多功能(1.将<strong>英文单词</strong>与<strong>标点符号</strong>用空格分开 2.将多个连续空格简化为一个空格 3.将很多符号替换成转义字符，如：把<code>&quot;</code>替换成<code>&quot;</code>、把<code>can&#39;t</code>替换成<code>can &#39;t</code>)，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;TOKENIZER&#125; -l en &lt; $&#123;data_dir&#125;&#x2F;norm.en &gt; $&#123;data_dir&#125;&#x2F;norm.tok.en</span><br><span class="line">$&#123;TOKENIZER&#125; -l zh &lt; $&#123;data_dir&#125;&#x2F;norm.seg.zh &gt; $&#123;data_dir&#125;&#x2F;norm.seg.tok.zh</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ...</span><br><span class="line">        ├── norm.tok.en</span><br><span class="line">        └── norm.seg.tok.zh</span><br></pre></td></tr></table></figure>

<p>效果如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># norm.seg.zh</span><br><span class="line">目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲   ）   ，   要么 是 努力 的 扩展 （ 美国   ）   。</span><br><span class="line">而 历史 是 不 公平 的 。   尽管 美国 要 为 当今 的 全球 危机 负 更 大 的 责任 ， 但 美国 可能 会 比 大多数 国家 以 更 良好 的 势态 走出 困境 。</span><br><span class="line"></span><br><span class="line"># norm.seg.tok.zh</span><br><span class="line">目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲 ） ， 要么 是 努力 的 扩展 （ 美国 ） 。</span><br><span class="line">而 历史 是 不 公平 的 。 尽管 美国 要 为 当今 的 全球 危机 负 更 大 的 责任 ， 但 美国 可能 会 比 大多数 国家 以 更 良好 的 势态 走出 困境 。</span><br><span class="line"></span><br><span class="line"># norm.en</span><br><span class="line">&quot;We can&#39;t waste time,&quot; he says.</span><br><span class="line">Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.</span><br><span class="line">Second, Zoellick should ask why the Bank spends only 2.5% of its budget on the &quot;knowledge bank&quot; research function that it trumpets so proudly in its external relations materials, while it spends three times that amount on maintaining its executive board.</span><br><span class="line"></span><br><span class="line"># norm.tok.en</span><br><span class="line">&quot; We can &amp;apos;t waste time , &quot; he says .</span><br><span class="line">Of course , the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall .</span><br><span class="line">Second , Zoellick should ask why the Bank spends only 2.5 % of its budget on the &quot; knowledge bank &quot; research function that it trumpets so proudly in its external relations materials , while it spends three times that amount on maintaining its executive board .</span><br></pre></td></tr></table></figure>

<h3 id="2-2-6-truecase"><a href="#2-2-6-truecase" class="headerlink" title="2.2.6 truecase"></a>2.2.6 truecase</h3><p>对上述处理后的英文文件(norm.tok.en)进行大小写转换处理(对于句中的每个英文单词，尤其是<strong>句首单词</strong>，在数据中<strong>学习</strong>最适合它们的大小写形式)，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;TRAIN_TC&#125; --model $&#123;model_dir&#125;&#x2F;truecase-model.en --corpus $&#123;data_dir&#125;&#x2F;norm.tok.en</span><br><span class="line">$&#123;TC&#125; --model $&#123;model_dir&#125;&#x2F;truecase-model.en &lt; $&#123;data_dir&#125;&#x2F;norm.tok.en &gt; $&#123;data_dir&#125;&#x2F;norm.tok.true.en</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        └── norm.tok.true.en</span><br><span class="line">├── models</span><br><span class="line">    └── v15news</span><br><span class="line">        └── truecase-model.en</span><br></pre></td></tr></table></figure>

<p>效果如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># norm.tok.en</span><br><span class="line">PARIS - As the economic crisis deepens and widens , the world has been searching for historical analogies to help us understand what has been happening .</span><br><span class="line">At the start of the crisis , many people likened it to 1982 or 1973 , which was reassuring , because both dates refer to classical cyclical downturns .</span><br><span class="line">When the TTIP was first proposed , Europe seemed to recognize its value .</span><br><span class="line">Europe is being cautious in the name of avoiding debt and defending the euro , whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms .</span><br><span class="line"></span><br><span class="line"># norm.tok.true.en</span><br><span class="line">Paris - As the economic crisis deepens and widens , the world has been searching for historical analogies to help us understand what has been happening .</span><br><span class="line">at the start of the crisis , many people likened it to 1982 or 1973 , which was reassuring , because both dates refer to classical cyclical downturns .</span><br><span class="line">when the TTIP was first proposed , Europe seemed to recognize its value .</span><br><span class="line">Europe is being cautious in the name of avoiding debt and defending the euro , whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms .</span><br></pre></td></tr></table></figure>

<h3 id="2-2-7-bpe"><a href="#2-2-7-bpe" class="headerlink" title="2.2.7 bpe"></a>2.2.7 bpe</h3><p>对上述处理后的双语文件(norm.tok.true.en, norm.seg.tok.zh)进行子词处理(可以理解为更细粒度的分词)，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python $&#123;BPEROOT&#125;&#x2F;learn_joint_bpe_and_vocab.py --input $&#123;data_dir&#125;&#x2F;norm.tok.true.en  -s 32000 -o $&#123;model_dir&#125;&#x2F;bpecode.en --write-vocabulary $&#123;model_dir&#125;&#x2F;voc.en</span><br><span class="line">python $&#123;BPEROOT&#125;&#x2F;apply_bpe.py -c $&#123;model_dir&#125;&#x2F;bpecode.en --vocabulary $&#123;model_dir&#125;&#x2F;voc.en &lt; $&#123;data_dir&#125;&#x2F;norm.tok.true.en &gt; $&#123;data_dir&#125;&#x2F;norm.tok.true.bpe.en</span><br><span class="line"></span><br><span class="line">python $&#123;BPEROOT&#125;&#x2F;learn_joint_bpe_and_vocab.py --input $&#123;data_dir&#125;&#x2F;norm.seg.tok.zh  -s 32000 -o $&#123;model_dir&#125;&#x2F;bpecode.zh --write-vocabulary $&#123;model_dir&#125;&#x2F;voc.zh</span><br><span class="line">python $&#123;BPEROOT&#125;&#x2F;apply_bpe.py -c $&#123;model_dir&#125;&#x2F;bpecode.zh --vocabulary $&#123;model_dir&#125;&#x2F;voc.zh &lt; $&#123;data_dir&#125;&#x2F;norm.seg.tok.zh &gt; $&#123;data_dir&#125;&#x2F;norm.seg.tok.bpe.zh</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── norm.seg.tok.bpe.zh</span><br><span class="line">        └── norm.tok.true.bpe.en</span><br><span class="line">├── models</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── voc.zh</span><br><span class="line">        ├── voc.en</span><br><span class="line">        ├── bpecode.zh</span><br><span class="line">        └── bpecode.en</span><br></pre></td></tr></table></figure>

<p>效果如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># norm.seg.tok.zh</span><br><span class="line">从 一流 的 麻省理工学院 的 媒体 实验室 到 哈佛大学 的 数学 和 经济系 ， 亚洲 人 - 尤其 是 中国 和 印度人 - 到处 都 是 ， 犹如 公元前 一 世纪 在 雅典 的 罗马 人 一样 ： 他们 对 那里 学到 太 多 东西 的 人们 充满 了 敬佩 ， 而 他们 将 在 今后 几十年 打败 他们 学习 的 对象 。</span><br><span class="line">这 不仅 加大 了 预防 危机 的 难度 - - 尤其 因为 它 为 参与者 提供 了 钻空子 和 逃避责任 的 机会 - - 还 使得 人们 越来越 难以 采取措施 来 应对 危机 。</span><br><span class="line">它们 将 通胀 目标 设定 在 2 % 左右 - - 这 意味着 当 波涛汹涌 时 他们 根本 没有 多少 施展 空间 。</span><br><span class="line"></span><br><span class="line"># norm.seg.tok.bpe.zh</span><br><span class="line">从 一流 的 麻省理工学院 的 媒体 实验室 到 哈佛大学 的 数学 和 经济@@ 系 ， 亚洲 人 - 尤其 是 中国 和 印度人 - 到处 都 是 ， 犹如 公元前 一 世纪 在 雅典 的 罗马 人 一样 ： 他们 对 那里 学到 太 多 东西 的 人们 充满 了 敬佩 ， 而 他们 将 在 今后 几十年 打败 他们 学习 的 对象 。</span><br><span class="line">这 不仅 加大 了 预防 危机 的 难度 - - 尤其 因为 它 为 参与者 提供 了 钻@@ 空子 和 逃避@@ 责任 的 机会 - - 还 使得 人们 越来越 难以 采取措施 来 应对 危机 。</span><br><span class="line">它们 将 通胀 目标 设定 在 2 % 左右 - - 这 意味着 当 波@@ 涛@@ 汹涌 时 他们 根本 没有 多少 施展 空间 。</span><br><span class="line"></span><br><span class="line"># norm.tok.true.en</span><br><span class="line">indeed , on the surface it seems to be its perfect antithesis : the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism .</span><br><span class="line">as a visiting professor at Harvard and MIT , I am getting a good preview of what the world could look like when the crisis finally passes .</span><br><span class="line">one senses something like the making of an American-Asian dominated universe .</span><br><span class="line"></span><br><span class="line"># norm.tok.true.bpe.en</span><br><span class="line">indeed , on the surface it seems to be its perfect anti@@ thesis : the collapse of a wall symboli@@ zing oppression and artificial divisions versus the collapse of a seemingly inde@@ struc@@ tible and reassuring institution of financial capitalism .</span><br><span class="line">as a visiting professor at Harvard and MIT , I am getting a good pre@@ view of what the world could look like when the crisis finally passes .</span><br><span class="line">one senses something like the making of an American-@@ Asian dominated universe .</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>后注：</strong><br>需要注意的是，我为了方便，步骤上失去了一些正确性。正确的做法应该是在<strong>训练集</strong>中学习bpe模型，再将bpe模型应用到<strong>测试集</strong>和<strong>验证集</strong>中。而我是直接在全部数据中学bpe模型了。</p>
</blockquote>
<h3 id="2-2-8-clean"><a href="#2-2-8-clean" class="headerlink" title="2.2.8 clean"></a>2.2.8 clean</h3><p>对上述处理后的双语文件(norm.tok.true.bpe.en, norm.seg.tok.bpe.zh)进行过滤(可以过滤<strong>最小长度</strong>和<strong>最大长度</strong>之间的句对，这样能够有效过滤空白行。还可以过滤<strong>长度比</strong>不合理的句对)，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mv $&#123;data_dir&#125;&#x2F;norm.seg.tok.bpe.zh $&#123;data_dir&#125;&#x2F;toclean.zh</span><br><span class="line">mv $&#123;data_dir&#125;&#x2F;norm.tok.true.bpe.en $&#123;data_dir&#125;&#x2F;toclean.en </span><br><span class="line">$&#123;CLEAN&#125; $&#123;data_dir&#125;&#x2F;toclean zh en $&#123;data_dir&#125;&#x2F;clean 1 256</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── clean.zh</span><br><span class="line">        └── clean.en</span><br></pre></td></tr></table></figure>

<p>效果如下(每行最开始标出了<strong>行号</strong>):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># norm.tok.true.bpe.en</span><br><span class="line">30 we can only hope that , in the end , the consequences of 2009 similarly prove to be far less dramatic than we now - intuitively and in our historical refle@@ xes - feel them to be .</span><br><span class="line">31</span><br><span class="line">32 one Hund@@ red Years of Ine@@ p@@ titude</span><br><span class="line"></span><br><span class="line"># clean.en</span><br><span class="line">30 we can only hope that , in the end , the consequences of 2009 similarly prove to be far less dramatic than we now - intuitively and in our historical refle@@ xes - feel them to be .</span><br><span class="line">31 one Hund@@ red Years of Ine@@ p@@ titude</span><br><span class="line">32 Berlin - The global financial and economic crisis that began in 2008 was the greatest economic stre@@ ss-@@ test since the Great Depression , and the greatest challenge to social and political systems since World War II .</span><br><span class="line"></span><br><span class="line"># norm.seg.tok.bpe.zh</span><br><span class="line">30 我们 只能 希望 2009 年 的 危机 同样 地 最后 被 证明 是 远远 低于 我们 现在 以 直觉 和 历史 回顾 的 方式 � � 感觉 到 的 那么 剧烈 。</span><br><span class="line">31 </span><br><span class="line">32 百年 愚@@ 顽</span><br><span class="line"></span><br><span class="line"># clean.zh</span><br><span class="line">30 我们 只能 希望 2009 年 的 危机 同样 地 最后 被 证明 是 远远 低于 我们 现在 以 直觉 和 历史 回顾 的 方式 � � 感觉 到 的 那么 剧烈 。</span><br><span class="line">31 百年 愚@@ 顽</span><br><span class="line">32 柏林 - - 2008 年 爆发 的 全球 金融 和 经济危机 是 自大 萧条 以来 最 严峻 的 一次 经济 压力 测试 ， 也 是 自 二战 以来 社会 和 政治 制度 所 面临 的 最 严重 挑战 。</span><br></pre></td></tr></table></figure>

<h3 id="2-2-9-split"><a href="#2-2-9-split" class="headerlink" title="2.2.9 split"></a>2.2.9 split</h3><p>最后，双语文件(clean.zh, clean.en)都需要按比例划分出训练集、测试集、开发集(所以共6个文件，为方便区分，直接以 ‘train.en’, ‘valid.zh’ 这样的格式命名)，附自己写的脚本(~/nmt/utils/split.py)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">Usage:</span><br><span class="line">python split.py src_fpath tgt_fpath new_data_dir</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">def split(src_fpath, tgt_fpath, nsrc&#x3D;&#39;zh&#39;, ntgt&#x3D;&#39;en&#39;, ratio&#x3D;(0.9, 0.05, 0.05), new_data_dir&#x3D;&#39;&#39;):</span><br><span class="line">  src_fp &#x3D; open(src_fpath, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  tgt_fp &#x3D; open(tgt_fpath, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  </span><br><span class="line">  src_train, src_test, src_val &#x3D; open(new_data_dir + &#39;train.&#39; + nsrc, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), \</span><br><span class="line">    open(new_data_dir + &#39;test.&#39; + nsrc, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), open(new_data_dir + &#39;valid.&#39; + nsrc, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  tgt_train, tgt_test, tgt_val &#x3D; open(new_data_dir + &#39;train.&#39; + ntgt, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), \</span><br><span class="line">    open(new_data_dir + &#39;test.&#39; + ntgt, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), open(new_data_dir + &#39;valid.&#39; + ntgt, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  </span><br><span class="line">  src, tgt &#x3D; src_fp.readlines(), tgt_fp.readlines()</span><br><span class="line">  for s, t in zip(src, tgt):</span><br><span class="line">      rand &#x3D; random.random()</span><br><span class="line">      if 0 &lt; rand &lt;&#x3D; ratio[0]:</span><br><span class="line">        src_train.write(s)</span><br><span class="line">        tgt_train.write(t)</span><br><span class="line">      elif ratio[0] &lt; rand &lt;&#x3D; ratio[0] + ratio[1]:</span><br><span class="line">        src_test.write(s)</span><br><span class="line">        tgt_test.write(t)</span><br><span class="line">      else:</span><br><span class="line">        src_val.write(s)</span><br><span class="line">        tgt_val.write(t)</span><br><span class="line">  </span><br><span class="line">  src_fp.close()</span><br><span class="line">  tgt_fp.close()</span><br><span class="line">  src_train.close()</span><br><span class="line">  src_test.close()</span><br><span class="line">  src_val.close()</span><br><span class="line">  tgt_train.close()</span><br><span class="line">  tgt_test.close()</span><br><span class="line">  tgt_val.close()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:      </span><br><span class="line">    split(src_fpath&#x3D;sys.argv[1], tgt_fpath&#x3D;sys.argv[2], nsrc&#x3D;&#39;zh&#39;, ntgt&#x3D;&#39;en&#39;, ratio&#x3D;(0.95, 0.025, 0.025), new_data_dir&#x3D;sys.argv[3])</span><br></pre></td></tr></table></figure>

<p>使用命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python $&#123;utils&#125;&#x2F;split.py $&#123;data_dir&#125;&#x2F;clean.zh $&#123;data_dir&#125;&#x2F;clean.en $&#123;data_dir&#125;&#x2F;</span><br></pre></td></tr></table></figure>

<p>最后，data/v15news目录中有如下数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── test.en</span><br><span class="line">        ├── test.zh</span><br><span class="line">        ├── train.en</span><br><span class="line">        ├── train.zh</span><br><span class="line">        ├── valid.en</span><br><span class="line">        └── valid.zh</span><br></pre></td></tr></table></figure>

<h1 id="3-训练过程"><a href="#3-训练过程" class="headerlink" title="3 训练过程"></a>3 训练过程</h1><h2 id="3-1-生成词表及二进制文件"><a href="#3-1-生成词表及二进制文件" class="headerlink" title="3.1 生成词表及二进制文件"></a>3.1 生成词表及二进制文件</h2><p>首先用预处理后的六个文件(train.zh, valid.en等)，使用<code>fairseq-preprocess</code>命令生成<strong>词表</strong>和<strong>训练用的二进制文件</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fairseq-preprocess --source-lang $&#123;src&#125; --target-lang $&#123;tgt&#125; \</span><br><span class="line">    --trainpref $&#123;data_dir&#125;&#x2F;train --validpref $&#123;data_dir&#125;&#x2F;valid --testpref $&#123;data_dir&#125;&#x2F;test \</span><br><span class="line">    --destdir $&#123;data_dir&#125;&#x2F;data-bin</span><br></pre></td></tr></table></figure>

<p>生成的文件都保存在data-bin目录中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        └── data-bin</span><br><span class="line">            ├── dict.zh</span><br><span class="line">            ├── dict.en</span><br><span class="line">            ├── preprocess.log</span><br><span class="line">            ├── train.zh-en.zh.idx</span><br><span class="line">            ...</span><br><span class="line">            └── valid.zh-en.en.bin</span><br></pre></td></tr></table></figure>

<p>需要提醒的是：训练阶段使用的是<strong>训练集</strong>和<strong>验证集</strong>，解码阶段使用的是<strong>测试集</strong></p>
<h2 id="3-2-训练"><a href="#3-2-训练" class="headerlink" title="3.2 训练"></a>3.2 训练</h2><p>使用<code>fairseq-train</code>命令进行训练，其中有很多可以自由设置的超参数，比如选择使用什么模型，模型的参数等。其中，<code>--save-dir</code> 这个参数是指每一个epoch结束后模型保存的位置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0,1,2,3 nohup fairseq-train $&#123;data_dir&#125;&#x2F;data-bin --arch transformer \</span><br><span class="line">	--source-lang $&#123;src&#125; --target-lang $&#123;tgt&#125;  \</span><br><span class="line">    --optimizer adam  --lr 0.001 --adam-betas &#39;(0.9, 0.98)&#39; \</span><br><span class="line">    --lr-scheduler inverse_sqrt --max-tokens 4096  --dropout 0.3 \</span><br><span class="line">    --criterion label_smoothed_cross_entropy  --label-smoothing 0.1 \</span><br><span class="line">    --max-update 200000  --warmup-updates 4000 --warmup-init-lr &#39;1e-07&#39; \</span><br><span class="line">    --keep-last-epochs 10 --num-workers 8 \</span><br><span class="line">	--save-dir $&#123;model_dir&#125;&#x2F;checkpoints &amp;</span><br></pre></td></tr></table></figure>

<p>我自己训练时是在3块GTX TITAN X卡上跑了6个小时，共跑了49个epoch，但是在第22个epoch的时候已经收敛(只需要看验证集上的ppl的变化即可)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">epoch 020 | valid on &#39;valid&#39; subset | loss 4.366 | nll_loss 2.652 | ppl 6.29 | wps 50387.3 | wpb 8026 | bsz 299.8 | num_updates 14400 | best_loss 4.366</span><br><span class="line">epoch 021 | valid on &#39;valid&#39; subset | loss 4.36 | nll_loss 2.647 | ppl 6.27 | wps 51992.7 | wpb 8026 | bsz 299.8 | num_updates 15120 | best_loss 4.36</span><br><span class="line">epoch 022 | valid on &#39;valid&#39; subset | loss 4.361 | nll_loss 2.644 | ppl 6.25 | wps 49009.9 | wpb 8026 | bsz 299.8 | num_updates 15840 | best_loss 4.36</span><br><span class="line">epoch 023 | valid on &#39;valid&#39; subset | loss 4.369 | nll_loss 2.65 | ppl 6.28 | wps 51878.9 | wpb 8026 | bsz 299.8 | num_updates 16560 | best_loss 4.36</span><br><span class="line">epoch 023 | valid on &#39;valid&#39; subset | loss 4.369 | nll_loss 2.65 | ppl 6.28 | wps 51878.9 | wpb 8026 | bsz 299.8 | num_updates 16560 | best_loss 4.36</span><br></pre></td></tr></table></figure>

<p>由于<code>--keep-last-epochs</code>这个参数我设为10，所以我最后10个epoch的模型都保存在以下目录中。此外，还会额外保存效果最好的模型(即第22个epoch)和最后一个模型(即第49个epoch，可以用于下一次训练)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── models</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        └── checkpoints</span><br><span class="line">            ├── checkpoint40.pt</span><br><span class="line">            ...</span><br><span class="line">            ├── checkpoint49.pt</span><br><span class="line">            ├── checkpoint_best.pt</span><br><span class="line">            └── checkpoint_last.pt</span><br></pre></td></tr></table></figure>

<h2 id="3-3-解码"><a href="#3-3-解码" class="headerlink" title="3.3 解码"></a>3.3 解码</h2><p>fairseq中支持两种解码命令<code>generate</code>和<code>interactive</code>。</p>
<p>其区别很简单，<code>generate</code>使用<strong>二进制文件</strong>，这个二进制文件是在<code>fairseq-preprocess</code>过程生成的，当时提供了一个<code>testpref</code>参数。也就是说测试集的src和tgt都是已获得的，这种场景符合自己在公开的数据集上做实验（如WMT14en-de），需要在论文中报告测试集结果。</p>
<p>而<code>interactive</code>用于文本文件，也就是说不需要二进制文件，在<code>fairseq-preprocess</code>中也就不需要提供<code>testpref</code>参数。这种场景符合在比赛中，比赛方只提供测试集中的src部分，需要自己来解码得到tgt，并最终提交。</p>
<h3 id="3-3-1-生成式解码"><a href="#3-3-1-生成式解码" class="headerlink" title="3.3.1 生成式解码"></a>3.3.1 生成式解码</h3><p>使用<code>fairseq-generate</code>命令进行生成式解码(<strong>用于预处理后的二进制文件</strong>)，可以自行选择是否添加<code>--remove-bpe</code>参数，使得在生成时就去掉bpe符号(@@)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fairseq-generate $&#123;data_dir&#125;&#x2F;data-bin \</span><br><span class="line">    --path $&#123;model_dir&#125;&#x2F;checkpoints&#x2F;checkpoint_best.pt \</span><br><span class="line">    --batch-size 128 --beam 8 &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt</span><br></pre></td></tr></table></figure>

<p>选取一部分结果展示如下 (<strong>S</strong>: 源句子，<strong>T</strong>: 目标句子，<strong>H/D</strong>: 预测的句子及其生成概率的log，句子质量越好，其生成概率越接近1，其log越接近0。<strong>P</strong>: 每一个词的生成概率的log。其中，H=\frac{\sum P}{n}<em>H</em>=<em>n</em>∑<em>P</em>)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">S-537	西班牙 的 人权 困境</span><br><span class="line">T-537	Spain &amp;apos;s Human-Rights Dilemma</span><br><span class="line">H-537	-0.16863664984703064	Spain &amp;apos;s Human Rights Quandary</span><br><span class="line">D-537	-0.16863664984703064	Spain &amp;apos;s Human Rights Quandary</span><br><span class="line">P-537	-0.0973 -0.1385 -0.1464 -0.0123 -0.4252 -0.4299 -0.0110 -0.0884</span><br><span class="line"></span><br><span class="line">S-5516	这是 不可 接受 的 。</span><br><span class="line">T-5516	that is unacceptable .</span><br><span class="line">H-5516	-0.35840675234794617	this is unacceptable .</span><br><span class="line">D-5516	-0.35840675234794617	this is unacceptable .</span><br><span class="line">P-5516	-0.7625 -0.5517 -0.2005 -0.1513 -0.1261</span><br><span class="line"></span><br><span class="line">S-676	与 最初 版本 的 破产法 相 比较 ， 2006 年 的 法律 是 牢牢 扎根 于 市场经济 的 。</span><br><span class="line">T-676	compared with the original bankruptcy code , the 2006 code is firmly rooted in the needs of a market economy .</span><br><span class="line">H-676	-0.624997079372406	in contrast to the original bankruptcy law , the law of 2006 was firmly rooted in the market economy .</span><br><span class="line">D-676	-0.624997079372406	in contrast to the original bankruptcy law , the law of 2006 was firmly rooted in the market economy .</span><br><span class="line">P-676	-1.4995 -0.9434 -0.1292 -0.3479 -0.9758 -0.6600 -0.9037 -0.1836 -0.4983 -1.6406 -0.3142 -0.0344 -0.1685 -1.0289 -1.0286 -0.1917 -1.5369 -0.6586 -0.1119 -0.1333 -0.1361</span><br><span class="line"></span><br><span class="line">S-432	用 缅因州 共和党 参议员 苏珊 · 柯林斯 （ Susan Collins ） 的话 说 ， 政府 关门 对 其 缅因州 阿卡迪亚 国家 公园 （ Acadia National Park ） 周边 &quot; 所有 小企业 都 造成 了 伤害 &quot; ， &quot; 这是 完全 错误 的 。 &quot; 是 她 首先 提出 了 和解 协议 纲要 并 送交 参议院 。</span><br><span class="line">T-432	in the words of Senator Susan Collins , a Republican from Maine who first put together the outlines of a deal and took it to the Senate floor , the shutdown &quot; hurt all the small businesses &quot; around Acadia National Park in her home state , &quot; and that is plain wrong . &quot;</span><br><span class="line">H-432	-0.7003933787345886	in the words of Susan Collins , a Republican senator from Maine , it would be a mistake to shut down the government &amp;apos;s &quot; all small business &quot; around the Maine National Park , where she proposed a settlement and delivered it to the Senate .</span><br><span class="line">D-432	-0.7003933787345886	in the words of Susan Collins , a Republican senator from Maine , it would be a mistake to shut down the government &amp;apos;s &quot; all small business &quot; around the Maine National Park , where she proposed a settlement and delivered it to the Senate .</span><br><span class="line">P-432	-1.2762 -0.3546 -0.0142 -0.1261 -0.0058 -0.7617 -0.1695 -0.2992 -0.0777 -0.3016 -0.4818 -0.0061 -0.0308 -0.3509 -2.5533 -1.5254 -0.2761 -1.1667 -0.6169 -0.6285 -1.2463 -0.0973 -1.4414 -0.3324 -0.2302 -0.3312 -0.6847 -1.0005 -0.1812 -2.9048 -0.3072 -1.8045 -0.0473 -0.8421 -0.4715 -0.6841 -1.1902 -1.6192 -0.3370 -2.3317 -0.3701 -0.2508 -3.0284 -0.2336 -1.1318 -0.3904 -0.1124 -0.0262 -0.2203 -0.1480</span><br></pre></td></tr></table></figure>

<h3 id="3-3-2-交互式解码"><a href="#3-3-2-交互式解码" class="headerlink" title="3.3.2 交互式解码"></a>3.3.2 交互式解码</h3><p>使用<code>fairseq-interactive</code>命令进行交互式解码(<strong>用于文本文件</strong>)。注意其<code>input</code>参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!fairseq-interactive $&#123;data_dir&#125;&#x2F;data-bin \</span><br><span class="line">    --input $&#123;data_dir&#125;&#x2F;test.zh \</span><br><span class="line">    --path $&#123;model_dir&#125;&#x2F;checkpoints&#x2F;checkpoint_best.pt \</span><br><span class="line">    --batch-size 1 --beam 8 --remove-bpe &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt</span><br></pre></td></tr></table></figure>

<h2 id="3-4-后处理及评价"><a href="#3-4-后处理及评价" class="headerlink" title="3.4 后处理及评价"></a>3.4 后处理及评价</h2><h3 id="3-4-1-抽取译文"><a href="#3-4-1-抽取译文" class="headerlink" title="3.4.1 抽取译文"></a>3.4.1 抽取译文</h3><p>由于解码生成的文件包含大量无关信息，所以需要把<strong>译文</strong>和<strong>正确答案</strong>单独抽取出来，其中predict是译文，answer是正确答案：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep ^H $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt | cut -f3- &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.bpe.en</span><br><span class="line">grep ^T $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt | cut -f2- &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.bpe.en</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># predict.tok.true.bpe.en</span><br><span class="line">the subsidy .</span><br><span class="line">this is unacceptable .</span><br><span class="line">there is even worse .</span><br><span class="line">this must change .</span><br><span class="line"></span><br><span class="line"># answer.tok.true.bpe.en</span><br><span class="line">removal of subsidies .</span><br><span class="line">that is unacceptable .</span><br><span class="line">it gets worse .</span><br><span class="line">this must change .</span><br></pre></td></tr></table></figure>

<h3 id="3-4-1-去除bpe符号"><a href="#3-4-1-去除bpe符号" class="headerlink" title="3.4.1 去除bpe符号"></a>3.4.1 去除bpe符号</h3><p>有两种方法可以去除bpe符号，第一种是在解码时添加<code>--remove-bpe</code>参数，第二种是使用<code>sed</code>指令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -r &#39;s&#x2F;(@@ )| (@@ ?$)&#x2F;&#x2F;g&#39; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.bpe.en  &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.en</span><br><span class="line">sed -r &#39;s&#x2F;(@@ )| (@@ ?$)&#x2F;&#x2F;g&#39; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.bpe.en  &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.en</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># answer.tok.true.bpe.en</span><br><span class="line">a World of Under@@ investment</span><br><span class="line">that needs to change .</span><br><span class="line">revolts of the Righ@@ teous</span><br><span class="line">Russia &amp;apos;s Economic Imperi@@ alism</span><br><span class="line">shock and Pan@@ ic</span><br><span class="line"></span><br><span class="line"># answer.tok.true.en</span><br><span class="line">a World of Underinvestment</span><br><span class="line">that needs to change .</span><br><span class="line">revolts of the Righteous</span><br><span class="line">Russia &amp;apos;s Economic Imperialism</span><br><span class="line">shock and Panic</span><br></pre></td></tr></table></figure>

<h3 id="3-4-2-detruecase"><a href="#3-4-2-detruecase" class="headerlink" title="3.4.2 detruecase"></a>3.4.2 detruecase</h3><p>需要使用detruecase.perl将文件中的大小写恢复正常：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;DETC&#125; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.en &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.en</span><br><span class="line">$&#123;DETC&#125; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.en &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.en</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># predict.tok.true.en</span><br><span class="line">the subsidy .</span><br><span class="line">this is unacceptable .</span><br><span class="line">there is even worse .</span><br><span class="line">this must change .</span><br><span class="line"></span><br><span class="line"># predict.tok.en</span><br><span class="line">The subsidy .</span><br><span class="line">This is unacceptable .</span><br><span class="line">There is even worse .</span><br><span class="line">This must change .</span><br></pre></td></tr></table></figure>

<h3 id="3-4-3-评价"><a href="#3-4-3-评价" class="headerlink" title="3.4.3 评价"></a>3.4.3 评价</h3><p>1.<strong>multi-bleu</strong>：在detokenize前进行评价</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;MULTI_BLEU&#125; -lc $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.en &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.en</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BLEU &#x3D; 28.81, 61.8&#x2F;35.4&#x2F;22.8&#x2F;15.2 (BP&#x3D;0.976, ratio&#x3D;0.977, hyp_len&#x3D;187605, ref_len&#x3D;192093)</span><br><span class="line">It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.</span><br></pre></td></tr></table></figure>

<p>2.<strong>sacrebleu</strong>：在detokenize后进行评价。<a href="https://github.com/mjpost/sacreBLEU">link</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Option 1: Pass the reference file as a positional argument to sacreBLEU</span><br><span class="line">sacrebleu ref.detok.txt -i output.detok.txt -m bleu -b -w 4</span><br><span class="line">20.7965</span><br><span class="line"></span><br><span class="line"># Option 2: Redirect the system into STDIN (Compatible with multi-bleu.perl way of doing things)</span><br><span class="line">cat output.detok.txt | sacrebleu ref.detok.txt -m bleu -b -w 4</span><br><span class="line">20.7965</span><br></pre></td></tr></table></figure>

<p>3.<strong>mteval-v14</strong>：Usage: <code>$0 -r &lt;ref_file&gt; -s &lt;src_file&gt; -t &lt;tst_file&gt;</code></p>
<h3 id="3-4-4-detokenize"><a href="#3-4-4-detokenize" class="headerlink" title="3.4.4 detokenize"></a>3.4.4 detokenize</h3><p>最后一步，是使用detokenize.perl得到纯预测文本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;DETOKENIZER&#125; -l en &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.en &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.en</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># predict.tok.en</span><br><span class="line">what &amp;apos;s wrong with protectionism ?</span><br><span class="line">The &quot; establishment &quot; and counterinsurgency strategy , introduced by President Barack Obama &amp;apos;s military surge in 2010 , was intended to reverse the war .</span><br><span class="line"></span><br><span class="line"># predict.en</span><br><span class="line">What&#39;s wrong with protectionism?</span><br><span class="line">The &quot;establishment&quot; and counterinsurgency strategy, introduced by President Barack Obama&#39;s military surge in 2010, was intended to reverse the war.</span><br></pre></td></tr></table></figure>

<blockquote>
<p>参考：</p>
<p> <a href="https://hannlp.github.io/2021-01-16-Use-fairseq-to-train-a-Chinese-English-translation-model-from-scratch/">https://hannlp.github.io/2021-01-16-Use-fairseq-to-train-a-Chinese-English-translation-model-from-scratch/</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/194176917">https://zhuanlan.zhihu.com/p/194176917</a></p>
</blockquote>
]]></content>
      <categories>
        <category>research</category>
        <category>NMT</category>
      </categories>
  </entry>
  <entry>
    <title>chinese_text_classify</title>
    <url>/2021/11/28/chinese-text-classify/</url>
    <content><![CDATA[<blockquote>
<p>中文文本分类流程</p>
</blockquote>
<span id="more"></span>

<h1 id="1-相关工具及目录结构"><a href="#1-相关工具及目录结构" class="headerlink" title="1 相关工具及目录结构"></a>1 相关工具及目录结构</h1><h2 id="1-1-相关工具"><a href="#1-1-相关工具" class="headerlink" title="1.1 相关工具"></a>1.1 相关工具</h2><p>除<strong>jieba</strong>是使用<code>pip install</code>安装外，其他几个工具都是建议直接克隆库到自己的用户目录中，方便使用其脚本(<strong>moses</strong>/<strong>subword-nmt</strong>)，或未来可能要自己拓展其中的模型(<strong>fairseq</strong>)</p>
<ol>
<li><p>jieba (中文分词组件)，安装指令如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="1-2-目录结构与初始化"><a href="#1-2-目录结构与初始化" class="headerlink" title="1.2 目录结构与初始化"></a>1.2 目录结构与初始化</h2><h3 id="1-2-1-目录结构"><a href="#1-2-1-目录结构" class="headerlink" title="1.2.1 目录结构"></a>1.2.1 目录结构</h3><p>提前组织一个目录结构的好处是可以让后面的一系列操作更加统一、规范化。下表中<code>~</code>代表linux系统中<strong>我的用户目录</strong>, THUCNews目录名代表此次我使用的数据集名称</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~</span><br><span class="line">├── mosesdecoder</span><br><span class="line">└── text-classify</span><br><span class="line">    ├── master</span><br><span class="line">    	├── THUCNews</span><br><span class="line">    		├── split_data.py   # 用于划分train,valid,test</span><br><span class="line">        	├── data            # 用于存放数据</span><br><span class="line">            	├── train.txt          </span><br><span class="line">            	├── train_long.txt</span><br><span class="line">            	├── dev.txt  </span><br><span class="line">            	├── dev_long.txt </span><br><span class="line">            	├── test.txt </span><br><span class="line">            	├── test_long.txt </span><br><span class="line">            	├── class.txt  </span><br><span class="line">            	├── embedding_SougouNews.npz  </span><br><span class="line">            	├── embedding_Tencent.npz  </span><br><span class="line">            	└── vocab.pkl</span><br><span class="line">            └── saved_dict      # 用于存放翻译结果</span><br><span class="line">    ├── models                  </span><br><span class="line">        ├── Bert.py  </span><br><span class="line">        ├── TextCNN.py  </span><br><span class="line">        ├── TextRNN_Att.py  </span><br><span class="line">        └── Transformer.py</span><br><span class="line">    ├── pytorch_pretrained_bert</span><br><span class="line">    ├── scripts                 # 一些脚本</span><br><span class="line">    	├── train_bert.sh  </span><br><span class="line">    	├── train_bilstm_att.sh  </span><br><span class="line">    	├── train_cnn.sh  </span><br><span class="line">    	└── train_transformer.sh</span><br><span class="line">    ├── utils.py                # 一些其他工具</span><br><span class="line">    ├── utils_bert.py           </span><br><span class="line">    ├── run.py					# 模型运行</span><br><span class="line">    ├── run_bert.py				</span><br><span class="line">    ├── train_eval.py			</span><br><span class="line">    └── train_eval_bert.py</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="2-数据的准备"><a href="#2-数据的准备" class="headerlink" title="2 数据的准备"></a>2 数据的准备</h1><h2 id="2-1-THUCNews"><a href="#2-1-THUCNews" class="headerlink" title="2.1 THUCNews"></a>2.1 THUCNews</h2><p>本文采用 <a href="http://thuctc.thunlp.org/">THUCNews中文文本开源语料</a> ，THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。共分为14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。</p>
<p><strong>原始数据统计：</strong></p>
<p>总数量：836075条（科技：162929；股票：154398；体育：131604；娱乐：92632；时政：63086；社会：50849；教育：41936；财经：37098；家居：32586；游戏：24373；房产：20050；时尚：13368；彩票：7588；星座：3578）</p>
<h2 id="2-2-数据预处理"><a href="#2-2-数据预处理" class="headerlink" title="2.2 数据预处理"></a>2.2 数据预处理</h2><h3 id="2-2-1-原始数据格式"><a href="#2-2-1-原始数据格式" class="headerlink" title="2.2.1 原始数据格式"></a>2.2.1 原始数据格式</h3><p>分类放置，每个文档包含一篇新闻文本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">└── THUCNews</span><br><span class="line">        └── 体育     </span><br><span class="line">            └── 29016.txt</span><br><span class="line">        └── 财经     </span><br><span class="line">            └── 11016.txt</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>文档格式如下：由标题和正文两部分组成</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">组图：9月全球折扣明星带你去血拼</span><br><span class="line">　　导读：每年的8，9两月，都是普天下血拼狂人大开杀戒的日子。因为全球性的折扣季节已正式启动。以折扣率最疯狂的美国来说，无论一线大牌还是时尚潮物，统统为5 折为起点，一路狂飙到2折左右。去年8月，Sami就曾以0.5折的夸张扣率抢到一条MiuMiu连衣裙。此番Sami特别请到20位美国本土女星，她们让现身说法，带你掏遍折扣季节最值得入货的高性价比超值单品。</span><br><span class="line">　　Heidi Montag推荐超值折扣单品：GUCCI凉鞋</span><br><span class="line">　　Ashley Tisdale推荐超值折扣单品：破洞牛仔小脚裤</span><br></pre></td></tr></table></figure>

<h3 id="2-2-2-数据集生成"><a href="#2-2-2-数据集生成" class="headerlink" title="2.2.2 数据集生成"></a>2.2.2 数据集生成</h3><p>从中抽取10个类别（财经、房产、股票、教育、科技、社会、时政、体育、游戏、娱乐）随机抽取共20万条数据，其中训练集18万数据，验证集1万数据，测试集1万数据。</p>
<p>首先，从原始数据集中选择10个类别抽取部分数据，每个类别随机抽取18000条数据作为训练集，1000条数据作为验证集，1000条数据作为测试集，使用split_data.py划分：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python split_data.py</span><br></pre></td></tr></table></figure>

<p>使用shuffle.py打乱顺序：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python shuffle.py</span><br></pre></td></tr></table></figure>

<p>短文本（标题）效果如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">中国人寿承诺不裁员不减薪        0</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>长文本（标题+正文）效果如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">四新股冻结资金4112亿元 王亚伟青睐九安医疗\　　⊙见习记者方俊○编辑朱绍勇　　据今日公告披露，本周一发行的中小板新股兆驰股份、杭氧股份、九安医疗、棕榈园林合计冻结资金4112.5亿元，而整体来看打新资金对个股的选择出现了严重分化，其中兆驰股份和九安医疗的中签率分别为2.445%和0.487%，相差近5倍。　　公告显示，兆驰股份网上定价发行中签率为2.445%，仅次于此前科伦药业3.1%中签率，超额认购倍数为41倍，冻结资金549.68亿元，网下有效申购资金为53.37亿元。杭氧股份网上中签率为1.1%，超额认购倍数为90倍，冻结资金924.58亿元，网下有效申购资金为80.65亿元。棕榈园林网上中签率为0.88%，超额认购倍数为113倍，冻结资金1219.26亿元，网下有效申购资金为196.47亿元。　　九安医疗网上中签率为0.487%，超额认购倍数为205倍，冻结资金986.79亿元，网下有效申购资金为101.7亿元。而网下配售结果显示，九安医疗也受到了127家配售对象的青睐，其中包括王亚伟执掌的华夏大盘和华夏策略。	0</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>最后，THUCNews/data目录中有如下数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── THUCNews</span><br><span class="line">    └── data</span><br><span class="line">        ├── train.txt</span><br><span class="line">        ├── train_long.txt</span><br><span class="line">        ├── dev.txt</span><br><span class="line">        ├── dev_long.txt</span><br><span class="line">        ├── test.txt</span><br><span class="line">        └── test_long.txt</span><br></pre></td></tr></table></figure>

<h1 id="3-训练过程"><a href="#3-训练过程" class="headerlink" title="3 训练过程"></a>3 训练过程</h1><h2 id="3-1-bert模型"><a href="#3-1-bert模型" class="headerlink" title="3.1 bert模型"></a>3.1 bert模型</h2><p>1.运行脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run_bert.py --model bert &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>2.运行log</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">180000it [00:35, 5049.92it&#x2F;s]</span><br><span class="line">10000it [00:02, 4529.50it&#x2F;s]</span><br><span class="line">10000it [00:02, 3977.26it&#x2F;s]</span><br><span class="line">Time usage: 0:00:40</span><br><span class="line">model to cpu</span><br><span class="line">model to train</span><br><span class="line">no error -1</span><br><span class="line">no error 0</span><br><span class="line">no error 1</span><br><span class="line">no error 2</span><br><span class="line">Epoch [1&#x2F;3]</span><br><span class="line">&#x2F;data&#x2F;yyfxu&#x2F;text-classify&#x2F;tmp&#x2F;pytorch_pretrained_bert&#x2F;optimization.py:275: UserWarning: This overload of add_ is deprecated:</span><br><span class="line">	add_(Number alpha, Tensor other)</span><br><span class="line">Consider using one of the following signatures instead:</span><br><span class="line">	add_(Tensor other, *, Number alpha) (Triggered internally at  &#x2F;opt&#x2F;conda&#x2F;conda-bld&#x2F;pytorch_1595629403081&#x2F;work&#x2F;torch&#x2F;csrc&#x2F;utils&#x2F;python_arg_parser.cpp:766.)</span><br><span class="line">  next_m.mul_(beta1).add_(1 - beta1, grad)</span><br><span class="line">Iter:      0,  Train Loss:   2.5,  Train Acc:  6.25%,  Val Loss:   2.3,  Val Acc: 10.13%,  Time: 0:00:24 *</span><br><span class="line">Iter:    100,  Train Loss:  0.88,  Train Acc: 75.00%,  Val Loss:  0.73,  Val Acc: 81.03%,  Time: 0:01:24 *</span><br><span class="line">Iter:    200,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:  0.41,  Val Acc: 87.58%,  Time: 0:02:18 *</span><br><span class="line">Iter:    300,  Train Loss:   0.7,  Train Acc: 81.25%,  Val Loss:   0.4,  Val Acc: 88.25%,  Time: 0:03:13 *</span><br><span class="line">Iter:    400,  Train Loss:   1.0,  Train Acc: 81.25%,  Val Loss:  0.38,  Val Acc: 88.62%,  Time: 0:04:07 *</span><br><span class="line">Iter:    500,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.39,  Val Acc: 88.75%,  Time: 0:05:02 </span><br><span class="line">Iter:    600,  Train Loss:  0.31,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 89.58%,  Time: 0:05:57 *</span><br><span class="line">Iter:    700,  Train Loss:  0.29,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.25%,  Time: 0:06:52 </span><br><span class="line">Iter:    800,  Train Loss:  0.35,  Train Acc: 93.75%,  Val Loss:  0.41,  Val Acc: 88.22%,  Time: 0:07:46 </span><br><span class="line">Iter:    900,  Train Loss: 0.036,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 89.76%,  Time: 0:08:42 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.57,  Train Acc: 87.50%,  Val Loss:  0.42,  Val Acc: 89.01%,  Time: 0:09:38 </span><br><span class="line">Iter:   1100,  Train Loss: 0.017,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 88.69%,  Time: 0:10:32 </span><br><span class="line">Iter:   1200,  Train Loss:  0.97,  Train Acc: 81.25%,  Val Loss:   0.4,  Val Acc: 88.91%,  Time: 0:11:27 </span><br><span class="line">Iter:   1300,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.38,  Val Acc: 88.72%,  Time: 0:12:19 </span><br><span class="line">Iter:   1400,  Train Loss:  0.69,  Train Acc: 81.25%,  Val Loss:  0.44,  Val Acc: 87.96%,  Time: 0:13:11 </span><br><span class="line">Iter:   1500,  Train Loss:  0.52,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 88.91%,  Time: 0:14:05 </span><br><span class="line">Iter:   1600,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:  0.52,  Val Acc: 86.66%,  Time: 0:14:57 </span><br><span class="line">Iter:   1700,  Train Loss:  0.12,  Train Acc: 93.75%,  Val Loss:  0.41,  Val Acc: 89.22%,  Time: 0:15:52 </span><br><span class="line">Iter:   1800,  Train Loss: 0.054,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 88.87%,  Time: 0:16:44 </span><br><span class="line">Iter:   1900,  Train Loss:  0.12,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 88.96%,  Time: 0:17:38 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.34,  Test Acc: 90.05%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.8957    0.8670    0.8811      1000</span><br><span class="line">       realty     0.9262    0.9290    0.9276      1000</span><br><span class="line">       stocks     0.8510    0.8170    0.8337      1000</span><br><span class="line">    education     0.9732    0.9430    0.9578      1000</span><br><span class="line">      science     0.7790    0.9130    0.8407      1000</span><br><span class="line">      society     0.9564    0.8550    0.9029      1000</span><br><span class="line">     politics     0.8578    0.9290    0.8920      1000</span><br><span class="line">       sports     0.9092    0.9710    0.9391      1000</span><br><span class="line">         game     0.9383    0.9130    0.9255      1000</span><br><span class="line">entertainment     0.9538    0.8680    0.9089      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9005     10000</span><br><span class="line">    macro avg     0.9041    0.9005    0.9009     10000</span><br><span class="line"> weighted avg     0.9041    0.9005    0.9009     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[867  13  84   2  19   1   9   5   0   0]</span><br><span class="line"> [ 17 929   6   0  16   4  12   7   5   4]</span><br><span class="line"> [ 40  29 817   0  74   0  33   4   3   0]</span><br><span class="line"> [  5   2   3 943   8   9  15   6   2   7]</span><br><span class="line"> [  3   3  17   2 913  10  18   4  24   6]</span><br><span class="line"> [ 18  16   0  12  41 855  41   6   2   9]</span><br><span class="line"> [  8   3  25   6  17   6 929   3   0   3]</span><br><span class="line"> [  3   2   3   2   8   0   4 971   2   5]</span><br><span class="line"> [  2   0   4   0  52   6   7   8 913   8]</span><br><span class="line"> [  5   6   1   2  24   3  15  54  22 868]]</span><br><span class="line">Time usage: 0:00:21</span><br><span class="line">model finish</span><br></pre></td></tr></table></figure>

<p>使用了预训练的bert模型，长文本超出长度限制（512），bert没有进行长文本实验。</p>
<h2 id="3-2-bilstm-attention模型"><a href="#3-2-bilstm-attention模型" class="headerlink" title="3.2 bilstm+attention模型"></a>3.2 bilstm+attention模型</h2><p>1.运行脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run.py --model TextRNN_Att &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>2.运行log</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:02, 70404.96it/s]</span><br><span class="line">10000it [00:00, 90716.68it/s]</span><br><span class="line">10000it [00:00, 92472.73it/s]</span><br><span class="line">Time usage: 0:00:03</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)</span><br><span class="line">  (tanh1): Tanh()</span><br><span class="line">  (tanh2): Tanh()</span><br><span class="line">  (fc1): Linear(in_features=256, out_features=64, bias=True)</span><br><span class="line">  (fc): Linear(in_features=64, out_features=10, bias=True)</span><br><span class="line"><span class="meta">)&gt;</span></span><br><span class="line"><span class="bash">start train</span></span><br><span class="line">Epoch [1/10]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc:  6.25%,  Val Loss:   2.3,  Val Acc: 10.01%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:  0.75,  Train Acc: 75.00%,  Val Loss:  0.78,  Val Acc: 73.34%,  Time: 0:00:05 *</span><br><span class="line">Iter:    200,  Train Loss:  0.76,  Train Acc: 72.66%,  Val Loss:  0.58,  Val Acc: 81.38%,  Time: 0:00:07 *</span><br><span class="line">Iter:    300,  Train Loss:   0.4,  Train Acc: 88.28%,  Val Loss:  0.53,  Val Acc: 83.07%,  Time: 0:00:10 *</span><br><span class="line">Iter:    400,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.47,  Val Acc: 84.44%,  Time: 0:00:14 *</span><br><span class="line">Iter:    500,  Train Loss:  0.44,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 85.70%,  Time: 0:00:18 *</span><br><span class="line">Iter:    600,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 86.46%,  Time: 0:00:22 *</span><br><span class="line">Iter:    700,  Train Loss:  0.41,  Train Acc: 85.94%,  Val Loss:  0.41,  Val Acc: 86.79%,  Time: 0:00:25 *</span><br><span class="line">Iter:    800,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 87.86%,  Time: 0:00:29 *</span><br><span class="line">Iter:    900,  Train Loss:  0.44,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 87.53%,  Time: 0:00:33 </span><br><span class="line">Iter:   1000,  Train Loss:  0.26,  Train Acc: 90.62%,  Val Loss:  0.37,  Val Acc: 88.04%,  Time: 0:00:36 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.29,  Train Acc: 92.19%,  Val Loss:  0.39,  Val Acc: 86.97%,  Time: 0:00:40 </span><br><span class="line">Iter:   1200,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.41%,  Time: 0:00:43 *</span><br><span class="line">Iter:   1300,  Train Loss:  0.34,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.54%,  Time: 0:00:47 </span><br><span class="line">Iter:   1400,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.44%,  Time: 0:00:50 </span><br><span class="line">Epoch [2/10]</span><br><span class="line">Iter:   1500,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 88.15%,  Time: 0:00:54 </span><br><span class="line">Iter:   1600,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 88.53%,  Time: 0:00:57 </span><br><span class="line">Iter:   1700,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 88.63%,  Time: 0:01:00 *</span><br><span class="line">Iter:   1800,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.31%,  Time: 0:01:03 *</span><br><span class="line">Iter:   1900,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.55%,  Time: 0:01:07 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 89.12%,  Time: 0:01:10 </span><br><span class="line">Iter:   2100,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 89.42%,  Time: 0:01:14 </span><br><span class="line">Iter:   2200,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 89.52%,  Time: 0:01:17 </span><br><span class="line">Iter:   2300,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.31%,  Time: 0:01:21 *</span><br><span class="line">Iter:   2400,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 88.97%,  Time: 0:01:24 </span><br><span class="line">Iter:   2500,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.31,  Val Acc: 89.67%,  Time: 0:01:28 *</span><br><span class="line">Iter:   2600,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.74%,  Time: 0:01:31 </span><br><span class="line">Iter:   2700,  Train Loss:  0.25,  Train Acc: 93.75%,  Val Loss:  0.31,  Val Acc: 89.83%,  Time: 0:01:35 *</span><br><span class="line">Iter:   2800,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 89.59%,  Time: 0:01:39 </span><br><span class="line">Epoch [3/10]</span><br><span class="line">Iter:   2900,  Train Loss:  0.32,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.21%,  Time: 0:01:42 </span><br><span class="line">Iter:   3000,  Train Loss:  0.21,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 89.80%,  Time: 0:01:45 </span><br><span class="line">Iter:   3100,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.34,  Val Acc: 89.29%,  Time: 0:01:50 </span><br><span class="line">Iter:   3200,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.51%,  Time: 0:01:53 </span><br><span class="line">Iter:   3300,  Train Loss:  0.31,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.85%,  Time: 0:01:57 </span><br><span class="line">Iter:   3400,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.31,  Val Acc: 89.98%,  Time: 0:02:01 </span><br><span class="line">Iter:   3500,  Train Loss:  0.12,  Train Acc: 95.31%,  Val Loss:  0.34,  Val Acc: 89.42%,  Time: 0:02:04 </span><br><span class="line">Iter:   3600,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 90.14%,  Time: 0:02:08 </span><br><span class="line">Iter:   3700,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.31,  Val Acc: 89.92%,  Time: 0:02:11 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:   0.3,  Test Acc: 89.75%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9120    0.8600    0.8852      1000</span><br><span class="line">       realty     0.9037    0.9200    0.9118      1000</span><br><span class="line">       stocks     0.8652    0.7960    0.8292      1000</span><br><span class="line">    education     0.9660    0.9080    0.9361      1000</span><br><span class="line">      science     0.7947    0.8750    0.8329      1000</span><br><span class="line">      society     0.8610    0.9290    0.8937      1000</span><br><span class="line">     politics     0.8918    0.8740    0.8828      1000</span><br><span class="line">       sports     0.9777    0.9660    0.9718      1000</span><br><span class="line">         game     0.9430    0.8930    0.9173      1000</span><br><span class="line">entertainment     0.8801    0.9540    0.9155      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.8975     10000</span><br><span class="line">    macro avg     0.8995    0.8975    0.8976     10000</span><br><span class="line"> weighted avg     0.8995    0.8975    0.8976     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[860  24  59   4  20  13  11   3   1   5]</span><br><span class="line"> [  8 920  16   0  11  15   7   4   4  15]</span><br><span class="line"> [ 53  29 796   1  77   3  32   1   6   2]</span><br><span class="line"> [  2   4   2 908  10  43  13   1   3  14]</span><br><span class="line"> [ 11  10  12   5 875  15  20   3  28  21]</span><br><span class="line"> [  0  15   1  10   8 929  16   1   3  17]</span><br><span class="line"> [  5   6  26   8  19  40 874   1   2  19]</span><br><span class="line"> [  1   1   2   0   1   5   3 966   0  21]</span><br><span class="line"> [  1   4   5   1  69   6   3   2 893  16]</span><br><span class="line"> [  2   5   1   3  11  10   1   6   7 954]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>长文本（标题+正文）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14699.32it&#x2F;s]</span><br><span class="line">10000it [00:00, 12818.24it&#x2F;s]</span><br><span class="line">10000it [00:00, 16106.52it&#x2F;s]</span><br><span class="line">Time usage: 0:00:14</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">(base) yyfxu@cip57:~&#x2F;text-classify&#x2F;master$ vim nohup_bi_long.log </span><br><span class="line">(base) yyfxu@cip57:~&#x2F;text-classify&#x2F;master$ head -n 20 nohup_bi_long.log </span><br><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14699.32it&#x2F;s]</span><br><span class="line">10000it [00:00, 12818.24it&#x2F;s]</span><br><span class="line">10000it [00:00, 16106.52it&#x2F;s]</span><br><span class="line">Time usage: 0:00:14</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (lstm): LSTM(300, 128, num_layers&#x3D;2, batch_first&#x3D;True, dropout&#x3D;0.5, bidirectional&#x3D;True)</span><br><span class="line">  (tanh1): Tanh()</span><br><span class="line">  (tanh2): Tanh()</span><br><span class="line">  (fc1): Linear(in_features&#x3D;256, out_features&#x3D;64, bias&#x3D;True)</span><br><span class="line">  (fc): Linear(in_features&#x3D;64, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;10]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc: 18.75%,  Val Loss:   2.3,  Val Acc: 10.03%,  Time: 0:00:00 *</span><br><span class="line">Iter:    100,  Train Loss:  0.55,  Train Acc: 85.16%,  Val Loss:  0.55,  Val Acc: 82.30%,  Time: 0:00:02 *</span><br><span class="line">Iter:    200,  Train Loss:  0.41,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 85.91%,  Time: 0:00:04 *</span><br><span class="line">Iter:    300,  Train Loss:  0.57,  Train Acc: 82.03%,  Val Loss:  0.41,  Val Acc: 86.64%,  Time: 0:00:06 *</span><br><span class="line">Iter:    400,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.37,  Val Acc: 87.85%,  Time: 0:00:07 *</span><br><span class="line">Iter:    500,  Train Loss:   0.3,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.41%,  Time: 0:00:09 *</span><br><span class="line">Iter:    600,  Train Loss:  0.34,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.66%,  Time: 0:00:11 *</span><br><span class="line">Iter:    700,  Train Loss:  0.21,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 89.61%,  Time: 0:00:12</span><br><span class="line">Iter:    800,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:   0.3,  Val Acc: 90.31%,  Time: 0:00:14 *</span><br><span class="line">Iter:    900,  Train Loss:  0.55,  Train Acc: 85.94%,  Val Loss:  0.28,  Val Acc: 91.13%,  Time: 0:00:15 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.29,  Train Acc: 88.28%,  Val Loss:  0.28,  Val Acc: 91.06%,  Time: 0:00:17 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.70%,  Time: 0:00:19 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.31,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.70%,  Time: 0:00:21 *</span><br><span class="line">Iter:   1300,  Train Loss:  0.24,  Train Acc: 93.75%,  Val Loss:  0.28,  Val Acc: 91.10%,  Time: 0:00:22</span><br><span class="line">Iter:   1400,  Train Loss:  0.27,  Train Acc: 89.84%,  Val Loss:  0.25,  Val Acc: 91.79%,  Time: 0:00:24 *</span><br><span class="line">Epoch [2&#x2F;10]</span><br><span class="line">Iter:   1500,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.28,  Val Acc: 90.45%,  Time: 0:00:26</span><br><span class="line">Iter:   1600,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.70%,  Time: 0:00:27</span><br><span class="line">Iter:   1700,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.27,  Val Acc: 91.42%,  Time: 0:00:29</span><br><span class="line">Iter:   1800,  Train Loss:  0.14,  Train Acc: 95.31%,  Val Loss:  0.26,  Val Acc: 91.92%,  Time: 0:00:31</span><br><span class="line">Iter:   1900,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.33%,  Time: 0:00:32 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.24,  Train Acc: 89.06%,  Val Loss:  0.24,  Val Acc: 92.20%,  Time: 0:00:34</span><br><span class="line">Iter:   2100,  Train Loss:  0.27,  Train Acc: 92.19%,  Val Loss:  0.25,  Val Acc: 91.82%,  Time: 0:00:36</span><br><span class="line">Iter:   2200,  Train Loss:   0.2,  Train Acc: 92.19%,  Val Loss:  0.25,  Val Acc: 92.04%,  Time: 0:00:38</span><br><span class="line">Iter:   2300,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.06%,  Time: 0:00:42</span><br><span class="line">Iter:   2400,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.24,  Val Acc: 92.40%,  Time: 0:00:45 *</span><br><span class="line">Iter:   2500,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.22,  Val Acc: 92.59%,  Time: 0:00:48 *</span><br><span class="line">Iter:   2600,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.22,  Val Acc: 92.81%,  Time: 0:00:51 *</span><br><span class="line">Iter:   2700,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.24,  Val Acc: 92.30%,  Time: 0:00:54</span><br><span class="line">Iter:   2800,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 92.73%,  Time: 0:00:57</span><br><span class="line">Epoch [3&#x2F;10]</span><br><span class="line">Iter:   2900,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.26,  Val Acc: 91.74%,  Time: 0:01:01</span><br><span class="line">Iter:   3000,  Train Loss:  0.22,  Train Acc: 94.53%,  Val Loss:  0.23,  Val Acc: 92.44%,  Time: 0:01:04</span><br><span class="line">Iter:   3100,  Train Loss:  0.18,  Train Acc: 94.53%,  Val Loss:  0.23,  Val Acc: 92.74%,  Time: 0:01:07</span><br><span class="line">Iter:   3200,  Train Loss:   0.1,  Train Acc: 96.88%,  Val Loss:  0.22,  Val Acc: 92.83%,  Time: 0:01:11</span><br><span class="line">Iter:   3300,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 92.88%,  Time: 0:01:14</span><br><span class="line">Iter:   3400,  Train Loss:  0.19,  Train Acc: 95.31%,  Val Loss:  0.24,  Val Acc: 92.12%,  Time: 0:01:17</span><br><span class="line">Iter:   3500,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.23,  Val Acc: 92.45%,  Time: 0:01:21</span><br><span class="line">Iter:   3600,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.20%,  Time: 0:01:24</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.22,  Test Acc: 92.82%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9044    0.9080    0.9062      1000</span><br><span class="line">       realty     0.9308    0.9410    0.9359      1000</span><br><span class="line">       stocks     0.8939    0.8850    0.8894      1000</span><br><span class="line">    education     0.9541    0.9560    0.9550      1000</span><br><span class="line">      science     0.9121    0.8610    0.8858      1000</span><br><span class="line">      society     0.8962    0.9240    0.9099      1000</span><br><span class="line">     politics     0.9086    0.9340    0.9211      1000</span><br><span class="line">       sports     0.9869    0.9830    0.9850      1000</span><br><span class="line">         game     0.9336    0.9560    0.9447      1000</span><br><span class="line">entertainment     0.9629    0.9340    0.9482      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9282     10000</span><br><span class="line">    macro avg     0.9283    0.9282    0.9281     10000</span><br><span class="line"> weighted avg     0.9283    0.9282    0.9281     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[908  17  49   0   2  13   7   1   1   2]</span><br><span class="line"> [  7 941  14   4   8  14   6   0   3   3]</span><br><span class="line"> [ 51  17 885   3  18   3  17   0   5   1]</span><br><span class="line"> [  4   4   2 956   2  17   8   0   1   6]</span><br><span class="line"> [ 13   8  23   5 861  23  22   1  40   4]</span><br><span class="line"> [  5   9   3  15   6 924  27   1   2   8]</span><br><span class="line"> [  6   7   6  14   7  18 934   0   2   6]</span><br><span class="line"> [  3   1   1   0   1   3   2 983   1   5]</span><br><span class="line"> [  3   2   4   0  30   2   2   0 956   1]</span><br><span class="line"> [  4   5   3   5   9  14   3  10  13 934]]</span><br><span class="line">Time usage: 0:00:01</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure>



<h2 id="3-3-transformer模型（encoder）"><a href="#3-3-transformer模型（encoder）" class="headerlink" title="3.3 transformer模型（encoder）"></a>3.3 transformer模型（encoder）</h2><p>1.运行脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run.py --model Transformer &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>2.运行log</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:02, 71756.33it&#x2F;s]</span><br><span class="line">10000it [00:00, 91902.35it&#x2F;s]</span><br><span class="line">10000it [00:00, 89357.25it&#x2F;s]</span><br><span class="line">Time usage: 0:00:03</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (postion_embedding): Positional_Encoding(</span><br><span class="line">    (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  )</span><br><span class="line">  (encoder): Encoder(</span><br><span class="line">    (attention): Multi_Head_Attention(</span><br><span class="line">      (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">      (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">    (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">      (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">      (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (encoders): ModuleList(</span><br><span class="line">    (0): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (fc1): Linear(in_features&#x3D;9600, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.4,  Train Acc: 10.16%,  Val Loss:   4.6,  Val Acc: 10.02%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:   1.4,  Train Acc: 53.12%,  Val Loss:   1.4,  Val Acc: 57.23%,  Time: 0:00:03 *</span><br><span class="line">Iter:    200,  Train Loss:   1.3,  Train Acc: 55.47%,  Val Loss:   1.0,  Val Acc: 68.83%,  Time: 0:00:05 *</span><br><span class="line">Iter:    300,  Train Loss:  0.82,  Train Acc: 67.97%,  Val Loss:   0.9,  Val Acc: 74.19%,  Time: 0:00:07 *</span><br><span class="line">Iter:    400,  Train Loss:  0.84,  Train Acc: 75.00%,  Val Loss:  0.83,  Val Acc: 76.70%,  Time: 0:00:09 *</span><br><span class="line">Iter:    500,  Train Loss:  0.65,  Train Acc: 78.12%,  Val Loss:  0.76,  Val Acc: 78.63%,  Time: 0:00:12 *</span><br><span class="line">Iter:    600,  Train Loss:  0.76,  Train Acc: 75.78%,  Val Loss:  0.76,  Val Acc: 79.15%,  Time: 0:00:15 *</span><br><span class="line">Iter:    700,  Train Loss:  0.63,  Train Acc: 78.12%,  Val Loss:  0.78,  Val Acc: 79.21%,  Time: 0:00:17 </span><br><span class="line">Iter:    800,  Train Loss:  0.59,  Train Acc: 81.25%,  Val Loss:  0.63,  Val Acc: 82.16%,  Time: 0:00:19 *</span><br><span class="line">Iter:    900,  Train Loss:   0.7,  Train Acc: 76.56%,  Val Loss:  0.69,  Val Acc: 80.84%,  Time: 0:00:21 </span><br><span class="line">Iter:   1000,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.69,  Val Acc: 80.87%,  Time: 0:00:23 </span><br><span class="line">Iter:   1100,  Train Loss:  0.52,  Train Acc: 83.59%,  Val Loss:  0.62,  Val Acc: 82.61%,  Time: 0:00:26 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.63,  Train Acc: 82.81%,  Val Loss:  0.63,  Val Acc: 82.48%,  Time: 0:00:28 </span><br><span class="line">Iter:   1300,  Train Loss:  0.59,  Train Acc: 80.47%,  Val Loss:  0.55,  Val Acc: 85.06%,  Time: 0:00:32 *</span><br><span class="line">Iter:   1400,  Train Loss:  0.76,  Train Acc: 74.22%,  Val Loss:  0.56,  Val Acc: 84.04%,  Time: 0:00:35 </span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:  0.63,  Train Acc: 79.69%,  Val Loss:  0.57,  Val Acc: 84.12%,  Time: 0:00:38 </span><br><span class="line">Iter:   1600,  Train Loss:  0.52,  Train Acc: 78.91%,  Val Loss:  0.65,  Val Acc: 82.71%,  Time: 0:00:41 </span><br><span class="line">Iter:   1700,  Train Loss:  0.59,  Train Acc: 82.81%,  Val Loss:  0.58,  Val Acc: 83.98%,  Time: 0:00:44 </span><br><span class="line">Iter:   1800,  Train Loss:  0.46,  Train Acc: 87.50%,  Val Loss:  0.54,  Val Acc: 85.62%,  Time: 0:00:47 *</span><br><span class="line">Iter:   1900,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.53,  Val Acc: 85.76%,  Time: 0:00:50 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.58,  Train Acc: 82.81%,  Val Loss:  0.58,  Val Acc: 84.32%,  Time: 0:00:53 </span><br><span class="line">Iter:   2100,  Train Loss:  0.56,  Train Acc: 86.72%,  Val Loss:  0.55,  Val Acc: 84.90%,  Time: 0:00:56 </span><br><span class="line">Iter:   2200,  Train Loss:  0.36,  Train Acc: 89.84%,  Val Loss:  0.52,  Val Acc: 85.77%,  Time: 0:01:00 *</span><br><span class="line">Iter:   2300,  Train Loss:  0.43,  Train Acc: 86.72%,  Val Loss:  0.55,  Val Acc: 84.77%,  Time: 0:01:03 </span><br><span class="line">Iter:   2400,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.56,  Val Acc: 85.17%,  Time: 0:01:06 </span><br><span class="line">Iter:   2500,  Train Loss:   0.4,  Train Acc: 85.94%,  Val Loss:  0.56,  Val Acc: 84.99%,  Time: 0:01:10 </span><br><span class="line">Iter:   2600,  Train Loss:  0.57,  Train Acc: 79.69%,  Val Loss:  0.55,  Val Acc: 84.93%,  Time: 0:01:13 </span><br><span class="line">Iter:   2700,  Train Loss:  0.44,  Train Acc: 87.50%,  Val Loss:  0.48,  Val Acc: 86.88%,  Time: 0:01:16 *</span><br><span class="line">Iter:   2800,  Train Loss:  0.57,  Train Acc: 82.81%,  Val Loss:   0.5,  Val Acc: 86.41%,  Time: 0:01:20 </span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.59,  Train Acc: 79.69%,  Val Loss:  0.59,  Val Acc: 84.07%,  Time: 0:01:23 </span><br><span class="line">Iter:   3000,  Train Loss:  0.47,  Train Acc: 86.72%,  Val Loss:  0.52,  Val Acc: 86.12%,  Time: 0:01:27 </span><br><span class="line">Iter:   3100,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.48,  Val Acc: 86.31%,  Time: 0:01:29 </span><br><span class="line">Iter:   3200,  Train Loss:  0.71,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 86.09%,  Time: 0:01:33 </span><br><span class="line">Iter:   3300,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.48,  Val Acc: 87.24%,  Time: 0:01:36 *</span><br><span class="line">Iter:   3400,  Train Loss:  0.51,  Train Acc: 82.03%,  Val Loss:  0.47,  Val Acc: 87.28%,  Time: 0:01:39 *</span><br><span class="line">Iter:   3500,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.54,  Val Acc: 86.01%,  Time: 0:01:42 </span><br><span class="line">Iter:   3600,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.49,  Val Acc: 87.23%,  Time: 0:01:46 </span><br><span class="line">Iter:   3700,  Train Loss:  0.59,  Train Acc: 80.47%,  Val Loss:  0.48,  Val Acc: 86.53%,  Time: 0:01:49 </span><br><span class="line">Iter:   3800,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.52,  Val Acc: 86.27%,  Time: 0:01:52 </span><br><span class="line">Iter:   3900,  Train Loss:  0.51,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 86.02%,  Time: 0:01:56 </span><br><span class="line">Iter:   4000,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.48,  Val Acc: 87.14%,  Time: 0:02:01 </span><br><span class="line">Iter:   4100,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.48,  Val Acc: 87.02%,  Time: 0:02:04 </span><br><span class="line">Iter:   4200,  Train Loss:  0.52,  Train Acc: 84.38%,  Val Loss:  0.49,  Val Acc: 86.53%,  Time: 0:02:08 </span><br><span class="line">Epoch [4&#x2F;20]</span><br><span class="line">Iter:   4300,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 87.06%,  Time: 0:02:11 </span><br><span class="line">Iter:   4400,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.47,  Val Acc: 87.44%,  Time: 0:02:14 *</span><br><span class="line">Iter:   4500,  Train Loss:  0.47,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 87.48%,  Time: 0:02:17 *</span><br><span class="line">Iter:   4600,  Train Loss:  0.38,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 87.91%,  Time: 0:02:21 *</span><br><span class="line">Iter:   4700,  Train Loss:  0.52,  Train Acc: 82.81%,  Val Loss:  0.45,  Val Acc: 87.24%,  Time: 0:02:25 </span><br><span class="line">Iter:   4800,  Train Loss:  0.33,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 87.63%,  Time: 0:02:28 </span><br><span class="line">Iter:   4900,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 87.85%,  Time: 0:02:30 </span><br><span class="line">Iter:   5000,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 87.79%,  Time: 0:02:33 </span><br><span class="line">Iter:   5100,  Train Loss:  0.53,  Train Acc: 80.47%,  Val Loss:  0.45,  Val Acc: 87.48%,  Time: 0:02:36 </span><br><span class="line">Iter:   5200,  Train Loss:  0.58,  Train Acc: 85.16%,  Val Loss:  0.47,  Val Acc: 87.18%,  Time: 0:02:40 </span><br><span class="line">Iter:   5300,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.48,  Val Acc: 86.80%,  Time: 0:02:43 </span><br><span class="line">Iter:   5400,  Train Loss:  0.67,  Train Acc: 82.03%,  Val Loss:  0.45,  Val Acc: 87.63%,  Time: 0:02:46 </span><br><span class="line">Iter:   5500,  Train Loss:  0.42,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 87.70%,  Time: 0:02:48 *</span><br><span class="line">Iter:   5600,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.02%,  Time: 0:02:51 *</span><br><span class="line">Epoch [5&#x2F;20]</span><br><span class="line">Iter:   5700,  Train Loss:  0.41,  Train Acc: 86.72%,  Val Loss:  0.42,  Val Acc: 88.50%,  Time: 0:02:54 *</span><br><span class="line">Iter:   5800,  Train Loss:  0.22,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.11%,  Time: 0:02:57 </span><br><span class="line">Iter:   5900,  Train Loss:  0.39,  Train Acc: 91.41%,  Val Loss:  0.44,  Val Acc: 88.15%,  Time: 0:03:00 </span><br><span class="line">Iter:   6000,  Train Loss:  0.44,  Train Acc: 83.59%,  Val Loss:  0.41,  Val Acc: 88.30%,  Time: 0:03:03 *</span><br><span class="line">Iter:   6100,  Train Loss:  0.44,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 88.09%,  Time: 0:03:07 </span><br><span class="line">Iter:   6200,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.42,  Val Acc: 88.42%,  Time: 0:03:09 </span><br><span class="line">Iter:   6300,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.43,  Val Acc: 88.17%,  Time: 0:03:12 </span><br><span class="line">Iter:   6400,  Train Loss:  0.21,  Train Acc: 94.53%,  Val Loss:  0.41,  Val Acc: 88.25%,  Time: 0:03:15 *</span><br><span class="line">Iter:   6500,  Train Loss:  0.44,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 88.49%,  Time: 0:03:19 </span><br><span class="line">Iter:   6600,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.01%,  Time: 0:03:22 </span><br><span class="line">Iter:   6700,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.41,  Val Acc: 88.09%,  Time: 0:03:25 </span><br><span class="line">Iter:   6800,  Train Loss:  0.35,  Train Acc: 89.84%,  Val Loss:  0.45,  Val Acc: 88.01%,  Time: 0:03:28 </span><br><span class="line">Iter:   6900,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.41,  Val Acc: 88.16%,  Time: 0:03:32 </span><br><span class="line">Iter:   7000,  Train Loss:  0.41,  Train Acc: 84.38%,  Val Loss:   0.4,  Val Acc: 88.43%,  Time: 0:03:36 *</span><br><span class="line">Epoch [6&#x2F;20]</span><br><span class="line">Iter:   7100,  Train Loss:  0.41,  Train Acc: 83.59%,  Val Loss:   0.4,  Val Acc: 88.95%,  Time: 0:03:39 *</span><br><span class="line">Iter:   7200,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 88.71%,  Time: 0:03:41 </span><br><span class="line">Iter:   7300,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.42,  Val Acc: 88.67%,  Time: 0:03:44 </span><br><span class="line">Iter:   7400,  Train Loss:   0.6,  Train Acc: 77.34%,  Val Loss:  0.43,  Val Acc: 87.97%,  Time: 0:03:47 </span><br><span class="line">Iter:   7500,  Train Loss:  0.34,  Train Acc: 84.38%,  Val Loss:  0.42,  Val Acc: 88.02%,  Time: 0:03:50 </span><br><span class="line">Iter:   7600,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 87.11%,  Time: 0:03:54 </span><br><span class="line">Iter:   7700,  Train Loss:   0.4,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.16%,  Time: 0:03:57 *</span><br><span class="line">Iter:   7800,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 88.64%,  Time: 0:04:00 </span><br><span class="line">Iter:   7900,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 88.47%,  Time: 0:04:03 </span><br><span class="line">Iter:   8000,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 89.05%,  Time: 0:04:06 </span><br><span class="line">Iter:   8100,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.42,  Val Acc: 88.54%,  Time: 0:04:09 </span><br><span class="line">Iter:   8200,  Train Loss:  0.37,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.00%,  Time: 0:04:13 </span><br><span class="line">Iter:   8300,  Train Loss:  0.33,  Train Acc: 89.84%,  Val Loss:  0.41,  Val Acc: 87.85%,  Time: 0:04:16 </span><br><span class="line">Iter:   8400,  Train Loss:  0.53,  Train Acc: 78.91%,  Val Loss:  0.37,  Val Acc: 88.97%,  Time: 0:04:20 *</span><br><span class="line">Epoch [7&#x2F;20]</span><br><span class="line">Iter:   8500,  Train Loss:   0.5,  Train Acc: 85.16%,  Val Loss:  0.43,  Val Acc: 88.20%,  Time: 0:04:22 </span><br><span class="line">Iter:   8600,  Train Loss:  0.25,  Train Acc: 92.19%,  Val Loss:  0.42,  Val Acc: 88.93%,  Time: 0:04:26 </span><br><span class="line">Iter:   8700,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.48%,  Time: 0:04:29 </span><br><span class="line">Iter:   8800,  Train Loss:  0.44,  Train Acc: 82.81%,  Val Loss:  0.39,  Val Acc: 88.89%,  Time: 0:04:33 </span><br><span class="line">Iter:   8900,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 89.18%,  Time: 0:04:36 </span><br><span class="line">Iter:   9000,  Train Loss:  0.23,  Train Acc: 91.41%,  Val Loss:  0.39,  Val Acc: 89.37%,  Time: 0:04:39 </span><br><span class="line">Iter:   9100,  Train Loss:   0.5,  Train Acc: 84.38%,  Val Loss:  0.39,  Val Acc: 89.34%,  Time: 0:04:43 </span><br><span class="line">Iter:   9200,  Train Loss:   0.4,  Train Acc: 90.62%,  Val Loss:   0.4,  Val Acc: 88.81%,  Time: 0:04:46 </span><br><span class="line">Iter:   9300,  Train Loss:  0.41,  Train Acc: 85.16%,  Val Loss:  0.39,  Val Acc: 89.10%,  Time: 0:04:50 </span><br><span class="line">Iter:   9400,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.43,  Val Acc: 88.22%,  Time: 0:04:54 </span><br><span class="line">Iter:   9500,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:   0.4,  Val Acc: 89.02%,  Time: 0:04:57 </span><br><span class="line">Iter:   9600,  Train Loss:  0.47,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.25%,  Time: 0:05:01 </span><br><span class="line">Iter:   9700,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 88.91%,  Time: 0:05:04 </span><br><span class="line">Iter:   9800,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:  0.39,  Val Acc: 89.03%,  Time: 0:05:07 </span><br><span class="line">Epoch [8&#x2F;20]</span><br><span class="line">Iter:   9900,  Train Loss:  0.51,  Train Acc: 78.91%,  Val Loss:  0.39,  Val Acc: 89.19%,  Time: 0:05:11 </span><br><span class="line">Iter:  10000,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:   0.4,  Val Acc: 88.75%,  Time: 0:05:14 </span><br><span class="line">Iter:  10100,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.38,  Val Acc: 89.28%,  Time: 0:05:17 </span><br><span class="line">Iter:  10200,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:05:20 </span><br><span class="line">Iter:  10300,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 89.62%,  Time: 0:05:24 *</span><br><span class="line">Iter:  10400,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.22%,  Time: 0:05:27 </span><br><span class="line">Iter:  10500,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.41,  Val Acc: 88.86%,  Time: 0:05:31 </span><br><span class="line">Iter:  10600,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:   0.4,  Val Acc: 89.12%,  Time: 0:05:34 </span><br><span class="line">Iter:  10700,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 88.98%,  Time: 0:05:37 </span><br><span class="line">Iter:  10800,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.39,  Val Acc: 88.85%,  Time: 0:05:41 </span><br><span class="line">Iter:  10900,  Train Loss:  0.42,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 89.07%,  Time: 0:05:45 </span><br><span class="line">Iter:  11000,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.04%,  Time: 0:05:48 </span><br><span class="line">Iter:  11100,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 88.94%,  Time: 0:05:51 </span><br><span class="line">Iter:  11200,  Train Loss:  0.44,  Train Acc: 82.81%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:05:54 </span><br><span class="line">Epoch [9&#x2F;20]</span><br><span class="line">Iter:  11300,  Train Loss:  0.33,  Train Acc: 86.72%,  Val Loss:  0.38,  Val Acc: 89.51%,  Time: 0:05:57 </span><br><span class="line">Iter:  11400,  Train Loss:  0.39,  Train Acc: 85.16%,  Val Loss:   0.4,  Val Acc: 88.89%,  Time: 0:06:01 </span><br><span class="line">Iter:  11500,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:   0.4,  Val Acc: 89.20%,  Time: 0:06:04 </span><br><span class="line">Iter:  11600,  Train Loss:  0.19,  Train Acc: 95.31%,  Val Loss:  0.37,  Val Acc: 89.70%,  Time: 0:06:06 </span><br><span class="line">Iter:  11700,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.89%,  Time: 0:06:11 *</span><br><span class="line">Iter:  11800,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:06:14 </span><br><span class="line">Iter:  11900,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.39,  Val Acc: 89.47%,  Time: 0:06:17 </span><br><span class="line">Iter:  12000,  Train Loss:  0.48,  Train Acc: 86.72%,  Val Loss:  0.39,  Val Acc: 89.43%,  Time: 0:06:20 </span><br><span class="line">Iter:  12100,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.36,  Val Acc: 89.82%,  Time: 0:06:27 *</span><br><span class="line">Iter:  12200,  Train Loss:   0.3,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.53%,  Time: 0:06:30 </span><br><span class="line">Iter:  12300,  Train Loss:   0.3,  Train Acc: 92.97%,  Val Loss:  0.39,  Val Acc: 88.93%,  Time: 0:06:33 </span><br><span class="line">Iter:  12400,  Train Loss:  0.23,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.35%,  Time: 0:06:36 </span><br><span class="line">Iter:  12500,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 89.21%,  Time: 0:06:39 </span><br><span class="line">Iter:  12600,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.75%,  Time: 0:06:44 *</span><br><span class="line">Epoch [10&#x2F;20]</span><br><span class="line">Iter:  12700,  Train Loss:  0.27,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.64%,  Time: 0:06:48 *</span><br><span class="line">Iter:  12800,  Train Loss:   0.2,  Train Acc: 91.41%,  Val Loss:  0.37,  Val Acc: 89.23%,  Time: 0:06:51 </span><br><span class="line">Iter:  12900,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 89.47%,  Time: 0:06:54 </span><br><span class="line">Iter:  13000,  Train Loss:  0.38,  Train Acc: 91.41%,  Val Loss:   0.4,  Val Acc: 89.49%,  Time: 0:06:58 </span><br><span class="line">Iter:  13100,  Train Loss:  0.41,  Train Acc: 88.28%,  Val Loss:  0.37,  Val Acc: 89.39%,  Time: 0:07:01 </span><br><span class="line">Iter:  13200,  Train Loss:  0.55,  Train Acc: 85.16%,  Val Loss:  0.37,  Val Acc: 89.49%,  Time: 0:07:06 </span><br><span class="line">Iter:  13300,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 89.90%,  Time: 0:07:09 </span><br><span class="line">Iter:  13400,  Train Loss:  0.28,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.26%,  Time: 0:07:12 </span><br><span class="line">Iter:  13500,  Train Loss:   0.3,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.65%,  Time: 0:07:15 </span><br><span class="line">Iter:  13600,  Train Loss:  0.34,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 88.99%,  Time: 0:07:18 </span><br><span class="line">Iter:  13700,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.44%,  Time: 0:07:21 </span><br><span class="line">Iter:  13800,  Train Loss:  0.43,  Train Acc: 83.59%,  Val Loss:  0.35,  Val Acc: 89.62%,  Time: 0:07:25 </span><br><span class="line">Iter:  13900,  Train Loss:  0.28,  Train Acc: 92.19%,  Val Loss:   0.4,  Val Acc: 88.97%,  Time: 0:07:29 </span><br><span class="line">Iter:  14000,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.51%,  Time: 0:07:32 </span><br><span class="line">Epoch [11&#x2F;20]</span><br><span class="line">Iter:  14100,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.14%,  Time: 0:07:35 </span><br><span class="line">Iter:  14200,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.48%,  Time: 0:07:39 </span><br><span class="line">Iter:  14300,  Train Loss:  0.27,  Train Acc: 89.84%,  Val Loss:  0.37,  Val Acc: 89.69%,  Time: 0:07:42 </span><br><span class="line">Iter:  14400,  Train Loss:  0.23,  Train Acc: 90.62%,  Val Loss:  0.37,  Val Acc: 89.92%,  Time: 0:07:46 </span><br><span class="line">Iter:  14500,  Train Loss:  0.33,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.72%,  Time: 0:07:49 </span><br><span class="line">Iter:  14600,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.83%,  Time: 0:07:52 </span><br><span class="line">Iter:  14700,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 89.63%,  Time: 0:07:55 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.34,  Test Acc: 89.88%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.8745    0.8710    0.8727      1000</span><br><span class="line">       realty     0.9173    0.9210    0.9192      1000</span><br><span class="line">       stocks     0.8308    0.8350    0.8329      1000</span><br><span class="line">    education     0.9356    0.9450    0.9403      1000</span><br><span class="line">      science     0.8395    0.8160    0.8276      1000</span><br><span class="line">      society     0.9097    0.9070    0.9084      1000</span><br><span class="line">     politics     0.8994    0.8760    0.8875      1000</span><br><span class="line">       sports     0.9896    0.9530    0.9710      1000</span><br><span class="line">         game     0.9312    0.9070    0.9189      1000</span><br><span class="line">entertainment     0.8661    0.9570    0.9093      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.8988     10000</span><br><span class="line">    macro avg     0.8994    0.8988    0.8988     10000</span><br><span class="line"> weighted avg     0.8994    0.8988    0.8988     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[871  18  74   6  13   7   5   1   0   5]</span><br><span class="line"> [ 16 921  21   1   9  15   4   1   1  11]</span><br><span class="line"> [ 59  22 835   1  38   4  25   1  10   5]</span><br><span class="line"> [  3   4   0 945   8  11   8   1   4  16]</span><br><span class="line"> [ 15  10  40   9 816  15  24   1  38  32]</span><br><span class="line"> [  5  12   1  21   7 907  22   0   7  18]</span><br><span class="line"> [ 15   3  24  12  21  26 876   0   2  21]</span><br><span class="line"> [  1   2   2   3   5   3   6 953   0  25]</span><br><span class="line"> [  6   5   5   6  47   4   3   2 907  15]</span><br><span class="line"> [  5   7   3   6   8   5   1   3   5 957]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure>

<p>长文本（标题+正文）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14883.19it&#x2F;s]</span><br><span class="line">10000it [00:00, 12805.79it&#x2F;s]</span><br><span class="line">10000it [00:00, 14804.58it&#x2F;s]Time usage: 0:00:14</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (postion_embedding): Positional_Encoding(</span><br><span class="line">    (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  )</span><br><span class="line">  (encoder): Encoder(</span><br><span class="line">    (attention): Multi_Head_Attention(</span><br><span class="line">      (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">      (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">    (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">      (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">      (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (encoders): ModuleList(</span><br><span class="line">    (0): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (fc1): Linear(in_features&#x3D;9600, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.5,  Train Acc:  8.59%,  Val Loss:   5.1,  Val Acc: 10.00%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:   1.2,  Train Acc: 54.69%,  Val Loss:   1.4,  Val Acc: 59.11%,  Time: 0:00:05 *</span><br><span class="line">Iter:    200,  Train Loss:  0.91,  Train Acc: 64.06%,  Val Loss:  0.91,  Val Acc: 73.32%,  Time: 0:00:08 *</span><br><span class="line">Iter:    300,  Train Loss:  0.93,  Train Acc: 66.41%,  Val Loss:  0.67,  Val Acc: 80.04%,  Time: 0:00:11 *</span><br><span class="line">Iter:    400,  Train Loss:  0.94,  Train Acc: 68.75%,  Val Loss:  0.67,  Val Acc: 81.33%,  Time: 0:00:14</span><br><span class="line">Iter:    500,  Train Loss:   0.7,  Train Acc: 75.00%,  Val Loss:  0.59,  Val Acc: 83.27%,  Time: 0:00:18 *</span><br><span class="line">Iter:    600,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.56,  Val Acc: 84.59%,  Time: 0:00:21 *</span><br><span class="line">Iter:    700,  Train Loss:  0.59,  Train Acc: 79.69%,  Val Loss:  0.53,  Val Acc: 85.44%,  Time: 0:00:24 *</span><br><span class="line">Iter:    800,  Train Loss:  0.47,  Train Acc: 85.16%,  Val Loss:  0.66,  Val Acc: 82.96%,  Time: 0:00:27</span><br><span class="line">Iter:    900,  Train Loss:  0.71,  Train Acc: 81.25%,  Val Loss:  0.54,  Val Acc: 85.88%,  Time: 0:00:30</span><br><span class="line">Iter:   1000,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.48,  Val Acc: 86.87%,  Time: 0:00:34 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.51,  Train Acc: 84.38%,  Val Loss:   0.5,  Val Acc: 86.69%,  Time: 0:00:37</span><br><span class="line">Iter:   1200,  Train Loss:  0.63,  Train Acc: 82.03%,  Val Loss:   0.5,  Val Acc: 87.30%,  Time: 0:00:40</span><br><span class="line">Iter:   1300,  Train Loss:  0.48,  Train Acc: 83.59%,  Val Loss:  0.48,  Val Acc: 87.17%,  Time: 0:00:43</span><br><span class="line">Iter:   1400,  Train Loss:  0.52,  Train Acc: 83.59%,  Val Loss:  0.49,  Val Acc: 87.48%,  Time: 0:00:46</span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:   0.6,  Train Acc: 79.69%,  Val Loss:  0.48,  Val Acc: 87.80%,  Time: 0:00:49 *</span><br><span class="line">Iter:   1600,  Train Loss:  0.46,  Train Acc: 86.72%,  Val Loss:  0.48,  Val Acc: 87.64%,  Time: 0:00:50</span><br><span class="line">Iter:   1700,  Train Loss:  0.43,  Train Acc: 85.16%,  Val Loss:  0.56,  Val Acc: 85.89%,  Time: 0:00:52</span><br><span class="line">Iter:   1800,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.43,  Val Acc: 88.56%,  Time: 0:00:54 *</span><br><span class="line">Iter:   1900,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.43,  Val Acc: 89.19%,  Time: 0:00:56</span><br><span class="line">Iter:   2000,  Train Loss:  0.43,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 88.27%,  Time: 0:00:57</span><br><span class="line">Iter:   2100,  Train Loss:  0.58,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 88.74%,  Time: 0:00:59 *</span><br><span class="line">Iter:   2200,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:  0.45,  Val Acc: 88.39%,  Time: 0:01:01</span><br><span class="line">Iter:   2300,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.75%,  Time: 0:01:02 *</span><br><span class="line">Iter:   2400,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 88.05%,  Time: 0:01:04</span><br><span class="line">Iter:   2500,  Train Loss:  0.55,  Train Acc: 82.03%,  Val Loss:  0.45,  Val Acc: 88.85%,  Time: 0:01:05</span><br><span class="line">Iter:   2600,  Train Loss:  0.44,  Train Acc: 83.59%,  Val Loss:  0.44,  Val Acc: 89.37%,  Time: 0:01:07</span><br><span class="line">Iter:   2700,  Train Loss:  0.52,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 89.23%,  Time: 0:01:09</span><br><span class="line">Iter:   2800,  Train Loss:   0.5,  Train Acc: 82.03%,  Val Loss:  0.41,  Val Acc: 89.03%,  Time: 0:01:11</span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.41,  Val Acc: 89.40%,  Time: 0:01:12</span><br><span class="line">Iter:   3000,  Train Loss:  0.38,  Train Acc: 86.72%,  Val Loss:  0.41,  Val Acc: 89.39%,  Time: 0:01:14</span><br><span class="line">Iter:   3100,  Train Loss:  0.26,  Train Acc: 87.50%,  Val Loss:  0.45,  Val Acc: 89.05%,  Time: 0:01:15</span><br><span class="line">Iter:   3200,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 90.10%,  Time: 0:01:17 *</span><br><span class="line">Iter:   3300,  Train Loss:  0.37,  Train Acc: 91.41%,  Val Loss:   0.4,  Val Acc: 90.25%,  Time: 0:01:19</span><br><span class="line">Iter:   3400,  Train Loss:  0.43,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 89.23%,  Time: 0:01:20</span><br><span class="line">Iter:   3500,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.35,  Val Acc: 90.79%,  Time: 0:01:22 *</span><br><span class="line">Iter:   3600,  Train Loss:  0.51,  Train Acc: 87.50%,  Val Loss:   0.4,  Val Acc: 89.65%,  Time: 0:01:24</span><br><span class="line">Iter:   3700,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 90.74%,  Time: 0:01:26 *</span><br><span class="line">Iter:   3800,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 90.38%,  Time: 0:01:27</span><br><span class="line">Iter:   3900,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.41,  Val Acc: 89.36%,  Time: 0:01:29</span><br><span class="line">Iter:   4000,  Train Loss:  0.47,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 90.55%,  Time: 0:01:31</span><br><span class="line">Iter:   4100,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:  0.36,  Val Acc: 90.73%,  Time: 0:01:33</span><br><span class="line">Iter:   4200,  Train Loss:  0.31,  Train Acc: 92.97%,  Val Loss:  0.45,  Val Acc: 88.83%,  Time: 0:01:35</span><br><span class="line">Epoch [4&#x2F;20]</span><br><span class="line">Iter:   4300,  Train Loss:   0.2,  Train Acc: 93.75%,  Val Loss:   0.4,  Val Acc: 89.56%,  Time: 0:01:37</span><br><span class="line">Iter:   4400,  Train Loss:  0.34,  Train Acc: 86.72%,  Val Loss:  0.34,  Val Acc: 90.85%,  Time: 0:01:38 *</span><br><span class="line">Iter:   4500,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.45,  Val Acc: 89.03%,  Time: 0:01:40</span><br><span class="line">Iter:   4600,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 90.92%,  Time: 0:01:42</span><br><span class="line">Iter:   4700,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.38,  Val Acc: 90.14%,  Time: 0:01:44</span><br><span class="line">Iter:   4800,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 90.78%,  Time: 0:01:46</span><br><span class="line">Iter:   4900,  Train Loss:  0.49,  Train Acc: 85.16%,  Val Loss:  0.36,  Val Acc: 90.53%,  Time: 0:01:47</span><br><span class="line">Iter:   5000,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 90.31%,  Time: 0:01:49</span><br><span class="line">Iter:   5100,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 90.70%,  Time: 0:01:51 *</span><br><span class="line">Iter:   5200,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 90.89%,  Time: 0:01:53 *</span><br><span class="line">Iter:   5300,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.32,  Val Acc: 91.47%,  Time: 0:01:54 *</span><br><span class="line">Iter:   5400,  Train Loss:  0.28,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 91.17%,  Time: 0:01:56</span><br><span class="line">Iter:   5500,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.32,  Val Acc: 90.96%,  Time: 0:01:58 *</span><br><span class="line">Iter:   5600,  Train Loss:  0.35,  Train Acc: 87.50%,  Val Loss:  0.31,  Val Acc: 91.41%,  Time: 0:02:00 *</span><br><span class="line">Epoch [5&#x2F;20]</span><br><span class="line">Iter:   5700,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.33,  Val Acc: 90.82%,  Time: 0:02:02</span><br><span class="line">Iter:   5800,  Train Loss:  0.28,  Train Acc: 90.62%,  Val Loss:  0.32,  Val Acc: 91.13%,  Time: 0:02:03</span><br><span class="line">Iter:   5900,  Train Loss:  0.28,  Train Acc: 87.50%,  Val Loss:  0.33,  Val Acc: 90.78%,  Time: 0:02:05</span><br><span class="line">Iter:   6000,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.31,  Val Acc: 91.33%,  Time: 0:02:07</span><br><span class="line">Iter:   6100,  Train Loss:  0.18,  Train Acc: 95.31%,  Val Loss:  0.36,  Val Acc: 90.30%,  Time: 0:02:08</span><br><span class="line">Iter:   6200,  Train Loss:  0.46,  Train Acc: 84.38%,  Val Loss:  0.35,  Val Acc: 90.57%,  Time: 0:02:10</span><br><span class="line">Iter:   6300,  Train Loss:  0.24,  Train Acc: 91.41%,  Val Loss:   0.3,  Val Acc: 91.65%,  Time: 0:02:12 *</span><br><span class="line">Iter:   6400,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:   0.3,  Val Acc: 91.65%,  Time: 0:02:14</span><br><span class="line">Iter:   6500,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.31,  Val Acc: 91.30%,  Time: 0:02:16</span><br><span class="line">Iter:   6600,  Train Loss:  0.32,  Train Acc: 86.72%,  Val Loss:   0.3,  Val Acc: 91.93%,  Time: 0:02:18</span><br><span class="line">Iter:   6700,  Train Loss:   0.2,  Train Acc: 94.53%,  Val Loss:  0.31,  Val Acc: 91.30%,  Time: 0:02:20</span><br><span class="line">Iter:   6800,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.71%,  Time: 0:02:21 *</span><br><span class="line">Iter:   6900,  Train Loss:  0.21,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.89%,  Time: 0:02:23</span><br><span class="line">Iter:   7000,  Train Loss:  0.44,  Train Acc: 85.94%,  Val Loss:   0.3,  Val Acc: 91.50%,  Time: 0:02:25</span><br><span class="line">Epoch [6&#x2F;20]</span><br><span class="line">Iter:   7100,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.28,  Val Acc: 91.57%,  Time: 0:02:27 *</span><br><span class="line">Iter:   7200,  Train Loss:   0.2,  Train Acc: 92.19%,  Val Loss:  0.29,  Val Acc: 91.81%,  Time: 0:02:29</span><br><span class="line">Iter:   7300,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.34,  Val Acc: 90.86%,  Time: 0:02:31</span><br><span class="line">Iter:   7400,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.29,  Val Acc: 91.68%,  Time: 0:02:33</span><br><span class="line">Iter:   7500,  Train Loss:  0.21,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 92.08%,  Time: 0:02:35</span><br><span class="line">Iter:   7600,  Train Loss:  0.26,  Train Acc: 89.06%,  Val Loss:   0.3,  Val Acc: 91.82%,  Time: 0:02:36</span><br><span class="line">Iter:   7700,  Train Loss:  0.23,  Train Acc: 90.62%,  Val Loss:   0.3,  Val Acc: 91.81%,  Time: 0:02:38</span><br><span class="line">Iter:   7800,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.28,  Val Acc: 91.81%,  Time: 0:02:40 *</span><br><span class="line">Iter:   7900,  Train Loss:  0.34,  Train Acc: 91.41%,  Val Loss:  0.27,  Val Acc: 92.38%,  Time: 0:02:42 *</span><br><span class="line">Iter:   8000,  Train Loss:  0.51,  Train Acc: 82.81%,  Val Loss:  0.28,  Val Acc: 92.01%,  Time: 0:02:44</span><br><span class="line">Iter:   8100,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.69%,  Time: 0:02:46</span><br><span class="line">Iter:   8200,  Train Loss:  0.34,  Train Acc: 89.84%,  Val Loss:  0.31,  Val Acc: 91.31%,  Time: 0:02:48</span><br><span class="line">Iter:   8300,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.32,  Val Acc: 91.25%,  Time: 0:02:49</span><br><span class="line">Iter:   8400,  Train Loss:  0.27,  Train Acc: 92.19%,  Val Loss:  0.28,  Val Acc: 92.02%,  Time: 0:02:51</span><br><span class="line">Epoch [7&#x2F;20]</span><br><span class="line">Iter:   8500,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.79%,  Time: 0:02:53</span><br><span class="line">Iter:   8600,  Train Loss:  0.26,  Train Acc: 89.84%,  Val Loss:  0.28,  Val Acc: 92.21%,  Time: 0:02:55</span><br><span class="line">Iter:   8700,  Train Loss:  0.25,  Train Acc: 89.84%,  Val Loss:  0.29,  Val Acc: 91.82%,  Time: 0:02:57</span><br><span class="line">Iter:   8800,  Train Loss:  0.23,  Train Acc: 92.97%,  Val Loss:  0.27,  Val Acc: 91.80%,  Time: 0:02:59</span><br><span class="line">Iter:   8900,  Train Loss:  0.29,  Train Acc: 92.19%,  Val Loss:  0.29,  Val Acc: 91.47%,  Time: 0:03:01</span><br><span class="line">Iter:   9000,  Train Loss:  0.17,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.84%,  Time: 0:03:03</span><br><span class="line">Iter:   9100,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.31,  Val Acc: 90.89%,  Time: 0:03:05</span><br><span class="line">Iter:   9200,  Train Loss:  0.18,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.57%,  Time: 0:03:07</span><br><span class="line">Iter:   9300,  Train Loss:  0.34,  Train Acc: 89.84%,  Val Loss:  0.27,  Val Acc: 91.93%,  Time: 0:03:08</span><br><span class="line">Iter:   9400,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.27,  Val Acc: 92.09%,  Time: 0:03:10</span><br><span class="line">Iter:   9500,  Train Loss:  0.25,  Train Acc: 92.19%,  Val Loss:  0.29,  Val Acc: 92.05%,  Time: 0:03:12</span><br><span class="line">Iter:   9600,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.28,  Val Acc: 92.03%,  Time: 0:03:13</span><br><span class="line">Iter:   9700,  Train Loss:  0.26,  Train Acc: 89.84%,  Val Loss:   0.3,  Val Acc: 91.83%,  Time: 0:03:15</span><br><span class="line">Iter:   9800,  Train Loss:  0.19,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.76%,  Time: 0:03:17</span><br><span class="line">Epoch [8&#x2F;20]</span><br><span class="line">Iter:   9900,  Train Loss:  0.29,  Train Acc: 88.28%,  Val Loss:  0.28,  Val Acc: 91.96%,  Time: 0:03:18</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.26,  Test Acc: 92.28%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9250    0.8760    0.8998      1000</span><br><span class="line">       realty     0.9103    0.9340    0.9220      1000</span><br><span class="line">       stocks     0.8696    0.9000    0.8845      1000</span><br><span class="line">    education     0.9550    0.9330    0.9439      1000</span><br><span class="line">      science     0.9078    0.8660    0.8864      1000</span><br><span class="line">      society     0.9214    0.8790    0.8997      1000</span><br><span class="line">     politics     0.8917    0.9300    0.9104      1000</span><br><span class="line">       sports     0.9879    0.9820    0.9850      1000</span><br><span class="line">         game     0.9367    0.9620    0.9492      1000</span><br><span class="line">entertainment     0.9262    0.9660    0.9457      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9228     10000</span><br><span class="line">    macro avg     0.9231    0.9228    0.9227     10000</span><br><span class="line"> weighted avg     0.9231    0.9228    0.9227     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[876  19  72   1   8   9   7   0   0   8]</span><br><span class="line"> [  9 934  19   2  10  11   8   0   2   5]</span><br><span class="line"> [ 36  12 900   2  22   2  17   1   6   2]</span><br><span class="line"> [  1  12   4 933   2  12  24   1   3   8]</span><br><span class="line"> [ 12   8  22   5 866  10  15   1  43  18]</span><br><span class="line"> [  4  19   5  24  17 879  33   2   3  14]</span><br><span class="line"> [  4   9   7   5  10  23 930   1   2   9]</span><br><span class="line"> [  1   2   2   0   2   2   2 982   0   7]</span><br><span class="line"> [  4   6   4   1  13   2   2   0 962   6]</span><br><span class="line"> [  0   5   0   4   4   4   5   6   6 966]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure>



<h2 id="3-4-cnn模型"><a href="#3-4-cnn模型" class="headerlink" title="3.4 cnn模型"></a>3.4 cnn模型</h2><p>1.运行脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run.py --model TextCNN &gt; nohup_cnn.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>2.运行log</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:03, 47432.48it&#x2F;s]</span><br><span class="line">10000it [00:00, 46997.85it&#x2F;s]</span><br><span class="line">10000it [00:00, 48451.14it&#x2F;s]Time usage: 0:00:04</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (convs): ModuleList(</span><br><span class="line">    (0): Conv2d(1, 256, kernel_size&#x3D;(2, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(1, 256, kernel_size&#x3D;(3, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (2): Conv2d(1, 256, kernel_size&#x3D;(4, 300), stride&#x3D;(1, 1))</span><br><span class="line">  )</span><br><span class="line">  (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  (fc): Linear(in_features&#x3D;768, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc: 14.84%,  Val Loss:   2.7,  Val Acc: 10.70%,  Time: 0:00:02 *</span><br><span class="line">Iter:    100,  Train Loss:  0.77,  Train Acc: 70.31%,  Val Loss:   0.7,  Val Acc: 78.36%,  Time: 0:00:07 *</span><br><span class="line">Iter:    200,  Train Loss:  0.74,  Train Acc: 74.22%,  Val Loss:  0.55,  Val Acc: 83.18%,  Time: 0:00:12 *</span><br><span class="line">Iter:    300,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.49,  Val Acc: 84.72%,  Time: 0:00:16 *</span><br><span class="line">Iter:    400,  Train Loss:  0.72,  Train Acc: 79.69%,  Val Loss:  0.48,  Val Acc: 85.10%,  Time: 0:00:20 *</span><br><span class="line">Iter:    500,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.44,  Val Acc: 86.10%,  Time: 0:00:24 *</span><br><span class="line">Iter:    600,  Train Loss:  0.49,  Train Acc: 84.38%,  Val Loss:  0.42,  Val Acc: 86.77%,  Time: 0:00:27 *</span><br><span class="line">Iter:    700,  Train Loss:  0.49,  Train Acc: 83.59%,  Val Loss:  0.41,  Val Acc: 87.15%,  Time: 0:00:31 *</span><br><span class="line">Iter:    800,  Train Loss:  0.43,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 87.80%,  Time: 0:00:36 *</span><br><span class="line">Iter:    900,  Train Loss:  0.47,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 88.04%,  Time: 0:00:40 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.38,  Val Acc: 88.54%,  Time: 0:00:45 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.38,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 88.64%,  Time: 0:00:50 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.38,  Train Acc: 86.72%,  Val Loss:  0.37,  Val Acc: 88.99%,  Time: 0:00:55 *</span><br><span class="line">Iter:   1300,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.81%,  Time: 0:01:00 *</span><br><span class="line">Iter:   1400,  Train Loss:  0.52,  Train Acc: 82.81%,  Val Loss:  0.35,  Val Acc: 88.96%,  Time: 0:01:04 *</span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:   0.4,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.09%,  Time: 0:01:10 *</span><br><span class="line">Iter:   1600,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.15%,  Time: 0:01:14</span><br><span class="line">Iter:   1700,  Train Loss:   0.4,  Train Acc: 86.72%,  Val Loss:  0.35,  Val Acc: 89.68%,  Time: 0:01:18 *</span><br><span class="line">Iter:   1800,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.90%,  Time: 0:01:22</span><br><span class="line">Iter:   1900,  Train Loss:  0.36,  Train Acc: 89.84%,  Val Loss:  0.34,  Val Acc: 89.37%,  Time: 0:01:27 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.35,  Train Acc: 85.16%,  Val Loss:  0.35,  Val Acc: 89.47%,  Time: 0:01:31</span><br><span class="line">Iter:   2100,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.40%,  Time: 0:01:35 *</span><br><span class="line">Iter:   2200,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.43%,  Time: 0:01:40</span><br><span class="line">Iter:   2300,  Train Loss:  0.31,  Train Acc: 93.75%,  Val Loss:  0.34,  Val Acc: 89.67%,  Time: 0:01:44 *</span><br><span class="line">Iter:   2400,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.90%,  Time: 0:01:49 *</span><br><span class="line">Iter:   2500,  Train Loss:  0.16,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.18%,  Time: 0:01:53 *</span><br><span class="line">Iter:   2600,  Train Loss:   0.4,  Train Acc: 83.59%,  Val Loss:  0.33,  Val Acc: 90.04%,  Time: 0:01:58</span><br><span class="line">Iter:   2700,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.90%,  Time: 0:02:02</span><br><span class="line">Iter:   2800,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.67%,  Time: 0:02:06</span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 89.80%,  Time: 0:02:11</span><br><span class="line">Iter:   3000,  Train Loss:  0.25,  Train Acc: 91.41%,  Val Loss:  0.34,  Val Acc: 89.71%,  Time: 0:02:15</span><br><span class="line">Iter:   3100,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 89.98%,  Time: 0:02:19</span><br><span class="line">Iter:   3200,  Train Loss:  0.39,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 89.88%,  Time: 0:02:24</span><br><span class="line">Iter:   3300,  Train Loss:  0.29,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.11%,  Time: 0:02:28 *</span><br><span class="line">Iter:   3400,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.97%,  Time: 0:02:33</span><br><span class="line">Iter:   3500,  Train Loss:  0.16,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 90.13%,  Time: 0:02:38</span><br><span class="line">Iter:   3600,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:02:42</span><br><span class="line">Iter:   3700,  Train Loss:  0.36,  Train Acc: 86.72%,  Val Loss:  0.33,  Val Acc: 90.08%,  Time: 0:02:47</span><br><span class="line">Iter:   3800,  Train Loss:  0.36,  Train Acc: 84.38%,  Val Loss:  0.32,  Val Acc: 90.17%,  Time: 0:02:52 *</span><br><span class="line">Iter:   3900,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 90.20%,  Time: 0:02:56</span><br><span class="line">Iter:   4000,  Train Loss:  0.24,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 90.23%,  Time: 0:03:01</span><br><span class="line">Iter:   4100,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:03:05</span><br><span class="line">Iter:   4200,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:03:11</span><br><span class="line">Epoch [4&#x2F;20]</span><br><span class="line">Iter:   4300,  Train Loss:  0.19,  Train Acc: 92.97%,  Val Loss:  0.32,  Val Acc: 90.27%,  Time: 0:03:15 *</span><br><span class="line">Iter:   4400,  Train Loss:  0.19,  Train Acc: 93.75%,  Val Loss:  0.32,  Val Acc: 90.37%,  Time: 0:03:19</span><br><span class="line">Iter:   4500,  Train Loss:  0.37,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 90.21%,  Time: 0:03:24</span><br><span class="line">Iter:   4600,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 90.02%,  Time: 0:03:29</span><br><span class="line">Iter:   4700,  Train Loss:  0.48,  Train Acc: 87.50%,  Val Loss:  0.32,  Val Acc: 90.25%,  Time: 0:03:34</span><br><span class="line">Iter:   4800,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 90.34%,  Time: 0:03:38</span><br><span class="line">Iter:   4900,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 90.26%,  Time: 0:03:41</span><br><span class="line">Iter:   5000,  Train Loss:  0.21,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.88%,  Time: 0:03:46</span><br><span class="line">Iter:   5100,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.21%,  Time: 0:03:50</span><br><span class="line">Iter:   5200,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 90.12%,  Time: 0:03:55</span><br><span class="line">Iter:   5300,  Train Loss:  0.19,  Train Acc: 96.09%,  Val Loss:  0.32,  Val Acc: 90.46%,  Time: 0:04:00 *</span><br><span class="line">Iter:   5400,  Train Loss:  0.47,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 90.17%,  Time: 0:04:04</span><br><span class="line">Iter:   5500,  Train Loss:  0.27,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.41%,  Time: 0:04:08</span><br><span class="line">Iter:   5600,  Train Loss:  0.13,  Train Acc: 95.31%,  Val Loss:  0.33,  Val Acc: 90.17%,  Time: 0:04:13</span><br><span class="line">Epoch [5&#x2F;20]</span><br><span class="line">Iter:   5700,  Train Loss:  0.24,  Train Acc: 94.53%,  Val Loss:  0.34,  Val Acc: 90.07%,  Time: 0:04:18</span><br><span class="line">Iter:   5800,  Train Loss:  0.16,  Train Acc: 93.75%,  Val Loss:  0.34,  Val Acc: 90.14%,  Time: 0:04:22</span><br><span class="line">Iter:   5900,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 90.52%,  Time: 0:04:28</span><br><span class="line">Iter:   6000,  Train Loss:  0.15,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.45%,  Time: 0:04:32</span><br><span class="line">Iter:   6100,  Train Loss:  0.26,  Train Acc: 89.06%,  Val Loss:  0.32,  Val Acc: 90.58%,  Time: 0:04:36</span><br><span class="line">Iter:   6200,  Train Loss:  0.13,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 90.46%,  Time: 0:04:41</span><br><span class="line">Iter:   6300,  Train Loss:  0.12,  Train Acc: 96.88%,  Val Loss:  0.33,  Val Acc: 90.56%,  Time: 0:04:45</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.29,  Test Acc: 91.34%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9193    0.8890    0.9039      1000</span><br><span class="line">       realty     0.9134    0.9490    0.9308      1000</span><br><span class="line">       stocks     0.8363    0.8790    0.8571      1000</span><br><span class="line">    education     0.9540    0.9550    0.9545      1000</span><br><span class="line">      science     0.8800    0.8580    0.8689      1000</span><br><span class="line">      society     0.9067    0.9140    0.9104      1000</span><br><span class="line">     politics     0.9054    0.8900    0.8976      1000</span><br><span class="line">       sports     0.9578    0.9540    0.9559      1000</span><br><span class="line">         game     0.9383    0.9130    0.9255      1000</span><br><span class="line">entertainment     0.9265    0.9330    0.9297      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9134     10000</span><br><span class="line">    macro avg     0.9138    0.9134    0.9134     10000</span><br><span class="line"> weighted avg     0.9138    0.9134    0.9134     10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[889  17  59   4  10   9   7   3   0   2]</span><br><span class="line"> [  9 949  13   2   4   8   4   2   3   6]</span><br><span class="line"> [ 40  24 879   3  21   0  25   3   4   1]</span><br><span class="line"> [  2   2   1 955   6  13   5   2   1  13]</span><br><span class="line"> [  8   8  38   6 858  15  16   5  36  10]</span><br><span class="line"> [  4  22   3  11  10 914  26   2   2   6]</span><br><span class="line"> [  9   7  32  10  15  27 890   4   0   6]</span><br><span class="line"> [  3   2   7   1   3   8   4 954   2  16]</span><br><span class="line"> [  2   1  15   4  36   4   3   8 913  14]</span><br><span class="line"> [  1   7   4   5  12  10   3  13  12 933]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure>

<p>长文本（标题+正文）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14970.72it&#x2F;s]</span><br><span class="line">10000it [00:00, 13435.87it&#x2F;s]</span><br><span class="line">10000it [00:00, 16773.90it&#x2F;s]</span><br><span class="line">Time usage: 0:00:13</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (convs): ModuleList(</span><br><span class="line">    (0): Conv2d(1, 256, kernel_size&#x3D;(2, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(1, 256, kernel_size&#x3D;(3, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (2): Conv2d(1, 256, kernel_size&#x3D;(4, 300), stride&#x3D;(1, 1))</span><br><span class="line">  )</span><br><span class="line">  (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  (fc): Linear(in_features&#x3D;768, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (convs): ModuleList(</span><br><span class="line">    (0): Conv2d(1, 256, kernel_size&#x3D;(2, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(1, 256, kernel_size&#x3D;(3, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (2): Conv2d(1, 256, kernel_size&#x3D;(4, 300), stride&#x3D;(1, 1))</span><br><span class="line">  )</span><br><span class="line">  (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  (fc): Linear(in_features&#x3D;768, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc: 11.72%,  Val Loss:   2.5,  Val Acc: 10.00%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:  0.53,  Train Acc: 83.59%,  Val Loss:  0.53,  Val Acc: 84.06%,  Time: 0:00:07 *</span><br><span class="line">Iter:    200,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.42,  Val Acc: 87.16%,  Time: 0:00:13 *</span><br><span class="line">Iter:    300,  Train Loss:  0.57,  Train Acc: 80.47%,  Val Loss:  0.37,  Val Acc: 88.62%,  Time: 0:00:17 *</span><br><span class="line">Iter:    400,  Train Loss:  0.43,  Train Acc: 88.28%,  Val Loss:  0.34,  Val Acc: 89.45%,  Time: 0:00:23 *</span><br><span class="line">Iter:    500,  Train Loss:   0.3,  Train Acc: 89.84%,  Val Loss:  0.32,  Val Acc: 89.96%,  Time: 0:00:29 *</span><br><span class="line">Iter:    600,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 89.63%,  Time: 0:00:33</span><br><span class="line">Iter:    700,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:   0.3,  Val Acc: 90.63%,  Time: 0:00:39 *</span><br><span class="line">Iter:    800,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.21%,  Time: 0:00:44 *</span><br><span class="line">Iter:    900,  Train Loss:  0.47,  Train Acc: 87.50%,  Val Loss:  0.28,  Val Acc: 91.13%,  Time: 0:00:49 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.28,  Train Acc: 89.84%,  Val Loss:  0.27,  Val Acc: 91.50%,  Time: 0:00:55 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.33,  Train Acc: 90.62%,  Val Loss:  0.26,  Val Acc: 92.00%,  Time: 0:01:01 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.51,  Train Acc: 86.72%,  Val Loss:  0.26,  Val Acc: 91.98%,  Time: 0:01:06</span><br><span class="line">Iter:   1300,  Train Loss:  0.28,  Train Acc: 88.28%,  Val Loss:  0.26,  Val Acc: 91.88%,  Time: 0:01:12 *</span><br><span class="line">Iter:   1400,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.82%,  Time: 0:01:16 *</span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:   0.4,  Train Acc: 89.84%,  Val Loss:  0.25,  Val Acc: 92.30%,  Time: 0:01:22 *</span><br><span class="line">Iter:   1600,  Train Loss:  0.35,  Train Acc: 92.19%,  Val Loss:  0.25,  Val Acc: 92.14%,  Time: 0:01:28 *</span><br><span class="line">Iter:   1700,  Train Loss:  0.31,  Train Acc: 93.75%,  Val Loss:  0.25,  Val Acc: 92.26%,  Time: 0:01:33</span><br><span class="line">Iter:   1800,  Train Loss:  0.23,  Train Acc: 89.84%,  Val Loss:  0.26,  Val Acc: 91.56%,  Time: 0:01:39</span><br><span class="line">Iter:   1900,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.49%,  Time: 0:01:43 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.25,  Val Acc: 92.26%,  Time: 0:01:49</span><br><span class="line">Iter:   2100,  Train Loss:  0.31,  Train Acc: 92.97%,  Val Loss:  0.24,  Val Acc: 92.43%,  Time: 0:01:55 *</span><br><span class="line">Iter:   2200,  Train Loss:  0.27,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.63%,  Time: 0:01:59 *</span><br><span class="line">Iter:   2300,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.51%,  Time: 0:02:06</span><br><span class="line">Iter:   2400,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.24,  Val Acc: 92.71%,  Time: 0:02:12</span><br><span class="line">Iter:   2500,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.25,  Val Acc: 92.40%,  Time: 0:02:17</span><br><span class="line">Iter:   2600,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 92.69%,  Time: 0:02:24 *</span><br><span class="line">Iter:   2700,  Train Loss:  0.23,  Train Acc: 93.75%,  Val Loss:  0.23,  Val Acc: 92.76%,  Time: 0:02:29</span><br><span class="line">Iter:   2800,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 92.76%,  Time: 0:02:35 *</span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.13,  Train Acc: 96.09%,  Val Loss:  0.24,  Val Acc: 92.43%,  Time: 0:02:41</span><br><span class="line">Iter:   3000,  Train Loss:  0.17,  Train Acc: 92.19%,  Val Loss:  0.23,  Val Acc: 92.86%,  Time: 0:02:47</span><br><span class="line">Iter:   3100,  Train Loss:  0.21,  Train Acc: 94.53%,  Val Loss:  0.23,  Val Acc: 92.68%,  Time: 0:02:54</span><br><span class="line">Iter:   3200,  Train Loss:  0.14,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 92.91%,  Time: 0:02:58 *</span><br><span class="line">Iter:   3300,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.83%,  Time: 0:03:05</span><br><span class="line">Iter:   3400,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.75%,  Time: 0:03:10</span><br><span class="line">Iter:   3500,  Train Loss:  0.18,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 93.05%,  Time: 0:03:16</span><br><span class="line">Iter:   3600,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.24,  Val Acc: 92.68%,  Time: 0:03:22</span><br><span class="line">Iter:   3700,  Train Loss:  0.13,  Train Acc: 96.09%,  Val Loss:  0.23,  Val Acc: 92.89%,  Time: 0:03:28</span><br><span class="line">Iter:   3800,  Train Loss:  0.21,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.93%,  Time: 0:03:34</span><br><span class="line">Iter:   3900,  Train Loss:  0.13,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 93.03%,  Time: 0:03:39</span><br><span class="line">Iter:   4000,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 93.02%,  Time: 0:03:46</span><br><span class="line">Iter:   4100,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.24,  Val Acc: 92.72%,  Time: 0:03:52</span><br><span class="line">Iter:   4200,  Train Loss:  0.15,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.88%,  Time: 0:03:58</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.23,  Test Acc: 92.71%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9365    0.9000    0.9179      1000</span><br><span class="line">       realty     0.9260    0.9380    0.9319      1000</span><br><span class="line">       stocks     0.8817    0.9170    0.8990      1000</span><br><span class="line">    education     0.9568    0.9520    0.9544      1000</span><br><span class="line">      science     0.9192    0.8760    0.8971      1000</span><br><span class="line">      society     0.8951    0.8960    0.8956      1000</span><br><span class="line">     politics     0.9204    0.9130    0.9167      1000</span><br><span class="line">       sports     0.9732    0.9810    0.9771      1000</span><br><span class="line">         game     0.9360    0.9510    0.9435      1000</span><br><span class="line">entertainment     0.9275    0.9470    0.9372      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9271     10000</span><br><span class="line">    macro avg     0.9272    0.9271    0.9270     10000</span><br><span class="line"> weighted avg     0.9272    0.9271    0.9270     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[900  18  57   0   5   9   7   0   0   4]</span><br><span class="line"> [  5 938  16   2   9  10   7   2   6   5]</span><br><span class="line"> [ 32  14 917   4  11   0  15   0   4   3]</span><br><span class="line"> [  1   2   3 952   1  20   9   0   0  12]</span><br><span class="line"> [  5   6  32   5 876  20  11   2  34   9]</span><br><span class="line"> [  5  18   0  19  18 896  20   3   3  18]</span><br><span class="line"> [  8   8   9   9   5  29 913   5   4  10]</span><br><span class="line"> [  2   3   1   0   2   1   2 981   0   8]</span><br><span class="line"> [  1   4   4   0  20   4   2   9 951   5]</span><br><span class="line"> [  2   2   1   4   6  12   6   6  14 947]]</span><br><span class="line">Time usage: 0:00:01</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure>



<h1 id="4-评价"><a href="#4-评价" class="headerlink" title="4 评价"></a>4 评价</h1><h2 id="4-1-评价指标"><a href="#4-1-评价指标" class="headerlink" title="4.1 评价指标"></a>4.1 评价指标</h2><h2 id="4-2-结果汇总"><a href="#4-2-结果汇总" class="headerlink" title="4.2 结果汇总"></a>4.2 结果汇总</h2><blockquote>
<p>参考：</p>
<p>[1] Convolutional Neural Networks for Sentence Classification<br>[2] Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification<br>[3] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br>[4] Attention Is All You Need</p>
<p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch">https://github.com/649453932/Chinese-Text-Classification-Pytorch</a></p>
<p><a href="https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch">https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch</a></p>
</blockquote>
]]></content>
      <categories>
        <category>research</category>
        <category>text_classify</category>
      </categories>
  </entry>
</search>

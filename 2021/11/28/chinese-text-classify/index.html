<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="中文文本分类流程">
<meta property="og:type" content="article">
<meta property="og:title" content="chinese_text_classify">
<meta property="og:url" content="http://example.com/2021/11/28/chinese-text-classify/index.html">
<meta property="og:site_name" content="札记">
<meta property="og:description" content="中文文本分类流程">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-11-28T05:16:23.000Z">
<meta property="article:modified_time" content="2021-11-30T11:07:23.301Z">
<meta property="article:author" content="xydaytoy">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2021/11/28/chinese-text-classify/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>chinese_text_classify | 札记</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="札记" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">札记</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>摘记</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/28/chinese-text-classify/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="xydaytoy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="札记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          chinese_text_classify
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-28 13:16:23" itemprop="dateCreated datePublished" datetime="2021-11-28T13:16:23+08:00">2021-11-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-11-30 19:07:23" itemprop="dateModified" datetime="2021-11-30T19:07:23+08:00">2021-11-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/research/" itemprop="url" rel="index"><span itemprop="name">research</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/research/text-classify/" itemprop="url" rel="index"><span itemprop="name">text_classify</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>中文文本分类流程</p>
</blockquote>
<span id="more"></span>

<h1 id="1-相关工具及目录结构"><a href="#1-相关工具及目录结构" class="headerlink" title="1 相关工具及目录结构"></a>1 相关工具及目录结构</h1><h2 id="1-1-相关工具"><a href="#1-1-相关工具" class="headerlink" title="1.1 相关工具"></a>1.1 相关工具</h2><p>除<strong>jieba</strong>是使用<code>pip install</code>安装外，其他几个工具都是建议直接克隆库到自己的用户目录中，方便使用其脚本(<strong>moses</strong>/<strong>subword-nmt</strong>)，或未来可能要自己拓展其中的模型(<strong>fairseq</strong>)</p>
<ol>
<li><p>Moses (一个SMT工具，在这里只会用到一些预处理脚本，如：tokenisation,  truecasing, cleaning)，安装指令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;moses-smt&#x2F;mosesdecoder.git</span><br></pre></td></tr></table></figure></li>
<li><p>subword-nmt (使用BPE算法生成子词的预处理脚本)，安装指令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;rsennrich&#x2F;subword-nmt.git</span><br></pre></td></tr></table></figure></li>
<li><p>jieba (中文分词组件)，安装指令如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure></li>
<li><p>fairseq (一个基于PyTorch的序列建模工具)，安装指令如下：</p>
<p>fairseq安装参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/194176917">https://zhuanlan.zhihu.com/p/194176917</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;fairseq</span><br><span class="line">cd fairseq</span><br><span class="line">pip install --editable .&#x2F;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="1-2-目录结构与初始化"><a href="#1-2-目录结构与初始化" class="headerlink" title="1.2 目录结构与初始化"></a>1.2 目录结构与初始化</h2><h3 id="1-2-1-目录结构"><a href="#1-2-1-目录结构" class="headerlink" title="1.2.1 目录结构"></a>1.2.1 目录结构</h3><p>提前组织一个目录结构的好处是可以让后面的一系列操作更加统一、规范化。下表中<code>~</code>代表linux系统中<strong>我的用户目录</strong>, THUCNews目录名代表此次我使用的数据集名称</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">~</span><br><span class="line">├── subword-nmt</span><br><span class="line">├── fairseq</span><br><span class="line">└── text-classify</span><br><span class="line">    ├──bert_pretrain</span><br><span class="line">        └── data</span><br><span class="line">    	├── train.txt       # 用于存放翻译结果</span><br><span class="line">            └── d-bin        # 用于存放二进制文件</span><br><span class="line">    ├──models</span><br><span class="line">        └── bert.py</span><br><span class="line">    	├── bert.py       # 用于存放翻译结果</span><br><span class="line">        └── bert.py        # 用于存放二进制文件</span><br><span class="line">    ├──THUCNews</span><br><span class="line">    	└── data</span><br><span class="line">    	    ├── train.txt       # 用于存放翻译结果</span><br><span class="line">    	    ├── dev.txt       # 用于存放翻译结果</span><br><span class="line">    	    ├── test.txt       # 用于存放翻译结果</span><br><span class="line">            └── class.txt        # 用于存放二进制文件</span><br><span class="line">    ├──run.py</span><br><span class="line">    ├──train_eval.py</span><br><span class="line">    ├──utils.py</span><br><span class="line">    ├──bert_pretrain</span><br><span class="line">        └── v15news</span><br><span class="line">            ├── result          # 用于存放翻译结果</span><br><span class="line">            └── data-bin        # 用于存放二进制文件</span><br><span class="line">    ├── models                  # 用于保存过程中的model文件和checkpoint</span><br><span class="line">        └── v15news</span><br><span class="line">            └── checkpoints     # 保存checkpoints</span><br><span class="line">    ├── utils                   # 一些其他工具</span><br><span class="line">        ├── split.py            # 用于划分train,valid,test</span><br><span class="line">        └── cut2.py             # 用于划分src,tgt</span><br><span class="line">    └── scripts                 # 一些脚本</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">~</span><br><span class="line">├── mosesdecoder</span><br><span class="line">├── subword-nmt</span><br><span class="line">├── fairseq</span><br><span class="line">└── nmt</span><br><span class="line">    ├── data</span><br><span class="line">        └── v15news</span><br><span class="line">            ├── result          # 用于存放翻译结果</span><br><span class="line">            └── data-bin        # 用于存放二进制文件</span><br><span class="line">    ├── models                  # 用于保存过程中的model文件和checkpoint</span><br><span class="line">        └── v15news</span><br><span class="line">            └── checkpoints     # 保存checkpoints</span><br><span class="line">    ├── utils                   # 一些其他工具</span><br><span class="line">        ├── split.py            # 用于划分train,valid,test</span><br><span class="line">        └── cut2.py             # 用于划分src,tgt</span><br><span class="line">    └── scripts                 # 一些脚本</span><br></pre></td></tr></table></figure>

<h3 id="1-2-2-用于初始化的bash文件"><a href="#1-2-2-用于初始化的bash文件" class="headerlink" title="1.2.2 用于初始化的bash文件"></a>1.2.2 用于初始化的bash文件</h3><p>这个文件是在上述目录结构的基础下，定义了一些后面需要用到的变量(主要是<strong>各种脚本的路径</strong>)，包括tokenizer.perl, truecase.perl等，可以在linux中使用bash xx.sh运行，也可以把这些内容直接全部复制到linux命令行中按回车</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line"></span><br><span class="line">src&#x3D;zh</span><br><span class="line">tgt&#x3D;en</span><br><span class="line"></span><br><span class="line">SCRIPTS&#x3D;~&#x2F;mosesdecoder&#x2F;scripts</span><br><span class="line">TOKENIZER&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;tokenizer.perl</span><br><span class="line">DETOKENIZER&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;detokenizer.perl</span><br><span class="line">LC&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;lowercase.perl</span><br><span class="line">TRAIN_TC&#x3D;$&#123;SCRIPTS&#125;&#x2F;recaser&#x2F;train-truecaser.perl</span><br><span class="line">TC&#x3D;$&#123;SCRIPTS&#125;&#x2F;recaser&#x2F;truecase.perl</span><br><span class="line">DETC&#x3D;$&#123;SCRIPTS&#125;&#x2F;recaser&#x2F;detruecase.perl</span><br><span class="line">NORM_PUNC&#x3D;$&#123;SCRIPTS&#125;&#x2F;tokenizer&#x2F;normalize-punctuation.perl</span><br><span class="line">CLEAN&#x3D;$&#123;SCRIPTS&#125;&#x2F;training&#x2F;clean-corpus-n.perl</span><br><span class="line">BPEROOT&#x3D;~&#x2F;subword-nmt&#x2F;subword_nmt</span><br><span class="line">MULTI_BLEU&#x3D;$&#123;SCRIPTS&#125;&#x2F;generic&#x2F;multi-bleu.perl</span><br><span class="line">MTEVAL_V14&#x3D;$&#123;SCRIPTS&#125;&#x2F;generic&#x2F;mteval-v14.pl</span><br><span class="line"></span><br><span class="line">data_dir&#x3D;~&#x2F;nmt&#x2F;data&#x2F;v15news</span><br><span class="line">model_dir&#x3D;~&#x2F;nmt&#x2F;models&#x2F;v15news</span><br><span class="line">utils&#x3D;~&#x2F;nmt&#x2F;utils</span><br></pre></td></tr></table></figure>

<h1 id="2-数据的准备"><a href="#2-数据的准备" class="headerlink" title="2 数据的准备"></a>2 数据的准备</h1><h2 id="2-1-THUCNews"><a href="#2-1-THUCNews" class="headerlink" title="2.1 THUCNews"></a>2.1 THUCNews</h2><p>本文采用 <a target="_blank" rel="noopener" href="http://thuctc.thunlp.org/">THUCNews中文文本开源语料</a> ，THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。共分为14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。</p>
<p><strong>原始数据统计：</strong></p>
<p>总数量：836075</p>
<p>0-科技：162929</p>
<p>1-股票：154398</p>
<p>2-体育：131604</p>
<p>3-娱乐：92632</p>
<p>4-时政：63086</p>
<p>5-社会：50849</p>
<p>6-教育：41936</p>
<p>7-财经：37098</p>
<p>8-家居：32586</p>
<p>9-游戏：24373</p>
<p>房产：20050</p>
<p>时尚：13368</p>
<p>彩票：7588</p>
<p>星座：3578</p>
<h2 id="2-2-数据预处理"><a href="#2-2-数据预处理" class="headerlink" title="2.2 数据预处理"></a>2.2 数据预处理</h2><h3 id="2-2-1-数据格式"><a href="#2-2-1-数据格式" class="headerlink" title="2.2.1 数据格式"></a>2.2.1 数据格式</h3><p>在本篇博客中，使用THUCNews中文新闻分类语料，放于以下位置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">└── text-classify</span><br><span class="line">    ├── dataset</span><br><span class="line">        └── 体育     </span><br><span class="line">            └── 29016.txt</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>格式如下：由标题和正文两部分组成</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">组图：9月全球折扣明星带你去血拼</span><br><span class="line">　　导读：每年的8，9两月，都是普天下血拼狂人大开杀戒的日子。因为全球性的折扣季节已正式启动。以折扣率最疯狂的美国来说，无论一线大牌还是时尚潮物，统统为5 折为起点，一路狂飙到2折左右。去年8月，Sami就曾以0.5折的夸张扣率抢到一条MiuMiu连衣裙。此番Sami特别请到20位美国本土女星，她们让现身说法，带你掏遍折扣季节最值得入货的高性价比超值单品。</span><br><span class="line">　　Heidi Montag推荐超值折扣单品：GUCCI凉鞋</span><br><span class="line">　　Ashley Tisdale推荐超值折扣单品：破洞牛仔小脚裤</span><br></pre></td></tr></table></figure>

<h3 id="2-2-2-数据集生成"><a href="#2-2-2-数据集生成" class="headerlink" title="2.2.2 数据集生成"></a>2.2.2 数据集生成</h3><p>首先，从原始数据集中随机抽取部分数据，选择数量较多的前十个类别的数据，随机抽取20000条数据作为训练集，2000条数据作为验证集，2000条数据作为测试集，使用data/split_data.py划分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python -u split_data.py &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>效果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">category,type,file</span><br><span class="line">0,train,&#x2F;data&#x2F;yyfxu&#x2F;text-classify&#x2F;THUCNews&#x2F;科技&#x2F;555633.txt</span><br><span class="line">0,train,&#x2F;data&#x2F;yyfxu&#x2F;text-classify&#x2F;THUCNews&#x2F;科技&#x2F;637231.txt</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="2-2-3-tokenize"><a href="#2-2-3-tokenize" class="headerlink" title="2.2.3 tokenize"></a>2.2.3 tokenize</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python -u tokenize_data.py &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python $&#123;utils&#125;&#x2F;cut2.py $&#123;data_dir&#125;&#x2F;news-commentary-v15.en-zh.tsv $&#123;data_dir&#125;&#x2F;</span><br></pre></td></tr></table></figure>

<p><strong>后注：</strong> 在linux里可以直接用cut实现 <code>cut -f 1 fpath &gt; new_data_dir/raw.en | cut -f 2 fpath &gt; new_data_dir/raw.zh</code></p>
<p>切分后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ├── news-commentary-v15.en-zh.tsv</span><br><span class="line">        ├── raw.zh</span><br><span class="line">        └── raw.en</span><br></pre></td></tr></table></figure>

<h3 id="2-2-3-normalize-punctuation-可选"><a href="#2-2-3-normalize-punctuation-可选" class="headerlink" title="2.2.3 normalize-punctuation(可选)"></a>2.2.3 normalize-punctuation(可选)</h3><p>标点符号的标准化，同时对双语文件(raw.en, raw.zh)处理，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">perl $&#123;NORM_PUNC&#125; -l en &lt; $&#123;data_dir&#125;&#x2F;raw.en &gt; $&#123;data_dir&#125;&#x2F;norm.en</span><br><span class="line">perl $&#123;NORM_PUNC&#125; -l zh &lt; $&#123;data_dir&#125;&#x2F;raw.zh &gt; $&#123;data_dir&#125;&#x2F;norm.zh</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ...</span><br><span class="line">        ├── norm.zh</span><br><span class="line">        └── norm.en</span><br></pre></td></tr></table></figure>

<p>效果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># raw.en</span><br><span class="line">“We can’t waste time,” he says.</span><br><span class="line">Yet, according to the political economist Moeletsi Mbeki, at his core, “Zuma is a conservative.”</span><br><span class="line"></span><br><span class="line"># norm.en</span><br><span class="line">&quot;We can&#39;t waste time,&quot; he says.</span><br><span class="line">Yet, according to the political economist Moeletsi Mbeki, at his core, &quot;Zuma is a conservative.&quot;</span><br></pre></td></tr></table></figure>

<h3 id="2-2-4-中文分词"><a href="#2-2-4-中文分词" class="headerlink" title="2.2.4 中文分词"></a>2.2.4 中文分词</h3><p>对标点符号标准化后的中文文件(norm.zh)进行分词处理，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m jieba -d &quot; &quot; $&#123;data_dir&#125;&#x2F;norm.zh &gt; $&#123;data_dir&#125;&#x2F;norm.seg.zh</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ...</span><br><span class="line">        └── norm.seg.zh</span><br></pre></td></tr></table></figure>

<p>效果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># norm.zh</span><br><span class="line">1929年还是1989年?</span><br><span class="line">巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。</span><br><span class="line">一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。</span><br><span class="line"></span><br><span class="line"># norm.seg.zh</span><br><span class="line">1929 年 还是 1989 年 ?</span><br><span class="line">巴黎 - 随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 。</span><br><span class="line">一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 情况 ， 这样 得 类比 是 令人 宽心 的 ， 因为 这 两段 时期 意味着 典型 的 周期性 衰退 。</span><br></pre></td></tr></table></figure>

<h3 id="2-2-5-tokenize"><a href="#2-2-5-tokenize" class="headerlink" title="2.2.5 tokenize"></a>2.2.5 tokenize</h3><p>对上述处理后的双语文件(norm.en, norm.seg.zh)进行标记化处理，有很多功能(1.将<strong>英文单词</strong>与<strong>标点符号</strong>用空格分开 2.将多个连续空格简化为一个空格 3.将很多符号替换成转义字符，如：把<code>&quot;</code>替换成<code>&quot;</code>、把<code>can&#39;t</code>替换成<code>can &#39;t</code>)，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$&#123;TOKENIZER&#125; -l en &lt; $&#123;data_dir&#125;&#x2F;norm.en &gt; $&#123;data_dir&#125;&#x2F;norm.tok.en</span><br><span class="line">$&#123;TOKENIZER&#125; -l zh &lt; $&#123;data_dir&#125;&#x2F;norm.seg.zh &gt; $&#123;data_dir&#125;&#x2F;norm.seg.tok.zh</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news     </span><br><span class="line">        ...</span><br><span class="line">        ├── norm.tok.en</span><br><span class="line">        └── norm.seg.tok.zh</span><br></pre></td></tr></table></figure>

<p>效果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># norm.seg.zh</span><br><span class="line">目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲   ）   ，   要么 是 努力 的 扩展 （ 美国   ）   。</span><br><span class="line">而 历史 是 不 公平 的 。   尽管 美国 要 为 当今 的 全球 危机 负 更 大 的 责任 ， 但 美国 可能 会 比 大多数 国家 以 更 良好 的 势态 走出 困境 。</span><br><span class="line"></span><br><span class="line"># norm.seg.tok.zh</span><br><span class="line">目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲 ） ， 要么 是 努力 的 扩展 （ 美国 ） 。</span><br><span class="line">而 历史 是 不 公平 的 。 尽管 美国 要 为 当今 的 全球 危机 负 更 大 的 责任 ， 但 美国 可能 会 比 大多数 国家 以 更 良好 的 势态 走出 困境 。</span><br><span class="line"></span><br><span class="line"># norm.en</span><br><span class="line">&quot;We can&#39;t waste time,&quot; he says.</span><br><span class="line">Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.</span><br><span class="line">Second, Zoellick should ask why the Bank spends only 2.5% of its budget on the &quot;knowledge bank&quot; research function that it trumpets so proudly in its external relations materials, while it spends three times that amount on maintaining its executive board.</span><br><span class="line"></span><br><span class="line"># norm.tok.en</span><br><span class="line">&quot; We can &amp;apos;t waste time , &quot; he says .</span><br><span class="line">Of course , the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall .</span><br><span class="line">Second , Zoellick should ask why the Bank spends only 2.5 % of its budget on the &quot; knowledge bank &quot; research function that it trumpets so proudly in its external relations materials , while it spends three times that amount on maintaining its executive board .</span><br></pre></td></tr></table></figure>

<h3 id="2-2-6-truecase"><a href="#2-2-6-truecase" class="headerlink" title="2.2.6 truecase"></a>2.2.6 truecase</h3><p>对上述处理后的英文文件(norm.tok.en)进行大小写转换处理(对于句中的每个英文单词，尤其是<strong>句首单词</strong>，在数据中<strong>学习</strong>最适合它们的大小写形式)，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$&#123;TRAIN_TC&#125; --model $&#123;model_dir&#125;&#x2F;truecase-model.en --corpus $&#123;data_dir&#125;&#x2F;norm.tok.en</span><br><span class="line">$&#123;TC&#125; --model $&#123;model_dir&#125;&#x2F;truecase-model.en &lt; $&#123;data_dir&#125;&#x2F;norm.tok.en &gt; $&#123;data_dir&#125;&#x2F;norm.tok.true.en</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        └── norm.tok.true.en</span><br><span class="line">├── models</span><br><span class="line">    └── v15news</span><br><span class="line">        └── truecase-model.en</span><br></pre></td></tr></table></figure>

<p>效果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># norm.tok.en</span><br><span class="line">PARIS - As the economic crisis deepens and widens , the world has been searching for historical analogies to help us understand what has been happening .</span><br><span class="line">At the start of the crisis , many people likened it to 1982 or 1973 , which was reassuring , because both dates refer to classical cyclical downturns .</span><br><span class="line">When the TTIP was first proposed , Europe seemed to recognize its value .</span><br><span class="line">Europe is being cautious in the name of avoiding debt and defending the euro , whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms .</span><br><span class="line"></span><br><span class="line"># norm.tok.true.en</span><br><span class="line">Paris - As the economic crisis deepens and widens , the world has been searching for historical analogies to help us understand what has been happening .</span><br><span class="line">at the start of the crisis , many people likened it to 1982 or 1973 , which was reassuring , because both dates refer to classical cyclical downturns .</span><br><span class="line">when the TTIP was first proposed , Europe seemed to recognize its value .</span><br><span class="line">Europe is being cautious in the name of avoiding debt and defending the euro , whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms .</span><br></pre></td></tr></table></figure>

<h3 id="2-2-7-bpe"><a href="#2-2-7-bpe" class="headerlink" title="2.2.7 bpe"></a>2.2.7 bpe</h3><p>对上述处理后的双语文件(norm.tok.true.en, norm.seg.tok.zh)进行子词处理(可以理解为更细粒度的分词)，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python $&#123;BPEROOT&#125;&#x2F;learn_joint_bpe_and_vocab.py --input $&#123;data_dir&#125;&#x2F;norm.tok.true.en  -s 32000 -o $&#123;model_dir&#125;&#x2F;bpecode.en --write-vocabulary $&#123;model_dir&#125;&#x2F;voc.en</span><br><span class="line">python $&#123;BPEROOT&#125;&#x2F;apply_bpe.py -c $&#123;model_dir&#125;&#x2F;bpecode.en --vocabulary $&#123;model_dir&#125;&#x2F;voc.en &lt; $&#123;data_dir&#125;&#x2F;norm.tok.true.en &gt; $&#123;data_dir&#125;&#x2F;norm.tok.true.bpe.en</span><br><span class="line"></span><br><span class="line">python $&#123;BPEROOT&#125;&#x2F;learn_joint_bpe_and_vocab.py --input $&#123;data_dir&#125;&#x2F;norm.seg.tok.zh  -s 32000 -o $&#123;model_dir&#125;&#x2F;bpecode.zh --write-vocabulary $&#123;model_dir&#125;&#x2F;voc.zh</span><br><span class="line">python $&#123;BPEROOT&#125;&#x2F;apply_bpe.py -c $&#123;model_dir&#125;&#x2F;bpecode.zh --vocabulary $&#123;model_dir&#125;&#x2F;voc.zh &lt; $&#123;data_dir&#125;&#x2F;norm.seg.tok.zh &gt; $&#123;data_dir&#125;&#x2F;norm.seg.tok.bpe.zh</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── norm.seg.tok.bpe.zh</span><br><span class="line">        └── norm.tok.true.bpe.en</span><br><span class="line">├── models</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── voc.zh</span><br><span class="line">        ├── voc.en</span><br><span class="line">        ├── bpecode.zh</span><br><span class="line">        └── bpecode.en</span><br></pre></td></tr></table></figure>

<p>效果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># norm.seg.tok.zh</span><br><span class="line">从 一流 的 麻省理工学院 的 媒体 实验室 到 哈佛大学 的 数学 和 经济系 ， 亚洲 人 - 尤其 是 中国 和 印度人 - 到处 都 是 ， 犹如 公元前 一 世纪 在 雅典 的 罗马 人 一样 ： 他们 对 那里 学到 太 多 东西 的 人们 充满 了 敬佩 ， 而 他们 将 在 今后 几十年 打败 他们 学习 的 对象 。</span><br><span class="line">这 不仅 加大 了 预防 危机 的 难度 - - 尤其 因为 它 为 参与者 提供 了 钻空子 和 逃避责任 的 机会 - - 还 使得 人们 越来越 难以 采取措施 来 应对 危机 。</span><br><span class="line">它们 将 通胀 目标 设定 在 2 % 左右 - - 这 意味着 当 波涛汹涌 时 他们 根本 没有 多少 施展 空间 。</span><br><span class="line"></span><br><span class="line"># norm.seg.tok.bpe.zh</span><br><span class="line">从 一流 的 麻省理工学院 的 媒体 实验室 到 哈佛大学 的 数学 和 经济@@ 系 ， 亚洲 人 - 尤其 是 中国 和 印度人 - 到处 都 是 ， 犹如 公元前 一 世纪 在 雅典 的 罗马 人 一样 ： 他们 对 那里 学到 太 多 东西 的 人们 充满 了 敬佩 ， 而 他们 将 在 今后 几十年 打败 他们 学习 的 对象 。</span><br><span class="line">这 不仅 加大 了 预防 危机 的 难度 - - 尤其 因为 它 为 参与者 提供 了 钻@@ 空子 和 逃避@@ 责任 的 机会 - - 还 使得 人们 越来越 难以 采取措施 来 应对 危机 。</span><br><span class="line">它们 将 通胀 目标 设定 在 2 % 左右 - - 这 意味着 当 波@@ 涛@@ 汹涌 时 他们 根本 没有 多少 施展 空间 。</span><br><span class="line"></span><br><span class="line"># norm.tok.true.en</span><br><span class="line">indeed , on the surface it seems to be its perfect antithesis : the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism .</span><br><span class="line">as a visiting professor at Harvard and MIT , I am getting a good preview of what the world could look like when the crisis finally passes .</span><br><span class="line">one senses something like the making of an American-Asian dominated universe .</span><br><span class="line"></span><br><span class="line"># norm.tok.true.bpe.en</span><br><span class="line">indeed , on the surface it seems to be its perfect anti@@ thesis : the collapse of a wall symboli@@ zing oppression and artificial divisions versus the collapse of a seemingly inde@@ struc@@ tible and reassuring institution of financial capitalism .</span><br><span class="line">as a visiting professor at Harvard and MIT , I am getting a good pre@@ view of what the world could look like when the crisis finally passes .</span><br><span class="line">one senses something like the making of an American-@@ Asian dominated universe .</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>后注：</strong><br>需要注意的是，我为了方便，步骤上失去了一些正确性。正确的做法应该是在<strong>训练集</strong>中学习bpe模型，再将bpe模型应用到<strong>测试集</strong>和<strong>验证集</strong>中。而我是直接在全部数据中学bpe模型了。</p>
</blockquote>
<h3 id="2-2-8-clean"><a href="#2-2-8-clean" class="headerlink" title="2.2.8 clean"></a>2.2.8 clean</h3><p>对上述处理后的双语文件(norm.tok.true.bpe.en, norm.seg.tok.bpe.zh)进行过滤(可以过滤<strong>最小长度</strong>和<strong>最大长度</strong>之间的句对，这样能够有效过滤空白行。还可以过滤<strong>长度比</strong>不合理的句对)，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mv $&#123;data_dir&#125;&#x2F;norm.seg.tok.bpe.zh $&#123;data_dir&#125;&#x2F;toclean.zh</span><br><span class="line">mv $&#123;data_dir&#125;&#x2F;norm.tok.true.bpe.en $&#123;data_dir&#125;&#x2F;toclean.en </span><br><span class="line">$&#123;CLEAN&#125; $&#123;data_dir&#125;&#x2F;toclean zh en $&#123;data_dir&#125;&#x2F;clean 1 256</span><br></pre></td></tr></table></figure>

<p>处理后的文件在目录中如下格式存放：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── clean.zh</span><br><span class="line">        └── clean.en</span><br></pre></td></tr></table></figure>

<p>效果如下(每行最开始标出了<strong>行号</strong>):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># norm.tok.true.bpe.en</span><br><span class="line">30 we can only hope that , in the end , the consequences of 2009 similarly prove to be far less dramatic than we now - intuitively and in our historical refle@@ xes - feel them to be .</span><br><span class="line">31</span><br><span class="line">32 one Hund@@ red Years of Ine@@ p@@ titude</span><br><span class="line"></span><br><span class="line"># clean.en</span><br><span class="line">30 we can only hope that , in the end , the consequences of 2009 similarly prove to be far less dramatic than we now - intuitively and in our historical refle@@ xes - feel them to be .</span><br><span class="line">31 one Hund@@ red Years of Ine@@ p@@ titude</span><br><span class="line">32 Berlin - The global financial and economic crisis that began in 2008 was the greatest economic stre@@ ss-@@ test since the Great Depression , and the greatest challenge to social and political systems since World War II .</span><br><span class="line"></span><br><span class="line"># norm.seg.tok.bpe.zh</span><br><span class="line">30 我们 只能 希望 2009 年 的 危机 同样 地 最后 被 证明 是 远远 低于 我们 现在 以 直觉 和 历史 回顾 的 方式 � � 感觉 到 的 那么 剧烈 。</span><br><span class="line">31 </span><br><span class="line">32 百年 愚@@ 顽</span><br><span class="line"></span><br><span class="line"># clean.zh</span><br><span class="line">30 我们 只能 希望 2009 年 的 危机 同样 地 最后 被 证明 是 远远 低于 我们 现在 以 直觉 和 历史 回顾 的 方式 � � 感觉 到 的 那么 剧烈 。</span><br><span class="line">31 百年 愚@@ 顽</span><br><span class="line">32 柏林 - - 2008 年 爆发 的 全球 金融 和 经济危机 是 自大 萧条 以来 最 严峻 的 一次 经济 压力 测试 ， 也 是 自 二战 以来 社会 和 政治 制度 所 面临 的 最 严重 挑战 。</span><br></pre></td></tr></table></figure>

<h3 id="2-2-9-split"><a href="#2-2-9-split" class="headerlink" title="2.2.9 split"></a>2.2.9 split</h3><p>最后，双语文件(clean.zh, clean.en)都需要按比例划分出训练集、测试集、开发集(所以共6个文件，为方便区分，直接以 ‘train.en’, ‘valid.zh’ 这样的格式命名)，附自己写的脚本(~/nmt/utils/split.py)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">Usage:</span><br><span class="line">python split.py src_fpath tgt_fpath new_data_dir</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">def split(src_fpath, tgt_fpath, nsrc&#x3D;&#39;zh&#39;, ntgt&#x3D;&#39;en&#39;, ratio&#x3D;(0.9, 0.05, 0.05), new_data_dir&#x3D;&#39;&#39;):</span><br><span class="line">  src_fp &#x3D; open(src_fpath, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  tgt_fp &#x3D; open(tgt_fpath, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  </span><br><span class="line">  src_train, src_test, src_val &#x3D; open(new_data_dir + &#39;train.&#39; + nsrc, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), \</span><br><span class="line">    open(new_data_dir + &#39;test.&#39; + nsrc, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), open(new_data_dir + &#39;valid.&#39; + nsrc, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  tgt_train, tgt_test, tgt_val &#x3D; open(new_data_dir + &#39;train.&#39; + ntgt, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), \</span><br><span class="line">    open(new_data_dir + &#39;test.&#39; + ntgt, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;), open(new_data_dir + &#39;valid.&#39; + ntgt, &#39;w&#39;, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">  </span><br><span class="line">  src, tgt &#x3D; src_fp.readlines(), tgt_fp.readlines()</span><br><span class="line">  for s, t in zip(src, tgt):</span><br><span class="line">      rand &#x3D; random.random()</span><br><span class="line">      if 0 &lt; rand &lt;&#x3D; ratio[0]:</span><br><span class="line">        src_train.write(s)</span><br><span class="line">        tgt_train.write(t)</span><br><span class="line">      elif ratio[0] &lt; rand &lt;&#x3D; ratio[0] + ratio[1]:</span><br><span class="line">        src_test.write(s)</span><br><span class="line">        tgt_test.write(t)</span><br><span class="line">      else:</span><br><span class="line">        src_val.write(s)</span><br><span class="line">        tgt_val.write(t)</span><br><span class="line">  </span><br><span class="line">  src_fp.close()</span><br><span class="line">  tgt_fp.close()</span><br><span class="line">  src_train.close()</span><br><span class="line">  src_test.close()</span><br><span class="line">  src_val.close()</span><br><span class="line">  tgt_train.close()</span><br><span class="line">  tgt_test.close()</span><br><span class="line">  tgt_val.close()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:      </span><br><span class="line">    split(src_fpath&#x3D;sys.argv[1], tgt_fpath&#x3D;sys.argv[2], nsrc&#x3D;&#39;zh&#39;, ntgt&#x3D;&#39;en&#39;, ratio&#x3D;(0.95, 0.025, 0.025), new_data_dir&#x3D;sys.argv[3])</span><br></pre></td></tr></table></figure>

<p>使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python $&#123;utils&#125;&#x2F;split.py $&#123;data_dir&#125;&#x2F;clean.zh $&#123;data_dir&#125;&#x2F;clean.en $&#123;data_dir&#125;&#x2F;</span><br></pre></td></tr></table></figure>

<p>最后，data/v15news目录中有如下数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        ├── test.en</span><br><span class="line">        ├── test.zh</span><br><span class="line">        ├── train.en</span><br><span class="line">        ├── train.zh</span><br><span class="line">        ├── valid.en</span><br><span class="line">        └── valid.zh</span><br></pre></td></tr></table></figure>

<h1 id="3-训练过程"><a href="#3-训练过程" class="headerlink" title="3 训练过程"></a>3 训练过程</h1><h2 id="3-1-生成词表及二进制文件"><a href="#3-1-生成词表及二进制文件" class="headerlink" title="3.1 生成词表及二进制文件"></a>3.1 生成词表及二进制文件</h2><p>首先用预处理后的六个文件(train.zh, valid.en等)，使用<code>fairseq-preprocess</code>命令生成<strong>词表</strong>和<strong>训练用的二进制文件</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fairseq-preprocess --source-lang $&#123;src&#125; --target-lang $&#123;tgt&#125; \</span><br><span class="line">    --trainpref $&#123;data_dir&#125;&#x2F;train --validpref $&#123;data_dir&#125;&#x2F;valid --testpref $&#123;data_dir&#125;&#x2F;test \</span><br><span class="line">    --destdir $&#123;data_dir&#125;&#x2F;data-bin</span><br></pre></td></tr></table></figure>

<p>生成的文件都保存在data-bin目录中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        └── data-bin</span><br><span class="line">            ├── dict.zh</span><br><span class="line">            ├── dict.en</span><br><span class="line">            ├── preprocess.log</span><br><span class="line">            ├── train.zh-en.zh.idx</span><br><span class="line">            ...</span><br><span class="line">            └── valid.zh-en.en.bin</span><br></pre></td></tr></table></figure>

<p>需要提醒的是：训练阶段使用的是<strong>训练集</strong>和<strong>验证集</strong>，解码阶段使用的是<strong>测试集</strong></p>
<h2 id="3-2-训练"><a href="#3-2-训练" class="headerlink" title="3.2 训练"></a>3.2 训练</h2><p>使用<code>fairseq-train</code>命令进行训练，其中有很多可以自由设置的超参数，比如选择使用什么模型，模型的参数等。其中，<code>--save-dir</code> 这个参数是指每一个epoch结束后模型保存的位置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0,1,2,3 nohup fairseq-train $&#123;data_dir&#125;&#x2F;data-bin --arch transformer \</span><br><span class="line">	--source-lang $&#123;src&#125; --target-lang $&#123;tgt&#125;  \</span><br><span class="line">    --optimizer adam  --lr 0.001 --adam-betas &#39;(0.9, 0.98)&#39; \</span><br><span class="line">    --lr-scheduler inverse_sqrt --max-tokens 4096  --dropout 0.3 \</span><br><span class="line">    --criterion label_smoothed_cross_entropy  --label-smoothing 0.1 \</span><br><span class="line">    --max-update 200000  --warmup-updates 4000 --warmup-init-lr &#39;1e-07&#39; \</span><br><span class="line">    --keep-last-epochs 10 --num-workers 8 \</span><br><span class="line">	--save-dir $&#123;model_dir&#125;&#x2F;checkpoints &amp;</span><br></pre></td></tr></table></figure>

<p>我自己训练时是在3块GTX TITAN X卡上跑了6个小时，共跑了49个epoch，但是在第22个epoch的时候已经收敛(只需要看验证集上的ppl的变化即可)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoch 020 | valid on &#39;valid&#39; subset | loss 4.366 | nll_loss 2.652 | ppl 6.29 | wps 50387.3 | wpb 8026 | bsz 299.8 | num_updates 14400 | best_loss 4.366</span><br><span class="line">epoch 021 | valid on &#39;valid&#39; subset | loss 4.36 | nll_loss 2.647 | ppl 6.27 | wps 51992.7 | wpb 8026 | bsz 299.8 | num_updates 15120 | best_loss 4.36</span><br><span class="line">epoch 022 | valid on &#39;valid&#39; subset | loss 4.361 | nll_loss 2.644 | ppl 6.25 | wps 49009.9 | wpb 8026 | bsz 299.8 | num_updates 15840 | best_loss 4.36</span><br><span class="line">epoch 023 | valid on &#39;valid&#39; subset | loss 4.369 | nll_loss 2.65 | ppl 6.28 | wps 51878.9 | wpb 8026 | bsz 299.8 | num_updates 16560 | best_loss 4.36</span><br><span class="line">epoch 023 | valid on &#39;valid&#39; subset | loss 4.369 | nll_loss 2.65 | ppl 6.28 | wps 51878.9 | wpb 8026 | bsz 299.8 | num_updates 16560 | best_loss 4.36</span><br></pre></td></tr></table></figure>

<p>由于<code>--keep-last-epochs</code>这个参数我设为10，所以我最后10个epoch的模型都保存在以下目录中。此外，还会额外保存效果最好的模型(即第22个epoch)和最后一个模型(即第49个epoch，可以用于下一次训练)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">├── models</span><br><span class="line">    └── v15news</span><br><span class="line">        ...</span><br><span class="line">        └── checkpoints</span><br><span class="line">            ├── checkpoint40.pt</span><br><span class="line">            ...</span><br><span class="line">            ├── checkpoint49.pt</span><br><span class="line">            ├── checkpoint_best.pt</span><br><span class="line">            └── checkpoint_last.pt</span><br></pre></td></tr></table></figure>

<h2 id="3-3-解码"><a href="#3-3-解码" class="headerlink" title="3.3 解码"></a>3.3 解码</h2><p>fairseq中支持两种解码命令<code>generate</code>和<code>interactive</code>。</p>
<p>其区别很简单，<code>generate</code>使用<strong>二进制文件</strong>，这个二进制文件是在<code>fairseq-preprocess</code>过程生成的，当时提供了一个<code>testpref</code>参数。也就是说测试集的src和tgt都是已获得的，这种场景符合自己在公开的数据集上做实验（如WMT14en-de），需要在论文中报告测试集结果。</p>
<p>而<code>interactive</code>用于文本文件，也就是说不需要二进制文件，在<code>fairseq-preprocess</code>中也就不需要提供<code>testpref</code>参数。这种场景符合在比赛中，比赛方只提供测试集中的src部分，需要自己来解码得到tgt，并最终提交。</p>
<h3 id="3-3-1-生成式解码"><a href="#3-3-1-生成式解码" class="headerlink" title="3.3.1 生成式解码"></a>3.3.1 生成式解码</h3><p>使用<code>fairseq-generate</code>命令进行生成式解码(<strong>用于预处理后的二进制文件</strong>)，可以自行选择是否添加<code>--remove-bpe</code>参数，使得在生成时就去掉bpe符号(@@)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fairseq-generate $&#123;data_dir&#125;&#x2F;data-bin \</span><br><span class="line">    --path $&#123;model_dir&#125;&#x2F;checkpoints&#x2F;checkpoint_best.pt \</span><br><span class="line">    --batch-size 128 --beam 8 &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt</span><br></pre></td></tr></table></figure>

<p>选取一部分结果展示如下 (<strong>S</strong>: 源句子，<strong>T</strong>: 目标句子，<strong>H/D</strong>: 预测的句子及其生成概率的log，句子质量越好，其生成概率越接近1，其log越接近0。<strong>P</strong>: 每一个词的生成概率的log。其中，H=\frac{\sum P}{n}<em>H</em>=<em>n</em>∑<em>P</em>)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">S-537	西班牙 的 人权 困境</span><br><span class="line">T-537	Spain &amp;apos;s Human-Rights Dilemma</span><br><span class="line">H-537	-0.16863664984703064	Spain &amp;apos;s Human Rights Quandary</span><br><span class="line">D-537	-0.16863664984703064	Spain &amp;apos;s Human Rights Quandary</span><br><span class="line">P-537	-0.0973 -0.1385 -0.1464 -0.0123 -0.4252 -0.4299 -0.0110 -0.0884</span><br><span class="line"></span><br><span class="line">S-5516	这是 不可 接受 的 。</span><br><span class="line">T-5516	that is unacceptable .</span><br><span class="line">H-5516	-0.35840675234794617	this is unacceptable .</span><br><span class="line">D-5516	-0.35840675234794617	this is unacceptable .</span><br><span class="line">P-5516	-0.7625 -0.5517 -0.2005 -0.1513 -0.1261</span><br><span class="line"></span><br><span class="line">S-676	与 最初 版本 的 破产法 相 比较 ， 2006 年 的 法律 是 牢牢 扎根 于 市场经济 的 。</span><br><span class="line">T-676	compared with the original bankruptcy code , the 2006 code is firmly rooted in the needs of a market economy .</span><br><span class="line">H-676	-0.624997079372406	in contrast to the original bankruptcy law , the law of 2006 was firmly rooted in the market economy .</span><br><span class="line">D-676	-0.624997079372406	in contrast to the original bankruptcy law , the law of 2006 was firmly rooted in the market economy .</span><br><span class="line">P-676	-1.4995 -0.9434 -0.1292 -0.3479 -0.9758 -0.6600 -0.9037 -0.1836 -0.4983 -1.6406 -0.3142 -0.0344 -0.1685 -1.0289 -1.0286 -0.1917 -1.5369 -0.6586 -0.1119 -0.1333 -0.1361</span><br><span class="line"></span><br><span class="line">S-432	用 缅因州 共和党 参议员 苏珊 · 柯林斯 （ Susan Collins ） 的话 说 ， 政府 关门 对 其 缅因州 阿卡迪亚 国家 公园 （ Acadia National Park ） 周边 &quot; 所有 小企业 都 造成 了 伤害 &quot; ， &quot; 这是 完全 错误 的 。 &quot; 是 她 首先 提出 了 和解 协议 纲要 并 送交 参议院 。</span><br><span class="line">T-432	in the words of Senator Susan Collins , a Republican from Maine who first put together the outlines of a deal and took it to the Senate floor , the shutdown &quot; hurt all the small businesses &quot; around Acadia National Park in her home state , &quot; and that is plain wrong . &quot;</span><br><span class="line">H-432	-0.7003933787345886	in the words of Susan Collins , a Republican senator from Maine , it would be a mistake to shut down the government &amp;apos;s &quot; all small business &quot; around the Maine National Park , where she proposed a settlement and delivered it to the Senate .</span><br><span class="line">D-432	-0.7003933787345886	in the words of Susan Collins , a Republican senator from Maine , it would be a mistake to shut down the government &amp;apos;s &quot; all small business &quot; around the Maine National Park , where she proposed a settlement and delivered it to the Senate .</span><br><span class="line">P-432	-1.2762 -0.3546 -0.0142 -0.1261 -0.0058 -0.7617 -0.1695 -0.2992 -0.0777 -0.3016 -0.4818 -0.0061 -0.0308 -0.3509 -2.5533 -1.5254 -0.2761 -1.1667 -0.6169 -0.6285 -1.2463 -0.0973 -1.4414 -0.3324 -0.2302 -0.3312 -0.6847 -1.0005 -0.1812 -2.9048 -0.3072 -1.8045 -0.0473 -0.8421 -0.4715 -0.6841 -1.1902 -1.6192 -0.3370 -2.3317 -0.3701 -0.2508 -3.0284 -0.2336 -1.1318 -0.3904 -0.1124 -0.0262 -0.2203 -0.1480</span><br></pre></td></tr></table></figure>

<h3 id="3-3-2-交互式解码"><a href="#3-3-2-交互式解码" class="headerlink" title="3.3.2 交互式解码"></a>3.3.2 交互式解码</h3><p>使用<code>fairseq-interactive</code>命令进行交互式解码(<strong>用于文本文件</strong>)。注意其<code>input</code>参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!fairseq-interactive $&#123;data_dir&#125;&#x2F;data-bin \</span><br><span class="line">    --input $&#123;data_dir&#125;&#x2F;test.zh \</span><br><span class="line">    --path $&#123;model_dir&#125;&#x2F;checkpoints&#x2F;checkpoint_best.pt \</span><br><span class="line">    --batch-size 1 --beam 8 --remove-bpe &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt</span><br></pre></td></tr></table></figure>

<h2 id="3-4-后处理及评价"><a href="#3-4-后处理及评价" class="headerlink" title="3.4 后处理及评价"></a>3.4 后处理及评价</h2><h3 id="3-4-1-抽取译文"><a href="#3-4-1-抽取译文" class="headerlink" title="3.4.1 抽取译文"></a>3.4.1 抽取译文</h3><p>由于解码生成的文件包含大量无关信息，所以需要把<strong>译文</strong>和<strong>正确答案</strong>单独抽取出来，其中predict是译文，answer是正确答案：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grep ^H $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt | cut -f3- &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.bpe.en</span><br><span class="line">grep ^T $&#123;data_dir&#125;&#x2F;result&#x2F;bestbeam8.txt | cut -f2- &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.bpe.en</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># predict.tok.true.bpe.en</span><br><span class="line">the subsidy .</span><br><span class="line">this is unacceptable .</span><br><span class="line">there is even worse .</span><br><span class="line">this must change .</span><br><span class="line"></span><br><span class="line"># answer.tok.true.bpe.en</span><br><span class="line">removal of subsidies .</span><br><span class="line">that is unacceptable .</span><br><span class="line">it gets worse .</span><br><span class="line">this must change .</span><br></pre></td></tr></table></figure>

<h3 id="3-4-1-去除bpe符号"><a href="#3-4-1-去除bpe符号" class="headerlink" title="3.4.1 去除bpe符号"></a>3.4.1 去除bpe符号</h3><p>有两种方法可以去除bpe符号，第一种是在解码时添加<code>--remove-bpe</code>参数，第二种是使用<code>sed</code>指令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -r &#39;s&#x2F;(@@ )| (@@ ?$)&#x2F;&#x2F;g&#39; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.bpe.en  &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.en</span><br><span class="line">sed -r &#39;s&#x2F;(@@ )| (@@ ?$)&#x2F;&#x2F;g&#39; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.bpe.en  &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.en</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># answer.tok.true.bpe.en</span><br><span class="line">a World of Under@@ investment</span><br><span class="line">that needs to change .</span><br><span class="line">revolts of the Righ@@ teous</span><br><span class="line">Russia &amp;apos;s Economic Imperi@@ alism</span><br><span class="line">shock and Pan@@ ic</span><br><span class="line"></span><br><span class="line"># answer.tok.true.en</span><br><span class="line">a World of Underinvestment</span><br><span class="line">that needs to change .</span><br><span class="line">revolts of the Righteous</span><br><span class="line">Russia &amp;apos;s Economic Imperialism</span><br><span class="line">shock and Panic</span><br></pre></td></tr></table></figure>

<h3 id="3-4-2-detruecase"><a href="#3-4-2-detruecase" class="headerlink" title="3.4.2 detruecase"></a>3.4.2 detruecase</h3><p>需要使用detruecase.perl将文件中的大小写恢复正常：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$&#123;DETC&#125; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.true.en &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.en</span><br><span class="line">$&#123;DETC&#125; &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.true.en &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.en</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># predict.tok.true.en</span><br><span class="line">the subsidy .</span><br><span class="line">this is unacceptable .</span><br><span class="line">there is even worse .</span><br><span class="line">this must change .</span><br><span class="line"></span><br><span class="line"># predict.tok.en</span><br><span class="line">The subsidy .</span><br><span class="line">This is unacceptable .</span><br><span class="line">There is even worse .</span><br><span class="line">This must change .</span><br></pre></td></tr></table></figure>

<h3 id="3-4-3-评价"><a href="#3-4-3-评价" class="headerlink" title="3.4.3 评价"></a>3.4.3 评价</h3><p>1.<strong>multi-bleu</strong>：在detokenize前进行评价</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$&#123;MULTI_BLEU&#125; -lc $&#123;data_dir&#125;&#x2F;result&#x2F;answer.tok.en &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.en</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BLEU &#x3D; 28.81, 61.8&#x2F;35.4&#x2F;22.8&#x2F;15.2 (BP&#x3D;0.976, ratio&#x3D;0.977, hyp_len&#x3D;187605, ref_len&#x3D;192093)</span><br><span class="line">It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.</span><br></pre></td></tr></table></figure>

<p>2.<strong>sacrebleu</strong>：在detokenize后进行评价。<a target="_blank" rel="noopener" href="https://github.com/mjpost/sacreBLEU">link</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Option 1: Pass the reference file as a positional argument to sacreBLEU</span><br><span class="line">sacrebleu ref.detok.txt -i output.detok.txt -m bleu -b -w 4</span><br><span class="line">20.7965</span><br><span class="line"></span><br><span class="line"># Option 2: Redirect the system into STDIN (Compatible with multi-bleu.perl way of doing things)</span><br><span class="line">cat output.detok.txt | sacrebleu ref.detok.txt -m bleu -b -w 4</span><br><span class="line">20.7965</span><br></pre></td></tr></table></figure>

<p>3.<strong>mteval-v14</strong>：Usage: <code>$0 -r &lt;ref_file&gt; -s &lt;src_file&gt; -t &lt;tst_file&gt;</code></p>
<h3 id="3-4-4-detokenize"><a href="#3-4-4-detokenize" class="headerlink" title="3.4.4 detokenize"></a>3.4.4 detokenize</h3><p>最后一步，是使用detokenize.perl得到纯预测文本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$&#123;DETOKENIZER&#125; -l en &lt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.tok.en &gt; $&#123;data_dir&#125;&#x2F;result&#x2F;predict.en</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># predict.tok.en</span><br><span class="line">what &amp;apos;s wrong with protectionism ?</span><br><span class="line">The &quot; establishment &quot; and counterinsurgency strategy , introduced by President Barack Obama &amp;apos;s military surge in 2010 , was intended to reverse the war .</span><br><span class="line"></span><br><span class="line"># predict.en</span><br><span class="line">What&#39;s wrong with protectionism?</span><br><span class="line">The &quot;establishment&quot; and counterinsurgency strategy, introduced by President Barack Obama&#39;s military surge in 2010, was intended to reverse the war.</span><br></pre></td></tr></table></figure>







<h1 id="bilstm-attention"><a href="#bilstm-attention" class="headerlink" title="bilstm+attention:"></a>bilstm+attention:</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:02, 70404.96it/s]</span><br><span class="line">10000it [00:00, 90716.68it/s]</span><br><span class="line">10000it [00:00, 92472.73it/s]</span><br><span class="line">Time usage: 0:00:03</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)</span><br><span class="line">  (tanh1): Tanh()</span><br><span class="line">  (tanh2): Tanh()</span><br><span class="line">  (fc1): Linear(in_features=256, out_features=64, bias=True)</span><br><span class="line">  (fc): Linear(in_features=64, out_features=10, bias=True)</span><br><span class="line"><span class="meta">)&gt;</span></span><br><span class="line"><span class="bash">start train</span></span><br><span class="line">Epoch [1/10]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc:  6.25%,  Val Loss:   2.3,  Val Acc: 10.01%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:  0.75,  Train Acc: 75.00%,  Val Loss:  0.78,  Val Acc: 73.34%,  Time: 0:00:05 *</span><br><span class="line">Iter:    200,  Train Loss:  0.76,  Train Acc: 72.66%,  Val Loss:  0.58,  Val Acc: 81.38%,  Time: 0:00:07 *</span><br><span class="line">Iter:    300,  Train Loss:   0.4,  Train Acc: 88.28%,  Val Loss:  0.53,  Val Acc: 83.07%,  Time: 0:00:10 *</span><br><span class="line">Iter:    400,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.47,  Val Acc: 84.44%,  Time: 0:00:14 *</span><br><span class="line">Iter:    500,  Train Loss:  0.44,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 85.70%,  Time: 0:00:18 *</span><br><span class="line">Iter:    600,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 86.46%,  Time: 0:00:22 *</span><br><span class="line">Iter:    700,  Train Loss:  0.41,  Train Acc: 85.94%,  Val Loss:  0.41,  Val Acc: 86.79%,  Time: 0:00:25 *</span><br><span class="line">Iter:    800,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 87.86%,  Time: 0:00:29 *</span><br><span class="line">Iter:    900,  Train Loss:  0.44,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 87.53%,  Time: 0:00:33 </span><br><span class="line">Iter:   1000,  Train Loss:  0.26,  Train Acc: 90.62%,  Val Loss:  0.37,  Val Acc: 88.04%,  Time: 0:00:36 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.29,  Train Acc: 92.19%,  Val Loss:  0.39,  Val Acc: 86.97%,  Time: 0:00:40 </span><br><span class="line">Iter:   1200,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.41%,  Time: 0:00:43 *</span><br><span class="line">Iter:   1300,  Train Loss:  0.34,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.54%,  Time: 0:00:47 </span><br><span class="line">Iter:   1400,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.44%,  Time: 0:00:50 </span><br><span class="line">Epoch [2/10]</span><br><span class="line">Iter:   1500,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 88.15%,  Time: 0:00:54 </span><br><span class="line">Iter:   1600,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 88.53%,  Time: 0:00:57 </span><br><span class="line">Iter:   1700,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 88.63%,  Time: 0:01:00 *</span><br><span class="line">Iter:   1800,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.31%,  Time: 0:01:03 *</span><br><span class="line">Iter:   1900,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.55%,  Time: 0:01:07 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 89.12%,  Time: 0:01:10 </span><br><span class="line">Iter:   2100,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 89.42%,  Time: 0:01:14 </span><br><span class="line">Iter:   2200,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 89.52%,  Time: 0:01:17 </span><br><span class="line">Iter:   2300,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.31%,  Time: 0:01:21 *</span><br><span class="line">Iter:   2400,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 88.97%,  Time: 0:01:24 </span><br><span class="line">Iter:   2500,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.31,  Val Acc: 89.67%,  Time: 0:01:28 *</span><br><span class="line">Iter:   2600,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.74%,  Time: 0:01:31 </span><br><span class="line">Iter:   2700,  Train Loss:  0.25,  Train Acc: 93.75%,  Val Loss:  0.31,  Val Acc: 89.83%,  Time: 0:01:35 *</span><br><span class="line">Iter:   2800,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 89.59%,  Time: 0:01:39 </span><br><span class="line">Epoch [3/10]</span><br><span class="line">Iter:   2900,  Train Loss:  0.32,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.21%,  Time: 0:01:42 </span><br><span class="line">Iter:   3000,  Train Loss:  0.21,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 89.80%,  Time: 0:01:45 </span><br><span class="line">Iter:   3100,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.34,  Val Acc: 89.29%,  Time: 0:01:50 </span><br><span class="line">Iter:   3200,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.51%,  Time: 0:01:53 </span><br><span class="line">Iter:   3300,  Train Loss:  0.31,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.85%,  Time: 0:01:57 </span><br><span class="line">Iter:   3400,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.31,  Val Acc: 89.98%,  Time: 0:02:01 </span><br><span class="line">Iter:   3500,  Train Loss:  0.12,  Train Acc: 95.31%,  Val Loss:  0.34,  Val Acc: 89.42%,  Time: 0:02:04 </span><br><span class="line">Iter:   3600,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 90.14%,  Time: 0:02:08 </span><br><span class="line">Iter:   3700,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.31,  Val Acc: 89.92%,  Time: 0:02:11 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:   0.3,  Test Acc: 89.75%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9120    0.8600    0.8852      1000</span><br><span class="line">       realty     0.9037    0.9200    0.9118      1000</span><br><span class="line">       stocks     0.8652    0.7960    0.8292      1000</span><br><span class="line">    education     0.9660    0.9080    0.9361      1000</span><br><span class="line">      science     0.7947    0.8750    0.8329      1000</span><br><span class="line">      society     0.8610    0.9290    0.8937      1000</span><br><span class="line">     politics     0.8918    0.8740    0.8828      1000</span><br><span class="line">       sports     0.9777    0.9660    0.9718      1000</span><br><span class="line">         game     0.9430    0.8930    0.9173      1000</span><br><span class="line">entertainment     0.8801    0.9540    0.9155      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.8975     10000</span><br><span class="line">    macro avg     0.8995    0.8975    0.8976     10000</span><br><span class="line"> weighted avg     0.8995    0.8975    0.8976     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[860  24  59   4  20  13  11   3   1   5]</span><br><span class="line"> [  8 920  16   0  11  15   7   4   4  15]</span><br><span class="line"> [ 53  29 796   1  77   3  32   1   6   2]</span><br><span class="line"> [  2   4   2 908  10  43  13   1   3  14]</span><br><span class="line"> [ 11  10  12   5 875  15  20   3  28  21]</span><br><span class="line"> [  0  15   1  10   8 929  16   1   3  17]</span><br><span class="line"> [  5   6  26   8  19  40 874   1   2  19]</span><br><span class="line"> [  1   1   2   0   1   5   3 966   0  21]</span><br><span class="line"> [  1   4   5   1  69   6   3   2 893  16]</span><br><span class="line"> [  2   5   1   3  11  10   1   6   7 954]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:02, 71756.33it/s]</span><br><span class="line">10000it [00:00, 91902.35it/s]</span><br><span class="line">10000it [00:00, 89357.25it/s]</span><br><span class="line">Time usage: 0:00:03</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (postion_embedding): Positional_Encoding(</span><br><span class="line">    (dropout): Dropout(p=0.5, inplace=False)</span><br><span class="line">  )</span><br><span class="line">  (encoder): Encoder(</span><br><span class="line">    (attention): Multi_Head_Attention(</span><br><span class="line">      (fc_Q): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">      (fc_K): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">      (fc_V): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">      (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">      (fc): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">      (dropout): Dropout(p=0.5, inplace=False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">    )</span><br><span class="line">    (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">      (fc1): Linear(in_features=300, out_features=1024, bias=True)</span><br><span class="line">      (fc2): Linear(in_features=1024, out_features=300, bias=True)</span><br><span class="line">      (dropout): Dropout(p=0.5, inplace=False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (encoders): ModuleList(</span><br><span class="line">    (0): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">        (fc_K): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">        (fc_V): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">        (dropout): Dropout(p=0.5, inplace=False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features=300, out_features=1024, bias=True)</span><br><span class="line">        (fc2): Linear(in_features=1024, out_features=300, bias=True)</span><br><span class="line">        (dropout): Dropout(p=0.5, inplace=False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">        (fc_K): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">        (fc_V): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features=300, out_features=300, bias=True)</span><br><span class="line">        (dropout): Dropout(p=0.5, inplace=False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features=300, out_features=1024, bias=True)</span><br><span class="line">        (fc2): Linear(in_features=1024, out_features=300, bias=True)</span><br><span class="line">        (dropout): Dropout(p=0.5, inplace=False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (fc1): Linear(in_features=9600, out_features=10, bias=True)</span><br><span class="line"><span class="meta">)&gt;</span></span><br><span class="line"><span class="bash">start train</span></span><br><span class="line">Epoch [1/20]</span><br><span class="line">Iter:      0,  Train Loss:   2.4,  Train Acc: 10.16%,  Val Loss:   4.6,  Val Acc: 10.02%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:   1.4,  Train Acc: 53.12%,  Val Loss:   1.4,  Val Acc: 57.23%,  Time: 0:00:03 *</span><br><span class="line">Iter:    200,  Train Loss:   1.3,  Train Acc: 55.47%,  Val Loss:   1.0,  Val Acc: 68.83%,  Time: 0:00:05 *</span><br><span class="line">Iter:    300,  Train Loss:  0.82,  Train Acc: 67.97%,  Val Loss:   0.9,  Val Acc: 74.19%,  Time: 0:00:07 *</span><br><span class="line">Iter:    400,  Train Loss:  0.84,  Train Acc: 75.00%,  Val Loss:  0.83,  Val Acc: 76.70%,  Time: 0:00:09 *</span><br><span class="line">Iter:    500,  Train Loss:  0.65,  Train Acc: 78.12%,  Val Loss:  0.76,  Val Acc: 78.63%,  Time: 0:00:12 *</span><br><span class="line">Iter:    600,  Train Loss:  0.76,  Train Acc: 75.78%,  Val Loss:  0.76,  Val Acc: 79.15%,  Time: 0:00:15 *</span><br><span class="line">Iter:    700,  Train Loss:  0.63,  Train Acc: 78.12%,  Val Loss:  0.78,  Val Acc: 79.21%,  Time: 0:00:17 </span><br><span class="line">Iter:    800,  Train Loss:  0.59,  Train Acc: 81.25%,  Val Loss:  0.63,  Val Acc: 82.16%,  Time: 0:00:19 *</span><br><span class="line">Iter:    900,  Train Loss:   0.7,  Train Acc: 76.56%,  Val Loss:  0.69,  Val Acc: 80.84%,  Time: 0:00:21 </span><br><span class="line">Iter:   1000,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.69,  Val Acc: 80.87%,  Time: 0:00:23 </span><br><span class="line">Iter:   1100,  Train Loss:  0.52,  Train Acc: 83.59%,  Val Loss:  0.62,  Val Acc: 82.61%,  Time: 0:00:26 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.63,  Train Acc: 82.81%,  Val Loss:  0.63,  Val Acc: 82.48%,  Time: 0:00:28 </span><br><span class="line">Iter:   1300,  Train Loss:  0.59,  Train Acc: 80.47%,  Val Loss:  0.55,  Val Acc: 85.06%,  Time: 0:00:32 *</span><br><span class="line">Iter:   1400,  Train Loss:  0.76,  Train Acc: 74.22%,  Val Loss:  0.56,  Val Acc: 84.04%,  Time: 0:00:35 </span><br><span class="line">Epoch [2/20]</span><br><span class="line">Iter:   1500,  Train Loss:  0.63,  Train Acc: 79.69%,  Val Loss:  0.57,  Val Acc: 84.12%,  Time: 0:00:38 </span><br><span class="line">Iter:   1600,  Train Loss:  0.52,  Train Acc: 78.91%,  Val Loss:  0.65,  Val Acc: 82.71%,  Time: 0:00:41 </span><br><span class="line">Iter:   1700,  Train Loss:  0.59,  Train Acc: 82.81%,  Val Loss:  0.58,  Val Acc: 83.98%,  Time: 0:00:44 </span><br><span class="line">Iter:   1800,  Train Loss:  0.46,  Train Acc: 87.50%,  Val Loss:  0.54,  Val Acc: 85.62%,  Time: 0:00:47 *</span><br><span class="line">Iter:   1900,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.53,  Val Acc: 85.76%,  Time: 0:00:50 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.58,  Train Acc: 82.81%,  Val Loss:  0.58,  Val Acc: 84.32%,  Time: 0:00:53 </span><br><span class="line">Iter:   2100,  Train Loss:  0.56,  Train Acc: 86.72%,  Val Loss:  0.55,  Val Acc: 84.90%,  Time: 0:00:56 </span><br><span class="line">Iter:   2200,  Train Loss:  0.36,  Train Acc: 89.84%,  Val Loss:  0.52,  Val Acc: 85.77%,  Time: 0:01:00 *</span><br><span class="line">Iter:   2300,  Train Loss:  0.43,  Train Acc: 86.72%,  Val Loss:  0.55,  Val Acc: 84.77%,  Time: 0:01:03 </span><br><span class="line">Iter:   2400,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.56,  Val Acc: 85.17%,  Time: 0:01:06 </span><br><span class="line">Iter:   2500,  Train Loss:   0.4,  Train Acc: 85.94%,  Val Loss:  0.56,  Val Acc: 84.99%,  Time: 0:01:10 </span><br><span class="line">Iter:   2600,  Train Loss:  0.57,  Train Acc: 79.69%,  Val Loss:  0.55,  Val Acc: 84.93%,  Time: 0:01:13 </span><br><span class="line">Iter:   2700,  Train Loss:  0.44,  Train Acc: 87.50%,  Val Loss:  0.48,  Val Acc: 86.88%,  Time: 0:01:16 *</span><br><span class="line">Iter:   2800,  Train Loss:  0.57,  Train Acc: 82.81%,  Val Loss:   0.5,  Val Acc: 86.41%,  Time: 0:01:20 </span><br><span class="line">Epoch [3/20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.59,  Train Acc: 79.69%,  Val Loss:  0.59,  Val Acc: 84.07%,  Time: 0:01:23 </span><br><span class="line">Iter:   3000,  Train Loss:  0.47,  Train Acc: 86.72%,  Val Loss:  0.52,  Val Acc: 86.12%,  Time: 0:01:27 </span><br><span class="line">Iter:   3100,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.48,  Val Acc: 86.31%,  Time: 0:01:29 </span><br><span class="line">Iter:   3200,  Train Loss:  0.71,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 86.09%,  Time: 0:01:33 </span><br><span class="line">Iter:   3300,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.48,  Val Acc: 87.24%,  Time: 0:01:36 *</span><br><span class="line">Iter:   3400,  Train Loss:  0.51,  Train Acc: 82.03%,  Val Loss:  0.47,  Val Acc: 87.28%,  Time: 0:01:39 *</span><br><span class="line">Iter:   3500,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.54,  Val Acc: 86.01%,  Time: 0:01:42 </span><br><span class="line">Iter:   3600,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.49,  Val Acc: 87.23%,  Time: 0:01:46 </span><br><span class="line">Iter:   3700,  Train Loss:  0.59,  Train Acc: 80.47%,  Val Loss:  0.48,  Val Acc: 86.53%,  Time: 0:01:49 </span><br><span class="line">Iter:   3800,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.52,  Val Acc: 86.27%,  Time: 0:01:52 </span><br><span class="line">Iter:   3900,  Train Loss:  0.51,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 86.02%,  Time: 0:01:56 </span><br><span class="line">Iter:   4000,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.48,  Val Acc: 87.14%,  Time: 0:02:01 </span><br><span class="line">Iter:   4100,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.48,  Val Acc: 87.02%,  Time: 0:02:04 </span><br><span class="line">Iter:   4200,  Train Loss:  0.52,  Train Acc: 84.38%,  Val Loss:  0.49,  Val Acc: 86.53%,  Time: 0:02:08 </span><br><span class="line">Epoch [4/20]</span><br><span class="line">Iter:   4300,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 87.06%,  Time: 0:02:11 </span><br><span class="line">Iter:   4400,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.47,  Val Acc: 87.44%,  Time: 0:02:14 *</span><br><span class="line">Iter:   4500,  Train Loss:  0.47,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 87.48%,  Time: 0:02:17 *</span><br><span class="line">Iter:   4600,  Train Loss:  0.38,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 87.91%,  Time: 0:02:21 *</span><br><span class="line">Iter:   4700,  Train Loss:  0.52,  Train Acc: 82.81%,  Val Loss:  0.45,  Val Acc: 87.24%,  Time: 0:02:25 </span><br><span class="line">Iter:   4800,  Train Loss:  0.33,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 87.63%,  Time: 0:02:28 </span><br><span class="line">Iter:   4900,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 87.85%,  Time: 0:02:30 </span><br><span class="line">Iter:   5000,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 87.79%,  Time: 0:02:33 </span><br><span class="line">Iter:   5100,  Train Loss:  0.53,  Train Acc: 80.47%,  Val Loss:  0.45,  Val Acc: 87.48%,  Time: 0:02:36 </span><br><span class="line">Iter:   5200,  Train Loss:  0.58,  Train Acc: 85.16%,  Val Loss:  0.47,  Val Acc: 87.18%,  Time: 0:02:40 </span><br><span class="line">Iter:   5300,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.48,  Val Acc: 86.80%,  Time: 0:02:43 </span><br><span class="line">Iter:   5400,  Train Loss:  0.67,  Train Acc: 82.03%,  Val Loss:  0.45,  Val Acc: 87.63%,  Time: 0:02:46 </span><br><span class="line">Iter:   5500,  Train Loss:  0.42,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 87.70%,  Time: 0:02:48 *</span><br><span class="line">Iter:   5600,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.02%,  Time: 0:02:51 *</span><br><span class="line">Epoch [5/20]</span><br><span class="line">Iter:   5700,  Train Loss:  0.41,  Train Acc: 86.72%,  Val Loss:  0.42,  Val Acc: 88.50%,  Time: 0:02:54 *</span><br><span class="line">Iter:   5800,  Train Loss:  0.22,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.11%,  Time: 0:02:57 </span><br><span class="line">Iter:   5900,  Train Loss:  0.39,  Train Acc: 91.41%,  Val Loss:  0.44,  Val Acc: 88.15%,  Time: 0:03:00 </span><br><span class="line">Iter:   6000,  Train Loss:  0.44,  Train Acc: 83.59%,  Val Loss:  0.41,  Val Acc: 88.30%,  Time: 0:03:03 *</span><br><span class="line">Iter:   6100,  Train Loss:  0.44,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 88.09%,  Time: 0:03:07 </span><br><span class="line">Iter:   6200,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.42,  Val Acc: 88.42%,  Time: 0:03:09 </span><br><span class="line">Iter:   6300,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.43,  Val Acc: 88.17%,  Time: 0:03:12 </span><br><span class="line">Iter:   6400,  Train Loss:  0.21,  Train Acc: 94.53%,  Val Loss:  0.41,  Val Acc: 88.25%,  Time: 0:03:15 *</span><br><span class="line">Iter:   6500,  Train Loss:  0.44,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 88.49%,  Time: 0:03:19 </span><br><span class="line">Iter:   6600,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.01%,  Time: 0:03:22 </span><br><span class="line">Iter:   6700,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.41,  Val Acc: 88.09%,  Time: 0:03:25 </span><br><span class="line">Iter:   6800,  Train Loss:  0.35,  Train Acc: 89.84%,  Val Loss:  0.45,  Val Acc: 88.01%,  Time: 0:03:28 </span><br><span class="line">Iter:   6900,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.41,  Val Acc: 88.16%,  Time: 0:03:32 </span><br><span class="line">Iter:   7000,  Train Loss:  0.41,  Train Acc: 84.38%,  Val Loss:   0.4,  Val Acc: 88.43%,  Time: 0:03:36 *</span><br><span class="line">Epoch [6/20]</span><br><span class="line">Iter:   7100,  Train Loss:  0.41,  Train Acc: 83.59%,  Val Loss:   0.4,  Val Acc: 88.95%,  Time: 0:03:39 *</span><br><span class="line">Iter:   7200,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 88.71%,  Time: 0:03:41 </span><br><span class="line">Iter:   7300,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.42,  Val Acc: 88.67%,  Time: 0:03:44 </span><br><span class="line">Iter:   7400,  Train Loss:   0.6,  Train Acc: 77.34%,  Val Loss:  0.43,  Val Acc: 87.97%,  Time: 0:03:47 </span><br><span class="line">Iter:   7500,  Train Loss:  0.34,  Train Acc: 84.38%,  Val Loss:  0.42,  Val Acc: 88.02%,  Time: 0:03:50 </span><br><span class="line">Iter:   7600,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 87.11%,  Time: 0:03:54 </span><br><span class="line">Iter:   7700,  Train Loss:   0.4,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.16%,  Time: 0:03:57 *</span><br><span class="line">Iter:   7800,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 88.64%,  Time: 0:04:00 </span><br><span class="line">Iter:   7900,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 88.47%,  Time: 0:04:03 </span><br><span class="line">Iter:   8000,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 89.05%,  Time: 0:04:06 </span><br><span class="line">Iter:   8100,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.42,  Val Acc: 88.54%,  Time: 0:04:09 </span><br><span class="line">Iter:   8200,  Train Loss:  0.37,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.00%,  Time: 0:04:13 </span><br><span class="line">Iter:   8300,  Train Loss:  0.33,  Train Acc: 89.84%,  Val Loss:  0.41,  Val Acc: 87.85%,  Time: 0:04:16 </span><br><span class="line">Iter:   8400,  Train Loss:  0.53,  Train Acc: 78.91%,  Val Loss:  0.37,  Val Acc: 88.97%,  Time: 0:04:20 *</span><br><span class="line">Epoch [7/20]</span><br><span class="line">Iter:   8500,  Train Loss:   0.5,  Train Acc: 85.16%,  Val Loss:  0.43,  Val Acc: 88.20%,  Time: 0:04:22 </span><br><span class="line">Iter:   8600,  Train Loss:  0.25,  Train Acc: 92.19%,  Val Loss:  0.42,  Val Acc: 88.93%,  Time: 0:04:26 </span><br><span class="line">Iter:   8700,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.48%,  Time: 0:04:29 </span><br><span class="line">Iter:   8800,  Train Loss:  0.44,  Train Acc: 82.81%,  Val Loss:  0.39,  Val Acc: 88.89%,  Time: 0:04:33 </span><br><span class="line">Iter:   8900,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 89.18%,  Time: 0:04:36 </span><br><span class="line">Iter:   9000,  Train Loss:  0.23,  Train Acc: 91.41%,  Val Loss:  0.39,  Val Acc: 89.37%,  Time: 0:04:39 </span><br><span class="line">Iter:   9100,  Train Loss:   0.5,  Train Acc: 84.38%,  Val Loss:  0.39,  Val Acc: 89.34%,  Time: 0:04:43 </span><br><span class="line">Iter:   9200,  Train Loss:   0.4,  Train Acc: 90.62%,  Val Loss:   0.4,  Val Acc: 88.81%,  Time: 0:04:46 </span><br><span class="line">Iter:   9300,  Train Loss:  0.41,  Train Acc: 85.16%,  Val Loss:  0.39,  Val Acc: 89.10%,  Time: 0:04:50 </span><br><span class="line">Iter:   9400,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.43,  Val Acc: 88.22%,  Time: 0:04:54 </span><br><span class="line">Iter:   9500,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:   0.4,  Val Acc: 89.02%,  Time: 0:04:57 </span><br><span class="line">Iter:   9600,  Train Loss:  0.47,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.25%,  Time: 0:05:01 </span><br><span class="line">Iter:   9700,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 88.91%,  Time: 0:05:04 </span><br><span class="line">Iter:   9800,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:  0.39,  Val Acc: 89.03%,  Time: 0:05:07 </span><br><span class="line">Epoch [8/20]</span><br><span class="line">Iter:   9900,  Train Loss:  0.51,  Train Acc: 78.91%,  Val Loss:  0.39,  Val Acc: 89.19%,  Time: 0:05:11 </span><br><span class="line">Iter:  10000,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:   0.4,  Val Acc: 88.75%,  Time: 0:05:14 </span><br><span class="line">Iter:  10100,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.38,  Val Acc: 89.28%,  Time: 0:05:17 </span><br><span class="line">Iter:  10200,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:05:20 </span><br><span class="line">Iter:  10300,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 89.62%,  Time: 0:05:24 *</span><br><span class="line">Iter:  10400,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.22%,  Time: 0:05:27 </span><br><span class="line">Iter:  10500,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.41,  Val Acc: 88.86%,  Time: 0:05:31 </span><br><span class="line">Iter:  10600,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:   0.4,  Val Acc: 89.12%,  Time: 0:05:34 </span><br><span class="line">Iter:  10700,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 88.98%,  Time: 0:05:37 </span><br><span class="line">Iter:  10800,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.39,  Val Acc: 88.85%,  Time: 0:05:41 </span><br><span class="line">Iter:  10900,  Train Loss:  0.42,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 89.07%,  Time: 0:05:45 </span><br><span class="line">Iter:  11000,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.04%,  Time: 0:05:48 </span><br><span class="line">Iter:  11100,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 88.94%,  Time: 0:05:51 </span><br><span class="line">Iter:  11200,  Train Loss:  0.44,  Train Acc: 82.81%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:05:54 </span><br><span class="line">Epoch [9/20]</span><br><span class="line">Iter:  11300,  Train Loss:  0.33,  Train Acc: 86.72%,  Val Loss:  0.38,  Val Acc: 89.51%,  Time: 0:05:57 </span><br><span class="line">Iter:  11400,  Train Loss:  0.39,  Train Acc: 85.16%,  Val Loss:   0.4,  Val Acc: 88.89%,  Time: 0:06:01 </span><br><span class="line">Iter:  11500,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:   0.4,  Val Acc: 89.20%,  Time: 0:06:04 </span><br><span class="line">Iter:  11600,  Train Loss:  0.19,  Train Acc: 95.31%,  Val Loss:  0.37,  Val Acc: 89.70%,  Time: 0:06:06 </span><br><span class="line">Iter:  11700,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.89%,  Time: 0:06:11 *</span><br><span class="line">Iter:  11800,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:06:14 </span><br><span class="line">Iter:  11900,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.39,  Val Acc: 89.47%,  Time: 0:06:17 </span><br><span class="line">Iter:  12000,  Train Loss:  0.48,  Train Acc: 86.72%,  Val Loss:  0.39,  Val Acc: 89.43%,  Time: 0:06:20 </span><br><span class="line">Iter:  12100,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.36,  Val Acc: 89.82%,  Time: 0:06:27 *</span><br><span class="line">Iter:  12200,  Train Loss:   0.3,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.53%,  Time: 0:06:30 </span><br><span class="line">Iter:  12300,  Train Loss:   0.3,  Train Acc: 92.97%,  Val Loss:  0.39,  Val Acc: 88.93%,  Time: 0:06:33 </span><br><span class="line">Iter:  12400,  Train Loss:  0.23,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.35%,  Time: 0:06:36 </span><br><span class="line">Iter:  12500,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 89.21%,  Time: 0:06:39 </span><br><span class="line">Iter:  12600,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.75%,  Time: 0:06:44 *</span><br><span class="line">Epoch [10/20]</span><br><span class="line">Iter:  12700,  Train Loss:  0.27,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.64%,  Time: 0:06:48 *</span><br><span class="line">Iter:  12800,  Train Loss:   0.2,  Train Acc: 91.41%,  Val Loss:  0.37,  Val Acc: 89.23%,  Time: 0:06:51 </span><br><span class="line">Iter:  12900,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 89.47%,  Time: 0:06:54 </span><br><span class="line">Iter:  13000,  Train Loss:  0.38,  Train Acc: 91.41%,  Val Loss:   0.4,  Val Acc: 89.49%,  Time: 0:06:58 </span><br><span class="line">Iter:  13100,  Train Loss:  0.41,  Train Acc: 88.28%,  Val Loss:  0.37,  Val Acc: 89.39%,  Time: 0:07:01 </span><br><span class="line">Iter:  13200,  Train Loss:  0.55,  Train Acc: 85.16%,  Val Loss:  0.37,  Val Acc: 89.49%,  Time: 0:07:06 </span><br><span class="line">Iter:  13300,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 89.90%,  Time: 0:07:09 </span><br><span class="line">Iter:  13400,  Train Loss:  0.28,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.26%,  Time: 0:07:12 </span><br><span class="line">Iter:  13500,  Train Loss:   0.3,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.65%,  Time: 0:07:15 </span><br><span class="line">Iter:  13600,  Train Loss:  0.34,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 88.99%,  Time: 0:07:18 </span><br><span class="line">Iter:  13700,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.44%,  Time: 0:07:21 </span><br><span class="line">Iter:  13800,  Train Loss:  0.43,  Train Acc: 83.59%,  Val Loss:  0.35,  Val Acc: 89.62%,  Time: 0:07:25 </span><br><span class="line">Iter:  13900,  Train Loss:  0.28,  Train Acc: 92.19%,  Val Loss:   0.4,  Val Acc: 88.97%,  Time: 0:07:29 </span><br><span class="line">Iter:  14000,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.51%,  Time: 0:07:32 </span><br><span class="line">Epoch [11/20]</span><br><span class="line">Iter:  14100,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.14%,  Time: 0:07:35 </span><br><span class="line">Iter:  14200,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.48%,  Time: 0:07:39 </span><br><span class="line">Iter:  14300,  Train Loss:  0.27,  Train Acc: 89.84%,  Val Loss:  0.37,  Val Acc: 89.69%,  Time: 0:07:42 </span><br><span class="line">Iter:  14400,  Train Loss:  0.23,  Train Acc: 90.62%,  Val Loss:  0.37,  Val Acc: 89.92%,  Time: 0:07:46 </span><br><span class="line">Iter:  14500,  Train Loss:  0.33,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.72%,  Time: 0:07:49 </span><br><span class="line">Iter:  14600,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.83%,  Time: 0:07:52 </span><br><span class="line">Iter:  14700,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 89.63%,  Time: 0:07:55 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.34,  Test Acc: 89.88%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.8745    0.8710    0.8727      1000</span><br><span class="line">       realty     0.9173    0.9210    0.9192      1000</span><br><span class="line">       stocks     0.8308    0.8350    0.8329      1000</span><br><span class="line">    education     0.9356    0.9450    0.9403      1000</span><br><span class="line">      science     0.8395    0.8160    0.8276      1000</span><br><span class="line">      society     0.9097    0.9070    0.9084      1000</span><br><span class="line">     politics     0.8994    0.8760    0.8875      1000</span><br><span class="line">       sports     0.9896    0.9530    0.9710      1000</span><br><span class="line">         game     0.9312    0.9070    0.9189      1000</span><br><span class="line">entertainment     0.8661    0.9570    0.9093      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.8988     10000</span><br><span class="line">    macro avg     0.8994    0.8988    0.8988     10000</span><br><span class="line"> weighted avg     0.8994    0.8988    0.8988     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[871  18  74   6  13   7   5   1   0   5]</span><br><span class="line"> [ 16 921  21   1   9  15   4   1   1  11]</span><br><span class="line"> [ 59  22 835   1  38   4  25   1  10   5]</span><br><span class="line"> [  3   4   0 945   8  11   8   1   4  16]</span><br><span class="line"> [ 15  10  40   9 816  15  24   1  38  32]</span><br><span class="line"> [  5  12   1  21   7 907  22   0   7  18]</span><br><span class="line"> [ 15   3  24  12  21  26 876   0   2  21]</span><br><span class="line"> [  1   2   2   3   5   3   6 953   0  25]</span><br><span class="line"> [  6   5   5   6  47   4   3   2 907  15]</span><br><span class="line"> [  5   7   3   6   8   5   1   3   5 957]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="bert"><a href="#bert" class="headerlink" title="bert"></a>bert</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">180000it [00:35, 5049.92it&#x2F;s]</span><br><span class="line">10000it [00:02, 4529.50it&#x2F;s]</span><br><span class="line">10000it [00:02, 3977.26it&#x2F;s]</span><br><span class="line">Time usage: 0:00:40</span><br><span class="line">model to cpu</span><br><span class="line">model to train</span><br><span class="line">no error -1</span><br><span class="line">no error 0</span><br><span class="line">no error 1</span><br><span class="line">no error 2</span><br><span class="line">Epoch [1&#x2F;3]</span><br><span class="line">&#x2F;data&#x2F;yyfxu&#x2F;text-classify&#x2F;tmp&#x2F;pytorch_pretrained_bert&#x2F;optimization.py:275: UserWarning: This overload of add_ is deprecated:</span><br><span class="line">	add_(Number alpha, Tensor other)</span><br><span class="line">Consider using one of the following signatures instead:</span><br><span class="line">	add_(Tensor other, *, Number alpha) (Triggered internally at  &#x2F;opt&#x2F;conda&#x2F;conda-bld&#x2F;pytorch_1595629403081&#x2F;work&#x2F;torch&#x2F;csrc&#x2F;utils&#x2F;python_arg_parser.cpp:766.)</span><br><span class="line">  next_m.mul_(beta1).add_(1 - beta1, grad)</span><br><span class="line">Iter:      0,  Train Loss:   2.5,  Train Acc:  6.25%,  Val Loss:   2.3,  Val Acc: 10.13%,  Time: 0:00:24 *</span><br><span class="line">Iter:    100,  Train Loss:  0.88,  Train Acc: 75.00%,  Val Loss:  0.73,  Val Acc: 81.03%,  Time: 0:01:24 *</span><br><span class="line">Iter:    200,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:  0.41,  Val Acc: 87.58%,  Time: 0:02:18 *</span><br><span class="line">Iter:    300,  Train Loss:   0.7,  Train Acc: 81.25%,  Val Loss:   0.4,  Val Acc: 88.25%,  Time: 0:03:13 *</span><br><span class="line">Iter:    400,  Train Loss:   1.0,  Train Acc: 81.25%,  Val Loss:  0.38,  Val Acc: 88.62%,  Time: 0:04:07 *</span><br><span class="line">Iter:    500,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.39,  Val Acc: 88.75%,  Time: 0:05:02 </span><br><span class="line">Iter:    600,  Train Loss:  0.31,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 89.58%,  Time: 0:05:57 *</span><br><span class="line">Iter:    700,  Train Loss:  0.29,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.25%,  Time: 0:06:52 </span><br><span class="line">Iter:    800,  Train Loss:  0.35,  Train Acc: 93.75%,  Val Loss:  0.41,  Val Acc: 88.22%,  Time: 0:07:46 </span><br><span class="line">Iter:    900,  Train Loss: 0.036,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 89.76%,  Time: 0:08:42 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.57,  Train Acc: 87.50%,  Val Loss:  0.42,  Val Acc: 89.01%,  Time: 0:09:38 </span><br><span class="line">Iter:   1100,  Train Loss: 0.017,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 88.69%,  Time: 0:10:32 </span><br><span class="line">Iter:   1200,  Train Loss:  0.97,  Train Acc: 81.25%,  Val Loss:   0.4,  Val Acc: 88.91%,  Time: 0:11:27 </span><br><span class="line">Iter:   1300,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.38,  Val Acc: 88.72%,  Time: 0:12:19 </span><br><span class="line">Iter:   1400,  Train Loss:  0.69,  Train Acc: 81.25%,  Val Loss:  0.44,  Val Acc: 87.96%,  Time: 0:13:11 </span><br><span class="line">Iter:   1500,  Train Loss:  0.52,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 88.91%,  Time: 0:14:05 </span><br><span class="line">Iter:   1600,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:  0.52,  Val Acc: 86.66%,  Time: 0:14:57 </span><br><span class="line">Iter:   1700,  Train Loss:  0.12,  Train Acc: 93.75%,  Val Loss:  0.41,  Val Acc: 89.22%,  Time: 0:15:52 </span><br><span class="line">Iter:   1800,  Train Loss: 0.054,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 88.87%,  Time: 0:16:44 </span><br><span class="line">Iter:   1900,  Train Loss:  0.12,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 88.96%,  Time: 0:17:38 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.34,  Test Acc: 90.05%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.8957    0.8670    0.8811      1000</span><br><span class="line">       realty     0.9262    0.9290    0.9276      1000</span><br><span class="line">       stocks     0.8510    0.8170    0.8337      1000</span><br><span class="line">    education     0.9732    0.9430    0.9578      1000</span><br><span class="line">      science     0.7790    0.9130    0.8407      1000</span><br><span class="line">      society     0.9564    0.8550    0.9029      1000</span><br><span class="line">     politics     0.8578    0.9290    0.8920      1000</span><br><span class="line">       sports     0.9092    0.9710    0.9391      1000</span><br><span class="line">         game     0.9383    0.9130    0.9255      1000</span><br><span class="line">entertainment     0.9538    0.8680    0.9089      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9005     10000</span><br><span class="line">    macro avg     0.9041    0.9005    0.9009     10000</span><br><span class="line"> weighted avg     0.9041    0.9005    0.9009     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[867  13  84   2  19   1   9   5   0   0]</span><br><span class="line"> [ 17 929   6   0  16   4  12   7   5   4]</span><br><span class="line"> [ 40  29 817   0  74   0  33   4   3   0]</span><br><span class="line"> [  5   2   3 943   8   9  15   6   2   7]</span><br><span class="line"> [  3   3  17   2 913  10  18   4  24   6]</span><br><span class="line"> [ 18  16   0  12  41 855  41   6   2   9]</span><br><span class="line"> [  8   3  25   6  17   6 929   3   0   3]</span><br><span class="line"> [  3   2   3   2   8   0   4 971   2   5]</span><br><span class="line"> [  2   0   4   0  52   6   7   8 913   8]</span><br><span class="line"> [  5   6   1   2  24   3  15  54  22 868]]</span><br><span class="line">Time usage: 0:00:21</span><br><span class="line">model finish</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:03, 47432.48it&#x2F;s]</span><br><span class="line">10000it [00:00, 46997.85it&#x2F;s]</span><br><span class="line">10000it [00:00, 48451.14it&#x2F;s]Time usage: 0:00:04</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (convs): ModuleList(</span><br><span class="line">    (0): Conv2d(1, 256, kernel_size&#x3D;(2, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(1, 256, kernel_size&#x3D;(3, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (2): Conv2d(1, 256, kernel_size&#x3D;(4, 300), stride&#x3D;(1, 1))</span><br><span class="line">  )</span><br><span class="line">  (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  (fc): Linear(in_features&#x3D;768, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc: 14.84%,  Val Loss:   2.7,  Val Acc: 10.70%,  Time: 0:00:02 *</span><br><span class="line">Iter:    100,  Train Loss:  0.77,  Train Acc: 70.31%,  Val Loss:   0.7,  Val Acc: 78.36%,  Time: 0:00:07 *</span><br><span class="line">Iter:    200,  Train Loss:  0.74,  Train Acc: 74.22%,  Val Loss:  0.55,  Val Acc: 83.18%,  Time: 0:00:12 *</span><br><span class="line">Iter:    300,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.49,  Val Acc: 84.72%,  Time: 0:00:16 *</span><br><span class="line">Iter:    400,  Train Loss:  0.72,  Train Acc: 79.69%,  Val Loss:  0.48,  Val Acc: 85.10%,  Time: 0:00:20 *</span><br><span class="line">Iter:    500,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.44,  Val Acc: 86.10%,  Time: 0:00:24 *</span><br><span class="line">Iter:    600,  Train Loss:  0.49,  Train Acc: 84.38%,  Val Loss:  0.42,  Val Acc: 86.77%,  Time: 0:00:27 *</span><br><span class="line">Iter:    700,  Train Loss:  0.49,  Train Acc: 83.59%,  Val Loss:  0.41,  Val Acc: 87.15%,  Time: 0:00:31 *</span><br><span class="line">Iter:    800,  Train Loss:  0.43,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 87.80%,  Time: 0:00:36 *</span><br><span class="line">Iter:    900,  Train Loss:  0.47,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 88.04%,  Time: 0:00:40 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.38,  Val Acc: 88.54%,  Time: 0:00:45 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.38,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 88.64%,  Time: 0:00:50 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.38,  Train Acc: 86.72%,  Val Loss:  0.37,  Val Acc: 88.99%,  Time: 0:00:55 *</span><br><span class="line">Iter:   1300,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.81%,  Time: 0:01:00 *</span><br><span class="line">Iter:   1400,  Train Loss:  0.52,  Train Acc: 82.81%,  Val Loss:  0.35,  Val Acc: 88.96%,  Time: 0:01:04 *</span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:   0.4,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.09%,  Time: 0:01:10 *</span><br><span class="line">Iter:   1600,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.15%,  Time: 0:01:14</span><br><span class="line">Iter:   1700,  Train Loss:   0.4,  Train Acc: 86.72%,  Val Loss:  0.35,  Val Acc: 89.68%,  Time: 0:01:18 *</span><br><span class="line">Iter:   1800,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.90%,  Time: 0:01:22</span><br><span class="line">Iter:   1900,  Train Loss:  0.36,  Train Acc: 89.84%,  Val Loss:  0.34,  Val Acc: 89.37%,  Time: 0:01:27 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.35,  Train Acc: 85.16%,  Val Loss:  0.35,  Val Acc: 89.47%,  Time: 0:01:31</span><br><span class="line">Iter:   2100,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.40%,  Time: 0:01:35 *</span><br><span class="line">Iter:   2200,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.43%,  Time: 0:01:40</span><br><span class="line">Iter:   2300,  Train Loss:  0.31,  Train Acc: 93.75%,  Val Loss:  0.34,  Val Acc: 89.67%,  Time: 0:01:44 *</span><br><span class="line">Iter:   2400,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.90%,  Time: 0:01:49 *</span><br><span class="line">Iter:   2500,  Train Loss:  0.16,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.18%,  Time: 0:01:53 *</span><br><span class="line">Iter:   2600,  Train Loss:   0.4,  Train Acc: 83.59%,  Val Loss:  0.33,  Val Acc: 90.04%,  Time: 0:01:58</span><br><span class="line">Iter:   2700,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.90%,  Time: 0:02:02</span><br><span class="line">Iter:   2800,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.67%,  Time: 0:02:06</span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 89.80%,  Time: 0:02:11</span><br><span class="line">Iter:   3000,  Train Loss:  0.25,  Train Acc: 91.41%,  Val Loss:  0.34,  Val Acc: 89.71%,  Time: 0:02:15</span><br><span class="line">Iter:   3100,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 89.98%,  Time: 0:02:19</span><br><span class="line">Iter:   3200,  Train Loss:  0.39,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 89.88%,  Time: 0:02:24</span><br><span class="line">Iter:   3300,  Train Loss:  0.29,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.11%,  Time: 0:02:28 *</span><br><span class="line">Iter:   3400,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.97%,  Time: 0:02:33</span><br><span class="line">Iter:   3500,  Train Loss:  0.16,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 90.13%,  Time: 0:02:38</span><br><span class="line">Iter:   3600,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:02:42</span><br><span class="line">Iter:   3700,  Train Loss:  0.36,  Train Acc: 86.72%,  Val Loss:  0.33,  Val Acc: 90.08%,  Time: 0:02:47</span><br><span class="line">Iter:   3800,  Train Loss:  0.36,  Train Acc: 84.38%,  Val Loss:  0.32,  Val Acc: 90.17%,  Time: 0:02:52 *</span><br><span class="line">Iter:   3900,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 90.20%,  Time: 0:02:56</span><br><span class="line">Iter:   4000,  Train Loss:  0.24,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 90.23%,  Time: 0:03:01</span><br><span class="line">Iter:   4100,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:03:05</span><br><span class="line">Iter:   4200,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:03:11</span><br><span class="line">Epoch [4&#x2F;20]</span><br><span class="line">Iter:   4300,  Train Loss:  0.19,  Train Acc: 92.97%,  Val Loss:  0.32,  Val Acc: 90.27%,  Time: 0:03:15 *</span><br><span class="line">Iter:   4400,  Train Loss:  0.19,  Train Acc: 93.75%,  Val Loss:  0.32,  Val Acc: 90.37%,  Time: 0:03:19</span><br><span class="line">Iter:   4500,  Train Loss:  0.37,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 90.21%,  Time: 0:03:24</span><br><span class="line">Iter:   4600,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 90.02%,  Time: 0:03:29</span><br><span class="line">Iter:   4700,  Train Loss:  0.48,  Train Acc: 87.50%,  Val Loss:  0.32,  Val Acc: 90.25%,  Time: 0:03:34</span><br><span class="line">Iter:   4800,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 90.34%,  Time: 0:03:38</span><br><span class="line">Iter:   4900,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 90.26%,  Time: 0:03:41</span><br><span class="line">Iter:   5000,  Train Loss:  0.21,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.88%,  Time: 0:03:46</span><br><span class="line">Iter:   5100,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.21%,  Time: 0:03:50</span><br><span class="line">Iter:   5200,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 90.12%,  Time: 0:03:55</span><br><span class="line">Iter:   5300,  Train Loss:  0.19,  Train Acc: 96.09%,  Val Loss:  0.32,  Val Acc: 90.46%,  Time: 0:04:00 *</span><br><span class="line">Iter:   5400,  Train Loss:  0.47,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 90.17%,  Time: 0:04:04</span><br><span class="line">Iter:   5500,  Train Loss:  0.27,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.41%,  Time: 0:04:08</span><br><span class="line">Iter:   5600,  Train Loss:  0.13,  Train Acc: 95.31%,  Val Loss:  0.33,  Val Acc: 90.17%,  Time: 0:04:13</span><br><span class="line">Epoch [5&#x2F;20]</span><br><span class="line">Iter:   5700,  Train Loss:  0.24,  Train Acc: 94.53%,  Val Loss:  0.34,  Val Acc: 90.07%,  Time: 0:04:18</span><br><span class="line">Iter:   5800,  Train Loss:  0.16,  Train Acc: 93.75%,  Val Loss:  0.34,  Val Acc: 90.14%,  Time: 0:04:22</span><br><span class="line">Iter:   5900,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 90.52%,  Time: 0:04:28</span><br><span class="line">Iter:   6000,  Train Loss:  0.15,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.45%,  Time: 0:04:32</span><br><span class="line">Iter:   6100,  Train Loss:  0.26,  Train Acc: 89.06%,  Val Loss:  0.32,  Val Acc: 90.58%,  Time: 0:04:36</span><br><span class="line">Iter:   6200,  Train Loss:  0.13,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 90.46%,  Time: 0:04:41</span><br><span class="line">Iter:   6300,  Train Loss:  0.12,  Train Acc: 96.88%,  Val Loss:  0.33,  Val Acc: 90.56%,  Time: 0:04:45</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.29,  Test Acc: 91.34%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9193    0.8890    0.9039      1000</span><br><span class="line">       realty     0.9134    0.9490    0.9308      1000</span><br><span class="line">       stocks     0.8363    0.8790    0.8571      1000</span><br><span class="line">    education     0.9540    0.9550    0.9545      1000</span><br><span class="line">      science     0.8800    0.8580    0.8689      1000</span><br><span class="line">      society     0.9067    0.9140    0.9104      1000</span><br><span class="line">     politics     0.9054    0.8900    0.8976      1000</span><br><span class="line">       sports     0.9578    0.9540    0.9559      1000</span><br><span class="line">         game     0.9383    0.9130    0.9255      1000</span><br><span class="line">entertainment     0.9265    0.9330    0.9297      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9134     10000</span><br><span class="line">    macro avg     0.9138    0.9134    0.9134     10000</span><br><span class="line"> weighted avg     0.9138    0.9134    0.9134     10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[889  17  59   4  10   9   7   3   0   2]</span><br><span class="line"> [  9 949  13   2   4   8   4   2   3   6]</span><br><span class="line"> [ 40  24 879   3  21   0  25   3   4   1]</span><br><span class="line"> [  2   2   1 955   6  13   5   2   1  13]</span><br><span class="line"> [  8   8  38   6 858  15  16   5  36  10]</span><br><span class="line"> [  4  22   3  11  10 914  26   2   2   6]</span><br><span class="line"> [  9   7  32  10  15  27 890   4   0   6]</span><br><span class="line"> [  3   2   7   1   3   8   4 954   2  16]</span><br><span class="line"> [  2   1  15   4  36   4   3   8 913  14]</span><br><span class="line"> [  1   7   4   5  12  10   3  13  12 933]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure>



<blockquote>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/dingyh0626/Bert-THUCNews-Classification">Bert</a></p>
</blockquote>

    </div>

    
    
    

    <!--添加版权信息-->
    <div>
        
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script>
  <script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script>
  <link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css">

  <p><span>本文标题:</span>chinese_text_classify</a></p>
  <p><span>本文作者:</span>xydaytoy</a></p>
  <p><span>本文链接:</span><a href="/2021/11/28/chinese-text-classify/" title="chinese_text_classify">http://example.com/2021/11/28/chinese-text-classify/</a>
    <span class="copy-path"  title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://example.com/2021/11/28/chinese-text-classify/"  aria-label="复制成功！"></i></span>
  </p>
  <p><span>版权声明:</span>本博客所有文章除特别声明外，均采用<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND</a>许可协议。转载请注明出处！</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    clipboard.on('success', $(function(){
      $(".fa-clipboard").click(function(){
        swal({   
          title: "",   
          text: '复制成功',   
          html: false,
          timer: 500,   
          showConfirmButton: false
        });
      });
    }));  
</script>

        
    </div>

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/11/27/data-preprocess/" rel="prev" title="data_preprocess">
      <i class="fa fa-chevron-left"></i> data_preprocess
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/11/28/linux-tips/" rel="next" title="linux-tips">
      linux-tips <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7%E5%8F%8A%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="nav-number">1.</span> <span class="nav-text">1 相关工具及目录结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 相关工具</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E4%B8%8E%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 目录结构与初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.2.1 目录结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2-%E7%94%A8%E4%BA%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84bash%E6%96%87%E4%BB%B6"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2.2 用于初始化的bash文件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E6%95%B0%E6%8D%AE%E7%9A%84%E5%87%86%E5%A4%87"><span class="nav-number">2.</span> <span class="nav-text">2 数据的准备</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-THUCNews"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 THUCNews</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 数据格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%9F%E6%88%90"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2 数据集生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-tokenize"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.2.3 tokenize</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-normalize-punctuation-%E5%8F%AF%E9%80%89"><span class="nav-number">2.2.4.</span> <span class="nav-text">2.2.3 normalize-punctuation(可选)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-4-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D"><span class="nav-number">2.2.5.</span> <span class="nav-text">2.2.4 中文分词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-5-tokenize"><span class="nav-number">2.2.6.</span> <span class="nav-text">2.2.5 tokenize</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-6-truecase"><span class="nav-number">2.2.7.</span> <span class="nav-text">2.2.6 truecase</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-7-bpe"><span class="nav-number">2.2.8.</span> <span class="nav-text">2.2.7 bpe</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-8-clean"><span class="nav-number">2.2.9.</span> <span class="nav-text">2.2.8 clean</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-9-split"><span class="nav-number">2.2.10.</span> <span class="nav-text">2.2.9 split</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">3.</span> <span class="nav-text">3 训练过程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E7%94%9F%E6%88%90%E8%AF%8D%E8%A1%A8%E5%8F%8A%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 生成词表及二进制文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E8%AE%AD%E7%BB%83"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-%E8%A7%A3%E7%A0%81"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 解码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-%E7%94%9F%E6%88%90%E5%BC%8F%E8%A7%A3%E7%A0%81"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.1 生成式解码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-%E4%BA%A4%E4%BA%92%E5%BC%8F%E8%A7%A3%E7%A0%81"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.3.2 交互式解码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-%E5%90%8E%E5%A4%84%E7%90%86%E5%8F%8A%E8%AF%84%E4%BB%B7"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 后处理及评价</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-1-%E6%8A%BD%E5%8F%96%E8%AF%91%E6%96%87"><span class="nav-number">3.4.1.</span> <span class="nav-text">3.4.1 抽取译文</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-1-%E5%8E%BB%E9%99%A4bpe%E7%AC%A6%E5%8F%B7"><span class="nav-number">3.4.2.</span> <span class="nav-text">3.4.1 去除bpe符号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-2-detruecase"><span class="nav-number">3.4.3.</span> <span class="nav-text">3.4.2 detruecase</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-3-%E8%AF%84%E4%BB%B7"><span class="nav-number">3.4.4.</span> <span class="nav-text">3.4.3 评价</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-4-detokenize"><span class="nav-number">3.4.5.</span> <span class="nav-text">3.4.4 detokenize</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#bilstm-attention"><span class="nav-number">4.</span> <span class="nav-text">bilstm+attention:</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer"><span class="nav-number">5.</span> <span class="nav-text">Transformer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#bert"><span class="nav-number">6.</span> <span class="nav-text">bert</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN"><span class="nav-number">7.</span> <span class="nav-text">CNN</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">xydaytoy</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/xydaytoy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xydaytoy" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:daytoy2018@gmail.com" title="E-Mail → mailto:daytoy2018@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xydaytoy</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>

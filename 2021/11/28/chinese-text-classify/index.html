<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="中文文本分类流程">
<meta property="og:type" content="article">
<meta property="og:title" content="chinese_text_classify">
<meta property="og:url" content="http://example.com/2021/11/28/chinese-text-classify/index.html">
<meta property="og:site_name" content="札记">
<meta property="og:description" content="中文文本分类流程">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-11-28T05:16:23.000Z">
<meta property="article:modified_time" content="2021-12-03T15:30:34.915Z">
<meta property="article:author" content="xydaytoy">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2021/11/28/chinese-text-classify/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>chinese_text_classify | 札记</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="札记" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">札记</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>摘记</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/28/chinese-text-classify/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="xydaytoy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="札记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          chinese_text_classify
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-28 13:16:23" itemprop="dateCreated datePublished" datetime="2021-11-28T13:16:23+08:00">2021-11-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-03 23:30:34" itemprop="dateModified" datetime="2021-12-03T23:30:34+08:00">2021-12-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/research/" itemprop="url" rel="index"><span itemprop="name">research</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/research/text-classify/" itemprop="url" rel="index"><span itemprop="name">text_classify</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>中文文本分类流程</p>
</blockquote>
<span id="more"></span>

<h1 id="1-相关工具及目录结构"><a href="#1-相关工具及目录结构" class="headerlink" title="1 相关工具及目录结构"></a>1 相关工具及目录结构</h1><h2 id="1-1-相关工具"><a href="#1-1-相关工具" class="headerlink" title="1.1 相关工具"></a>1.1 相关工具</h2><p>除<strong>jieba</strong>是使用<code>pip install</code>安装外，其他几个工具都是建议直接克隆库到自己的用户目录中，方便使用其脚本(<strong>moses</strong>/<strong>subword-nmt</strong>)，或未来可能要自己拓展其中的模型(<strong>fairseq</strong>)</p>
<ol>
<li><p>jieba (中文分词组件)，安装指令如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="1-2-目录结构与初始化"><a href="#1-2-目录结构与初始化" class="headerlink" title="1.2 目录结构与初始化"></a>1.2 目录结构与初始化</h2><h3 id="1-2-1-目录结构"><a href="#1-2-1-目录结构" class="headerlink" title="1.2.1 目录结构"></a>1.2.1 目录结构</h3><p>提前组织一个目录结构的好处是可以让后面的一系列操作更加统一、规范化。下表中<code>~</code>代表linux系统中<strong>我的用户目录</strong>, THUCNews目录名代表此次我使用的数据集名称</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">~</span><br><span class="line">├── mosesdecoder</span><br><span class="line">└── text-classify</span><br><span class="line">    ├── master</span><br><span class="line">    	├── THUCNews</span><br><span class="line">    		├── split_data.py   # 用于划分train,valid,test</span><br><span class="line">        	├── data            # 用于存放数据</span><br><span class="line">            	├── train.txt          </span><br><span class="line">            	├── train_long.txt</span><br><span class="line">            	├── dev.txt  </span><br><span class="line">            	├── dev_long.txt </span><br><span class="line">            	├── test.txt </span><br><span class="line">            	├── test_long.txt </span><br><span class="line">            	├── class.txt  </span><br><span class="line">            	├── embedding_SougouNews.npz  </span><br><span class="line">            	├── embedding_Tencent.npz  </span><br><span class="line">            	└── vocab.pkl</span><br><span class="line">            └── saved_dict      # 用于存放翻译结果</span><br><span class="line">    ├── models                  </span><br><span class="line">        ├── Bert.py  </span><br><span class="line">        ├── TextCNN.py  </span><br><span class="line">        ├── TextRNN_Att.py  </span><br><span class="line">        └── Transformer.py</span><br><span class="line">    ├── pytorch_pretrained_bert</span><br><span class="line">    ├── scripts                 # 一些脚本</span><br><span class="line">    	├── train_bert.sh  </span><br><span class="line">    	├── train_bilstm_att.sh  </span><br><span class="line">    	├── train_cnn.sh  </span><br><span class="line">    	└── train_transformer.sh</span><br><span class="line">    ├── utils.py                # 一些其他工具</span><br><span class="line">    ├── utils_bert.py           </span><br><span class="line">    ├── run.py					# 模型运行</span><br><span class="line">    ├── run_bert.py				</span><br><span class="line">    ├── train_eval.py			</span><br><span class="line">    └── train_eval_bert.py</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="2-数据的准备"><a href="#2-数据的准备" class="headerlink" title="2 数据的准备"></a>2 数据的准备</h1><h2 id="2-1-THUCNews"><a href="#2-1-THUCNews" class="headerlink" title="2.1 THUCNews"></a>2.1 THUCNews</h2><p>本文采用 <a target="_blank" rel="noopener" href="http://thuctc.thunlp.org/">THUCNews中文文本开源语料</a> ，THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。共分为14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。</p>
<p><strong>原始数据统计：</strong></p>
<p>总数量：836075条（科技：162929；股票：154398；体育：131604；娱乐：92632；时政：63086；社会：50849；教育：41936；财经：37098；家居：32586；游戏：24373；房产：20050；时尚：13368；彩票：7588；星座：3578）</p>
<h2 id="2-2-数据预处理"><a href="#2-2-数据预处理" class="headerlink" title="2.2 数据预处理"></a>2.2 数据预处理</h2><h3 id="2-2-1-原始数据格式"><a href="#2-2-1-原始数据格式" class="headerlink" title="2.2.1 原始数据格式"></a>2.2.1 原始数据格式</h3><p>分类放置，每个文档包含一篇新闻文本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">└── THUCNews</span><br><span class="line">        └── 体育     </span><br><span class="line">            └── 29016.txt</span><br><span class="line">        └── 财经     </span><br><span class="line">            └── 11016.txt</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>文档格式如下：由标题和正文两部分组成</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">组图：9月全球折扣明星带你去血拼</span><br><span class="line">　　导读：每年的8，9两月，都是普天下血拼狂人大开杀戒的日子。因为全球性的折扣季节已正式启动。以折扣率最疯狂的美国来说，无论一线大牌还是时尚潮物，统统为5 折为起点，一路狂飙到2折左右。去年8月，Sami就曾以0.5折的夸张扣率抢到一条MiuMiu连衣裙。此番Sami特别请到20位美国本土女星，她们让现身说法，带你掏遍折扣季节最值得入货的高性价比超值单品。</span><br><span class="line">　　Heidi Montag推荐超值折扣单品：GUCCI凉鞋</span><br><span class="line">　　Ashley Tisdale推荐超值折扣单品：破洞牛仔小脚裤</span><br></pre></td></tr></table></figure>

<h3 id="2-2-2-数据集生成"><a href="#2-2-2-数据集生成" class="headerlink" title="2.2.2 数据集生成"></a>2.2.2 数据集生成</h3><p>从中抽取10个类别（财经、房产、股票、教育、科技、社会、时政、体育、游戏、娱乐）随机抽取共20万条数据，其中训练集18万数据，验证集1万数据，测试集1万数据。</p>
<p>首先，从原始数据集中选择10个类别抽取部分数据，每个类别随机抽取18000条数据作为训练集，1000条数据作为验证集，1000条数据作为测试集，使用split_data.py划分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python split_data.py</span><br></pre></td></tr></table></figure>

<p>使用shuffle.py打乱顺序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python shuffle.py</span><br></pre></td></tr></table></figure>

<p>短文本（标题）效果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">中国人寿承诺不裁员不减薪        0</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>长文本（标题+正文）效果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">四新股冻结资金4112亿元 王亚伟青睐九安医疗\　　⊙见习记者方俊○编辑朱绍勇　　据今日公告披露，本周一发行的中小板新股兆驰股份、杭氧股份、九安医疗、棕榈园林合计冻结资金4112.5亿元，而整体来看打新资金对个股的选择出现了严重分化，其中兆驰股份和九安医疗的中签率分别为2.445%和0.487%，相差近5倍。　　公告显示，兆驰股份网上定价发行中签率为2.445%，仅次于此前科伦药业3.1%中签率，超额认购倍数为41倍，冻结资金549.68亿元，网下有效申购资金为53.37亿元。杭氧股份网上中签率为1.1%，超额认购倍数为90倍，冻结资金924.58亿元，网下有效申购资金为80.65亿元。棕榈园林网上中签率为0.88%，超额认购倍数为113倍，冻结资金1219.26亿元，网下有效申购资金为196.47亿元。　　九安医疗网上中签率为0.487%，超额认购倍数为205倍，冻结资金986.79亿元，网下有效申购资金为101.7亿元。而网下配售结果显示，九安医疗也受到了127家配售对象的青睐，其中包括王亚伟执掌的华夏大盘和华夏策略。	0</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>最后，THUCNews/data目录中有如下数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── THUCNews</span><br><span class="line">    └── data</span><br><span class="line">        ├── train.txt</span><br><span class="line">        ├── train_long.txt</span><br><span class="line">        ├── dev.txt</span><br><span class="line">        ├── dev_long.txt</span><br><span class="line">        ├── test.txt</span><br><span class="line">        └── test_long.txt</span><br></pre></td></tr></table></figure>

<h1 id="3-训练过程"><a href="#3-训练过程" class="headerlink" title="3 训练过程"></a>3 训练过程</h1><h2 id="3-1-bert模型"><a href="#3-1-bert模型" class="headerlink" title="3.1 bert模型"></a>3.1 bert模型</h2><p>1.运行脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run_bert.py --model bert &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>2.运行log</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">180000it [00:35, 5049.92it&#x2F;s]</span><br><span class="line">10000it [00:02, 4529.50it&#x2F;s]</span><br><span class="line">10000it [00:02, 3977.26it&#x2F;s]</span><br><span class="line">Time usage: 0:00:40</span><br><span class="line">model to cpu</span><br><span class="line">model to train</span><br><span class="line">no error -1</span><br><span class="line">no error 0</span><br><span class="line">no error 1</span><br><span class="line">no error 2</span><br><span class="line">Epoch [1&#x2F;3]</span><br><span class="line">&#x2F;data&#x2F;yyfxu&#x2F;text-classify&#x2F;tmp&#x2F;pytorch_pretrained_bert&#x2F;optimization.py:275: UserWarning: This overload of add_ is deprecated:</span><br><span class="line">	add_(Number alpha, Tensor other)</span><br><span class="line">Consider using one of the following signatures instead:</span><br><span class="line">	add_(Tensor other, *, Number alpha) (Triggered internally at  &#x2F;opt&#x2F;conda&#x2F;conda-bld&#x2F;pytorch_1595629403081&#x2F;work&#x2F;torch&#x2F;csrc&#x2F;utils&#x2F;python_arg_parser.cpp:766.)</span><br><span class="line">  next_m.mul_(beta1).add_(1 - beta1, grad)</span><br><span class="line">Iter:      0,  Train Loss:   2.5,  Train Acc:  6.25%,  Val Loss:   2.3,  Val Acc: 10.13%,  Time: 0:00:24 *</span><br><span class="line">Iter:    100,  Train Loss:  0.88,  Train Acc: 75.00%,  Val Loss:  0.73,  Val Acc: 81.03%,  Time: 0:01:24 *</span><br><span class="line">Iter:    200,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:  0.41,  Val Acc: 87.58%,  Time: 0:02:18 *</span><br><span class="line">Iter:    300,  Train Loss:   0.7,  Train Acc: 81.25%,  Val Loss:   0.4,  Val Acc: 88.25%,  Time: 0:03:13 *</span><br><span class="line">Iter:    400,  Train Loss:   1.0,  Train Acc: 81.25%,  Val Loss:  0.38,  Val Acc: 88.62%,  Time: 0:04:07 *</span><br><span class="line">Iter:    500,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.39,  Val Acc: 88.75%,  Time: 0:05:02 </span><br><span class="line">Iter:    600,  Train Loss:  0.31,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 89.58%,  Time: 0:05:57 *</span><br><span class="line">Iter:    700,  Train Loss:  0.29,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.25%,  Time: 0:06:52 </span><br><span class="line">Iter:    800,  Train Loss:  0.35,  Train Acc: 93.75%,  Val Loss:  0.41,  Val Acc: 88.22%,  Time: 0:07:46 </span><br><span class="line">Iter:    900,  Train Loss: 0.036,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 89.76%,  Time: 0:08:42 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.57,  Train Acc: 87.50%,  Val Loss:  0.42,  Val Acc: 89.01%,  Time: 0:09:38 </span><br><span class="line">Iter:   1100,  Train Loss: 0.017,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 88.69%,  Time: 0:10:32 </span><br><span class="line">Iter:   1200,  Train Loss:  0.97,  Train Acc: 81.25%,  Val Loss:   0.4,  Val Acc: 88.91%,  Time: 0:11:27 </span><br><span class="line">Iter:   1300,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.38,  Val Acc: 88.72%,  Time: 0:12:19 </span><br><span class="line">Iter:   1400,  Train Loss:  0.69,  Train Acc: 81.25%,  Val Loss:  0.44,  Val Acc: 87.96%,  Time: 0:13:11 </span><br><span class="line">Iter:   1500,  Train Loss:  0.52,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 88.91%,  Time: 0:14:05 </span><br><span class="line">Iter:   1600,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:  0.52,  Val Acc: 86.66%,  Time: 0:14:57 </span><br><span class="line">Iter:   1700,  Train Loss:  0.12,  Train Acc: 93.75%,  Val Loss:  0.41,  Val Acc: 89.22%,  Time: 0:15:52 </span><br><span class="line">Iter:   1800,  Train Loss: 0.054,  Train Acc: 100.00%,  Val Loss:  0.37,  Val Acc: 88.87%,  Time: 0:16:44 </span><br><span class="line">Iter:   1900,  Train Loss:  0.12,  Train Acc: 100.00%,  Val Loss:  0.38,  Val Acc: 88.96%,  Time: 0:17:38 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.34,  Test Acc: 90.05%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.8957    0.8670    0.8811      1000</span><br><span class="line">       realty     0.9262    0.9290    0.9276      1000</span><br><span class="line">       stocks     0.8510    0.8170    0.8337      1000</span><br><span class="line">    education     0.9732    0.9430    0.9578      1000</span><br><span class="line">      science     0.7790    0.9130    0.8407      1000</span><br><span class="line">      society     0.9564    0.8550    0.9029      1000</span><br><span class="line">     politics     0.8578    0.9290    0.8920      1000</span><br><span class="line">       sports     0.9092    0.9710    0.9391      1000</span><br><span class="line">         game     0.9383    0.9130    0.9255      1000</span><br><span class="line">entertainment     0.9538    0.8680    0.9089      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9005     10000</span><br><span class="line">    macro avg     0.9041    0.9005    0.9009     10000</span><br><span class="line"> weighted avg     0.9041    0.9005    0.9009     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[867  13  84   2  19   1   9   5   0   0]</span><br><span class="line"> [ 17 929   6   0  16   4  12   7   5   4]</span><br><span class="line"> [ 40  29 817   0  74   0  33   4   3   0]</span><br><span class="line"> [  5   2   3 943   8   9  15   6   2   7]</span><br><span class="line"> [  3   3  17   2 913  10  18   4  24   6]</span><br><span class="line"> [ 18  16   0  12  41 855  41   6   2   9]</span><br><span class="line"> [  8   3  25   6  17   6 929   3   0   3]</span><br><span class="line"> [  3   2   3   2   8   0   4 971   2   5]</span><br><span class="line"> [  2   0   4   0  52   6   7   8 913   8]</span><br><span class="line"> [  5   6   1   2  24   3  15  54  22 868]]</span><br><span class="line">Time usage: 0:00:21</span><br><span class="line">model finish</span><br></pre></td></tr></table></figure>

<p>使用了预训练的bert模型，长文本超出长度限制（512），bert没有进行长文本实验。</p>
<h2 id="3-2-bilstm-attention模型"><a href="#3-2-bilstm-attention模型" class="headerlink" title="3.2 bilstm+attention模型"></a>3.2 bilstm+attention模型</h2><p>1.运行脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run.py --model TextRNN_Att &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>2.运行log</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:02, 70404.96it/s]</span><br><span class="line">10000it [00:00, 90716.68it/s]</span><br><span class="line">10000it [00:00, 92472.73it/s]</span><br><span class="line">Time usage: 0:00:03</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)</span><br><span class="line">  (tanh1): Tanh()</span><br><span class="line">  (tanh2): Tanh()</span><br><span class="line">  (fc1): Linear(in_features=256, out_features=64, bias=True)</span><br><span class="line">  (fc): Linear(in_features=64, out_features=10, bias=True)</span><br><span class="line"><span class="meta">)&gt;</span></span><br><span class="line"><span class="bash">start train</span></span><br><span class="line">Epoch [1/10]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc:  6.25%,  Val Loss:   2.3,  Val Acc: 10.01%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:  0.75,  Train Acc: 75.00%,  Val Loss:  0.78,  Val Acc: 73.34%,  Time: 0:00:05 *</span><br><span class="line">Iter:    200,  Train Loss:  0.76,  Train Acc: 72.66%,  Val Loss:  0.58,  Val Acc: 81.38%,  Time: 0:00:07 *</span><br><span class="line">Iter:    300,  Train Loss:   0.4,  Train Acc: 88.28%,  Val Loss:  0.53,  Val Acc: 83.07%,  Time: 0:00:10 *</span><br><span class="line">Iter:    400,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.47,  Val Acc: 84.44%,  Time: 0:00:14 *</span><br><span class="line">Iter:    500,  Train Loss:  0.44,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 85.70%,  Time: 0:00:18 *</span><br><span class="line">Iter:    600,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 86.46%,  Time: 0:00:22 *</span><br><span class="line">Iter:    700,  Train Loss:  0.41,  Train Acc: 85.94%,  Val Loss:  0.41,  Val Acc: 86.79%,  Time: 0:00:25 *</span><br><span class="line">Iter:    800,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 87.86%,  Time: 0:00:29 *</span><br><span class="line">Iter:    900,  Train Loss:  0.44,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 87.53%,  Time: 0:00:33 </span><br><span class="line">Iter:   1000,  Train Loss:  0.26,  Train Acc: 90.62%,  Val Loss:  0.37,  Val Acc: 88.04%,  Time: 0:00:36 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.29,  Train Acc: 92.19%,  Val Loss:  0.39,  Val Acc: 86.97%,  Time: 0:00:40 </span><br><span class="line">Iter:   1200,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.41%,  Time: 0:00:43 *</span><br><span class="line">Iter:   1300,  Train Loss:  0.34,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.54%,  Time: 0:00:47 </span><br><span class="line">Iter:   1400,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.44%,  Time: 0:00:50 </span><br><span class="line">Epoch [2/10]</span><br><span class="line">Iter:   1500,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 88.15%,  Time: 0:00:54 </span><br><span class="line">Iter:   1600,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 88.53%,  Time: 0:00:57 </span><br><span class="line">Iter:   1700,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 88.63%,  Time: 0:01:00 *</span><br><span class="line">Iter:   1800,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.31%,  Time: 0:01:03 *</span><br><span class="line">Iter:   1900,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.55%,  Time: 0:01:07 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 89.12%,  Time: 0:01:10 </span><br><span class="line">Iter:   2100,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 89.42%,  Time: 0:01:14 </span><br><span class="line">Iter:   2200,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 89.52%,  Time: 0:01:17 </span><br><span class="line">Iter:   2300,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.31%,  Time: 0:01:21 *</span><br><span class="line">Iter:   2400,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 88.97%,  Time: 0:01:24 </span><br><span class="line">Iter:   2500,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.31,  Val Acc: 89.67%,  Time: 0:01:28 *</span><br><span class="line">Iter:   2600,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.74%,  Time: 0:01:31 </span><br><span class="line">Iter:   2700,  Train Loss:  0.25,  Train Acc: 93.75%,  Val Loss:  0.31,  Val Acc: 89.83%,  Time: 0:01:35 *</span><br><span class="line">Iter:   2800,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 89.59%,  Time: 0:01:39 </span><br><span class="line">Epoch [3/10]</span><br><span class="line">Iter:   2900,  Train Loss:  0.32,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.21%,  Time: 0:01:42 </span><br><span class="line">Iter:   3000,  Train Loss:  0.21,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 89.80%,  Time: 0:01:45 </span><br><span class="line">Iter:   3100,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.34,  Val Acc: 89.29%,  Time: 0:01:50 </span><br><span class="line">Iter:   3200,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.51%,  Time: 0:01:53 </span><br><span class="line">Iter:   3300,  Train Loss:  0.31,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.85%,  Time: 0:01:57 </span><br><span class="line">Iter:   3400,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.31,  Val Acc: 89.98%,  Time: 0:02:01 </span><br><span class="line">Iter:   3500,  Train Loss:  0.12,  Train Acc: 95.31%,  Val Loss:  0.34,  Val Acc: 89.42%,  Time: 0:02:04 </span><br><span class="line">Iter:   3600,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 90.14%,  Time: 0:02:08 </span><br><span class="line">Iter:   3700,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.31,  Val Acc: 89.92%,  Time: 0:02:11 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:   0.3,  Test Acc: 89.75%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9120    0.8600    0.8852      1000</span><br><span class="line">       realty     0.9037    0.9200    0.9118      1000</span><br><span class="line">       stocks     0.8652    0.7960    0.8292      1000</span><br><span class="line">    education     0.9660    0.9080    0.9361      1000</span><br><span class="line">      science     0.7947    0.8750    0.8329      1000</span><br><span class="line">      society     0.8610    0.9290    0.8937      1000</span><br><span class="line">     politics     0.8918    0.8740    0.8828      1000</span><br><span class="line">       sports     0.9777    0.9660    0.9718      1000</span><br><span class="line">         game     0.9430    0.8930    0.9173      1000</span><br><span class="line">entertainment     0.8801    0.9540    0.9155      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.8975     10000</span><br><span class="line">    macro avg     0.8995    0.8975    0.8976     10000</span><br><span class="line"> weighted avg     0.8995    0.8975    0.8976     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[860  24  59   4  20  13  11   3   1   5]</span><br><span class="line"> [  8 920  16   0  11  15   7   4   4  15]</span><br><span class="line"> [ 53  29 796   1  77   3  32   1   6   2]</span><br><span class="line"> [  2   4   2 908  10  43  13   1   3  14]</span><br><span class="line"> [ 11  10  12   5 875  15  20   3  28  21]</span><br><span class="line"> [  0  15   1  10   8 929  16   1   3  17]</span><br><span class="line"> [  5   6  26   8  19  40 874   1   2  19]</span><br><span class="line"> [  1   1   2   0   1   5   3 966   0  21]</span><br><span class="line"> [  1   4   5   1  69   6   3   2 893  16]</span><br><span class="line"> [  2   5   1   3  11  10   1   6   7 954]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>长文本（标题+正文）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14699.32it&#x2F;s]</span><br><span class="line">10000it [00:00, 12818.24it&#x2F;s]</span><br><span class="line">10000it [00:00, 16106.52it&#x2F;s]</span><br><span class="line">Time usage: 0:00:14</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">(base) yyfxu@cip57:~&#x2F;text-classify&#x2F;master$ vim nohup_bi_long.log </span><br><span class="line">(base) yyfxu@cip57:~&#x2F;text-classify&#x2F;master$ head -n 20 nohup_bi_long.log </span><br><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14699.32it&#x2F;s]</span><br><span class="line">10000it [00:00, 12818.24it&#x2F;s]</span><br><span class="line">10000it [00:00, 16106.52it&#x2F;s]</span><br><span class="line">Time usage: 0:00:14</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (lstm): LSTM(300, 128, num_layers&#x3D;2, batch_first&#x3D;True, dropout&#x3D;0.5, bidirectional&#x3D;True)</span><br><span class="line">  (tanh1): Tanh()</span><br><span class="line">  (tanh2): Tanh()</span><br><span class="line">  (fc1): Linear(in_features&#x3D;256, out_features&#x3D;64, bias&#x3D;True)</span><br><span class="line">  (fc): Linear(in_features&#x3D;64, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;10]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc: 18.75%,  Val Loss:   2.3,  Val Acc: 10.03%,  Time: 0:00:00 *</span><br><span class="line">Iter:    100,  Train Loss:  0.55,  Train Acc: 85.16%,  Val Loss:  0.55,  Val Acc: 82.30%,  Time: 0:00:02 *</span><br><span class="line">Iter:    200,  Train Loss:  0.41,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 85.91%,  Time: 0:00:04 *</span><br><span class="line">Iter:    300,  Train Loss:  0.57,  Train Acc: 82.03%,  Val Loss:  0.41,  Val Acc: 86.64%,  Time: 0:00:06 *</span><br><span class="line">Iter:    400,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.37,  Val Acc: 87.85%,  Time: 0:00:07 *</span><br><span class="line">Iter:    500,  Train Loss:   0.3,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.41%,  Time: 0:00:09 *</span><br><span class="line">Iter:    600,  Train Loss:  0.34,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 89.66%,  Time: 0:00:11 *</span><br><span class="line">Iter:    700,  Train Loss:  0.21,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 89.61%,  Time: 0:00:12</span><br><span class="line">Iter:    800,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:   0.3,  Val Acc: 90.31%,  Time: 0:00:14 *</span><br><span class="line">Iter:    900,  Train Loss:  0.55,  Train Acc: 85.94%,  Val Loss:  0.28,  Val Acc: 91.13%,  Time: 0:00:15 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.29,  Train Acc: 88.28%,  Val Loss:  0.28,  Val Acc: 91.06%,  Time: 0:00:17 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.70%,  Time: 0:00:19 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.31,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.70%,  Time: 0:00:21 *</span><br><span class="line">Iter:   1300,  Train Loss:  0.24,  Train Acc: 93.75%,  Val Loss:  0.28,  Val Acc: 91.10%,  Time: 0:00:22</span><br><span class="line">Iter:   1400,  Train Loss:  0.27,  Train Acc: 89.84%,  Val Loss:  0.25,  Val Acc: 91.79%,  Time: 0:00:24 *</span><br><span class="line">Epoch [2&#x2F;10]</span><br><span class="line">Iter:   1500,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.28,  Val Acc: 90.45%,  Time: 0:00:26</span><br><span class="line">Iter:   1600,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.70%,  Time: 0:00:27</span><br><span class="line">Iter:   1700,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.27,  Val Acc: 91.42%,  Time: 0:00:29</span><br><span class="line">Iter:   1800,  Train Loss:  0.14,  Train Acc: 95.31%,  Val Loss:  0.26,  Val Acc: 91.92%,  Time: 0:00:31</span><br><span class="line">Iter:   1900,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.33%,  Time: 0:00:32 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.24,  Train Acc: 89.06%,  Val Loss:  0.24,  Val Acc: 92.20%,  Time: 0:00:34</span><br><span class="line">Iter:   2100,  Train Loss:  0.27,  Train Acc: 92.19%,  Val Loss:  0.25,  Val Acc: 91.82%,  Time: 0:00:36</span><br><span class="line">Iter:   2200,  Train Loss:   0.2,  Train Acc: 92.19%,  Val Loss:  0.25,  Val Acc: 92.04%,  Time: 0:00:38</span><br><span class="line">Iter:   2300,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.06%,  Time: 0:00:42</span><br><span class="line">Iter:   2400,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.24,  Val Acc: 92.40%,  Time: 0:00:45 *</span><br><span class="line">Iter:   2500,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.22,  Val Acc: 92.59%,  Time: 0:00:48 *</span><br><span class="line">Iter:   2600,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.22,  Val Acc: 92.81%,  Time: 0:00:51 *</span><br><span class="line">Iter:   2700,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.24,  Val Acc: 92.30%,  Time: 0:00:54</span><br><span class="line">Iter:   2800,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 92.73%,  Time: 0:00:57</span><br><span class="line">Epoch [3&#x2F;10]</span><br><span class="line">Iter:   2900,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.26,  Val Acc: 91.74%,  Time: 0:01:01</span><br><span class="line">Iter:   3000,  Train Loss:  0.22,  Train Acc: 94.53%,  Val Loss:  0.23,  Val Acc: 92.44%,  Time: 0:01:04</span><br><span class="line">Iter:   3100,  Train Loss:  0.18,  Train Acc: 94.53%,  Val Loss:  0.23,  Val Acc: 92.74%,  Time: 0:01:07</span><br><span class="line">Iter:   3200,  Train Loss:   0.1,  Train Acc: 96.88%,  Val Loss:  0.22,  Val Acc: 92.83%,  Time: 0:01:11</span><br><span class="line">Iter:   3300,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 92.88%,  Time: 0:01:14</span><br><span class="line">Iter:   3400,  Train Loss:  0.19,  Train Acc: 95.31%,  Val Loss:  0.24,  Val Acc: 92.12%,  Time: 0:01:17</span><br><span class="line">Iter:   3500,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.23,  Val Acc: 92.45%,  Time: 0:01:21</span><br><span class="line">Iter:   3600,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.20%,  Time: 0:01:24</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.22,  Test Acc: 92.82%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9044    0.9080    0.9062      1000</span><br><span class="line">       realty     0.9308    0.9410    0.9359      1000</span><br><span class="line">       stocks     0.8939    0.8850    0.8894      1000</span><br><span class="line">    education     0.9541    0.9560    0.9550      1000</span><br><span class="line">      science     0.9121    0.8610    0.8858      1000</span><br><span class="line">      society     0.8962    0.9240    0.9099      1000</span><br><span class="line">     politics     0.9086    0.9340    0.9211      1000</span><br><span class="line">       sports     0.9869    0.9830    0.9850      1000</span><br><span class="line">         game     0.9336    0.9560    0.9447      1000</span><br><span class="line">entertainment     0.9629    0.9340    0.9482      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9282     10000</span><br><span class="line">    macro avg     0.9283    0.9282    0.9281     10000</span><br><span class="line"> weighted avg     0.9283    0.9282    0.9281     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[908  17  49   0   2  13   7   1   1   2]</span><br><span class="line"> [  7 941  14   4   8  14   6   0   3   3]</span><br><span class="line"> [ 51  17 885   3  18   3  17   0   5   1]</span><br><span class="line"> [  4   4   2 956   2  17   8   0   1   6]</span><br><span class="line"> [ 13   8  23   5 861  23  22   1  40   4]</span><br><span class="line"> [  5   9   3  15   6 924  27   1   2   8]</span><br><span class="line"> [  6   7   6  14   7  18 934   0   2   6]</span><br><span class="line"> [  3   1   1   0   1   3   2 983   1   5]</span><br><span class="line"> [  3   2   4   0  30   2   2   0 956   1]</span><br><span class="line"> [  4   5   3   5   9  14   3  10  13 934]]</span><br><span class="line">Time usage: 0:00:01</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure>



<h2 id="3-3-transformer模型（encoder）"><a href="#3-3-transformer模型（encoder）" class="headerlink" title="3.3 transformer模型（encoder）"></a>3.3 transformer模型（encoder）</h2><p>1.运行脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run.py --model Transformer &gt; nohup.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>2.运行log</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:02, 71756.33it&#x2F;s]</span><br><span class="line">10000it [00:00, 91902.35it&#x2F;s]</span><br><span class="line">10000it [00:00, 89357.25it&#x2F;s]</span><br><span class="line">Time usage: 0:00:03</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (postion_embedding): Positional_Encoding(</span><br><span class="line">    (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  )</span><br><span class="line">  (encoder): Encoder(</span><br><span class="line">    (attention): Multi_Head_Attention(</span><br><span class="line">      (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">      (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">    (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">      (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">      (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (encoders): ModuleList(</span><br><span class="line">    (0): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (fc1): Linear(in_features&#x3D;9600, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.4,  Train Acc: 10.16%,  Val Loss:   4.6,  Val Acc: 10.02%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:   1.4,  Train Acc: 53.12%,  Val Loss:   1.4,  Val Acc: 57.23%,  Time: 0:00:03 *</span><br><span class="line">Iter:    200,  Train Loss:   1.3,  Train Acc: 55.47%,  Val Loss:   1.0,  Val Acc: 68.83%,  Time: 0:00:05 *</span><br><span class="line">Iter:    300,  Train Loss:  0.82,  Train Acc: 67.97%,  Val Loss:   0.9,  Val Acc: 74.19%,  Time: 0:00:07 *</span><br><span class="line">Iter:    400,  Train Loss:  0.84,  Train Acc: 75.00%,  Val Loss:  0.83,  Val Acc: 76.70%,  Time: 0:00:09 *</span><br><span class="line">Iter:    500,  Train Loss:  0.65,  Train Acc: 78.12%,  Val Loss:  0.76,  Val Acc: 78.63%,  Time: 0:00:12 *</span><br><span class="line">Iter:    600,  Train Loss:  0.76,  Train Acc: 75.78%,  Val Loss:  0.76,  Val Acc: 79.15%,  Time: 0:00:15 *</span><br><span class="line">Iter:    700,  Train Loss:  0.63,  Train Acc: 78.12%,  Val Loss:  0.78,  Val Acc: 79.21%,  Time: 0:00:17 </span><br><span class="line">Iter:    800,  Train Loss:  0.59,  Train Acc: 81.25%,  Val Loss:  0.63,  Val Acc: 82.16%,  Time: 0:00:19 *</span><br><span class="line">Iter:    900,  Train Loss:   0.7,  Train Acc: 76.56%,  Val Loss:  0.69,  Val Acc: 80.84%,  Time: 0:00:21 </span><br><span class="line">Iter:   1000,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.69,  Val Acc: 80.87%,  Time: 0:00:23 </span><br><span class="line">Iter:   1100,  Train Loss:  0.52,  Train Acc: 83.59%,  Val Loss:  0.62,  Val Acc: 82.61%,  Time: 0:00:26 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.63,  Train Acc: 82.81%,  Val Loss:  0.63,  Val Acc: 82.48%,  Time: 0:00:28 </span><br><span class="line">Iter:   1300,  Train Loss:  0.59,  Train Acc: 80.47%,  Val Loss:  0.55,  Val Acc: 85.06%,  Time: 0:00:32 *</span><br><span class="line">Iter:   1400,  Train Loss:  0.76,  Train Acc: 74.22%,  Val Loss:  0.56,  Val Acc: 84.04%,  Time: 0:00:35 </span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:  0.63,  Train Acc: 79.69%,  Val Loss:  0.57,  Val Acc: 84.12%,  Time: 0:00:38 </span><br><span class="line">Iter:   1600,  Train Loss:  0.52,  Train Acc: 78.91%,  Val Loss:  0.65,  Val Acc: 82.71%,  Time: 0:00:41 </span><br><span class="line">Iter:   1700,  Train Loss:  0.59,  Train Acc: 82.81%,  Val Loss:  0.58,  Val Acc: 83.98%,  Time: 0:00:44 </span><br><span class="line">Iter:   1800,  Train Loss:  0.46,  Train Acc: 87.50%,  Val Loss:  0.54,  Val Acc: 85.62%,  Time: 0:00:47 *</span><br><span class="line">Iter:   1900,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.53,  Val Acc: 85.76%,  Time: 0:00:50 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.58,  Train Acc: 82.81%,  Val Loss:  0.58,  Val Acc: 84.32%,  Time: 0:00:53 </span><br><span class="line">Iter:   2100,  Train Loss:  0.56,  Train Acc: 86.72%,  Val Loss:  0.55,  Val Acc: 84.90%,  Time: 0:00:56 </span><br><span class="line">Iter:   2200,  Train Loss:  0.36,  Train Acc: 89.84%,  Val Loss:  0.52,  Val Acc: 85.77%,  Time: 0:01:00 *</span><br><span class="line">Iter:   2300,  Train Loss:  0.43,  Train Acc: 86.72%,  Val Loss:  0.55,  Val Acc: 84.77%,  Time: 0:01:03 </span><br><span class="line">Iter:   2400,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.56,  Val Acc: 85.17%,  Time: 0:01:06 </span><br><span class="line">Iter:   2500,  Train Loss:   0.4,  Train Acc: 85.94%,  Val Loss:  0.56,  Val Acc: 84.99%,  Time: 0:01:10 </span><br><span class="line">Iter:   2600,  Train Loss:  0.57,  Train Acc: 79.69%,  Val Loss:  0.55,  Val Acc: 84.93%,  Time: 0:01:13 </span><br><span class="line">Iter:   2700,  Train Loss:  0.44,  Train Acc: 87.50%,  Val Loss:  0.48,  Val Acc: 86.88%,  Time: 0:01:16 *</span><br><span class="line">Iter:   2800,  Train Loss:  0.57,  Train Acc: 82.81%,  Val Loss:   0.5,  Val Acc: 86.41%,  Time: 0:01:20 </span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.59,  Train Acc: 79.69%,  Val Loss:  0.59,  Val Acc: 84.07%,  Time: 0:01:23 </span><br><span class="line">Iter:   3000,  Train Loss:  0.47,  Train Acc: 86.72%,  Val Loss:  0.52,  Val Acc: 86.12%,  Time: 0:01:27 </span><br><span class="line">Iter:   3100,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.48,  Val Acc: 86.31%,  Time: 0:01:29 </span><br><span class="line">Iter:   3200,  Train Loss:  0.71,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 86.09%,  Time: 0:01:33 </span><br><span class="line">Iter:   3300,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.48,  Val Acc: 87.24%,  Time: 0:01:36 *</span><br><span class="line">Iter:   3400,  Train Loss:  0.51,  Train Acc: 82.03%,  Val Loss:  0.47,  Val Acc: 87.28%,  Time: 0:01:39 *</span><br><span class="line">Iter:   3500,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.54,  Val Acc: 86.01%,  Time: 0:01:42 </span><br><span class="line">Iter:   3600,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.49,  Val Acc: 87.23%,  Time: 0:01:46 </span><br><span class="line">Iter:   3700,  Train Loss:  0.59,  Train Acc: 80.47%,  Val Loss:  0.48,  Val Acc: 86.53%,  Time: 0:01:49 </span><br><span class="line">Iter:   3800,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.52,  Val Acc: 86.27%,  Time: 0:01:52 </span><br><span class="line">Iter:   3900,  Train Loss:  0.51,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 86.02%,  Time: 0:01:56 </span><br><span class="line">Iter:   4000,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.48,  Val Acc: 87.14%,  Time: 0:02:01 </span><br><span class="line">Iter:   4100,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.48,  Val Acc: 87.02%,  Time: 0:02:04 </span><br><span class="line">Iter:   4200,  Train Loss:  0.52,  Train Acc: 84.38%,  Val Loss:  0.49,  Val Acc: 86.53%,  Time: 0:02:08 </span><br><span class="line">Epoch [4&#x2F;20]</span><br><span class="line">Iter:   4300,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 87.06%,  Time: 0:02:11 </span><br><span class="line">Iter:   4400,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.47,  Val Acc: 87.44%,  Time: 0:02:14 *</span><br><span class="line">Iter:   4500,  Train Loss:  0.47,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 87.48%,  Time: 0:02:17 *</span><br><span class="line">Iter:   4600,  Train Loss:  0.38,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 87.91%,  Time: 0:02:21 *</span><br><span class="line">Iter:   4700,  Train Loss:  0.52,  Train Acc: 82.81%,  Val Loss:  0.45,  Val Acc: 87.24%,  Time: 0:02:25 </span><br><span class="line">Iter:   4800,  Train Loss:  0.33,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 87.63%,  Time: 0:02:28 </span><br><span class="line">Iter:   4900,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 87.85%,  Time: 0:02:30 </span><br><span class="line">Iter:   5000,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 87.79%,  Time: 0:02:33 </span><br><span class="line">Iter:   5100,  Train Loss:  0.53,  Train Acc: 80.47%,  Val Loss:  0.45,  Val Acc: 87.48%,  Time: 0:02:36 </span><br><span class="line">Iter:   5200,  Train Loss:  0.58,  Train Acc: 85.16%,  Val Loss:  0.47,  Val Acc: 87.18%,  Time: 0:02:40 </span><br><span class="line">Iter:   5300,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.48,  Val Acc: 86.80%,  Time: 0:02:43 </span><br><span class="line">Iter:   5400,  Train Loss:  0.67,  Train Acc: 82.03%,  Val Loss:  0.45,  Val Acc: 87.63%,  Time: 0:02:46 </span><br><span class="line">Iter:   5500,  Train Loss:  0.42,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 87.70%,  Time: 0:02:48 *</span><br><span class="line">Iter:   5600,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.02%,  Time: 0:02:51 *</span><br><span class="line">Epoch [5&#x2F;20]</span><br><span class="line">Iter:   5700,  Train Loss:  0.41,  Train Acc: 86.72%,  Val Loss:  0.42,  Val Acc: 88.50%,  Time: 0:02:54 *</span><br><span class="line">Iter:   5800,  Train Loss:  0.22,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.11%,  Time: 0:02:57 </span><br><span class="line">Iter:   5900,  Train Loss:  0.39,  Train Acc: 91.41%,  Val Loss:  0.44,  Val Acc: 88.15%,  Time: 0:03:00 </span><br><span class="line">Iter:   6000,  Train Loss:  0.44,  Train Acc: 83.59%,  Val Loss:  0.41,  Val Acc: 88.30%,  Time: 0:03:03 *</span><br><span class="line">Iter:   6100,  Train Loss:  0.44,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 88.09%,  Time: 0:03:07 </span><br><span class="line">Iter:   6200,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.42,  Val Acc: 88.42%,  Time: 0:03:09 </span><br><span class="line">Iter:   6300,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.43,  Val Acc: 88.17%,  Time: 0:03:12 </span><br><span class="line">Iter:   6400,  Train Loss:  0.21,  Train Acc: 94.53%,  Val Loss:  0.41,  Val Acc: 88.25%,  Time: 0:03:15 *</span><br><span class="line">Iter:   6500,  Train Loss:  0.44,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 88.49%,  Time: 0:03:19 </span><br><span class="line">Iter:   6600,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 88.01%,  Time: 0:03:22 </span><br><span class="line">Iter:   6700,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.41,  Val Acc: 88.09%,  Time: 0:03:25 </span><br><span class="line">Iter:   6800,  Train Loss:  0.35,  Train Acc: 89.84%,  Val Loss:  0.45,  Val Acc: 88.01%,  Time: 0:03:28 </span><br><span class="line">Iter:   6900,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.41,  Val Acc: 88.16%,  Time: 0:03:32 </span><br><span class="line">Iter:   7000,  Train Loss:  0.41,  Train Acc: 84.38%,  Val Loss:   0.4,  Val Acc: 88.43%,  Time: 0:03:36 *</span><br><span class="line">Epoch [6&#x2F;20]</span><br><span class="line">Iter:   7100,  Train Loss:  0.41,  Train Acc: 83.59%,  Val Loss:   0.4,  Val Acc: 88.95%,  Time: 0:03:39 *</span><br><span class="line">Iter:   7200,  Train Loss:  0.48,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 88.71%,  Time: 0:03:41 </span><br><span class="line">Iter:   7300,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.42,  Val Acc: 88.67%,  Time: 0:03:44 </span><br><span class="line">Iter:   7400,  Train Loss:   0.6,  Train Acc: 77.34%,  Val Loss:  0.43,  Val Acc: 87.97%,  Time: 0:03:47 </span><br><span class="line">Iter:   7500,  Train Loss:  0.34,  Train Acc: 84.38%,  Val Loss:  0.42,  Val Acc: 88.02%,  Time: 0:03:50 </span><br><span class="line">Iter:   7600,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 87.11%,  Time: 0:03:54 </span><br><span class="line">Iter:   7700,  Train Loss:   0.4,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.16%,  Time: 0:03:57 *</span><br><span class="line">Iter:   7800,  Train Loss:  0.39,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 88.64%,  Time: 0:04:00 </span><br><span class="line">Iter:   7900,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 88.47%,  Time: 0:04:03 </span><br><span class="line">Iter:   8000,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 89.05%,  Time: 0:04:06 </span><br><span class="line">Iter:   8100,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.42,  Val Acc: 88.54%,  Time: 0:04:09 </span><br><span class="line">Iter:   8200,  Train Loss:  0.37,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.00%,  Time: 0:04:13 </span><br><span class="line">Iter:   8300,  Train Loss:  0.33,  Train Acc: 89.84%,  Val Loss:  0.41,  Val Acc: 87.85%,  Time: 0:04:16 </span><br><span class="line">Iter:   8400,  Train Loss:  0.53,  Train Acc: 78.91%,  Val Loss:  0.37,  Val Acc: 88.97%,  Time: 0:04:20 *</span><br><span class="line">Epoch [7&#x2F;20]</span><br><span class="line">Iter:   8500,  Train Loss:   0.5,  Train Acc: 85.16%,  Val Loss:  0.43,  Val Acc: 88.20%,  Time: 0:04:22 </span><br><span class="line">Iter:   8600,  Train Loss:  0.25,  Train Acc: 92.19%,  Val Loss:  0.42,  Val Acc: 88.93%,  Time: 0:04:26 </span><br><span class="line">Iter:   8700,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.48%,  Time: 0:04:29 </span><br><span class="line">Iter:   8800,  Train Loss:  0.44,  Train Acc: 82.81%,  Val Loss:  0.39,  Val Acc: 88.89%,  Time: 0:04:33 </span><br><span class="line">Iter:   8900,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 89.18%,  Time: 0:04:36 </span><br><span class="line">Iter:   9000,  Train Loss:  0.23,  Train Acc: 91.41%,  Val Loss:  0.39,  Val Acc: 89.37%,  Time: 0:04:39 </span><br><span class="line">Iter:   9100,  Train Loss:   0.5,  Train Acc: 84.38%,  Val Loss:  0.39,  Val Acc: 89.34%,  Time: 0:04:43 </span><br><span class="line">Iter:   9200,  Train Loss:   0.4,  Train Acc: 90.62%,  Val Loss:   0.4,  Val Acc: 88.81%,  Time: 0:04:46 </span><br><span class="line">Iter:   9300,  Train Loss:  0.41,  Train Acc: 85.16%,  Val Loss:  0.39,  Val Acc: 89.10%,  Time: 0:04:50 </span><br><span class="line">Iter:   9400,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.43,  Val Acc: 88.22%,  Time: 0:04:54 </span><br><span class="line">Iter:   9500,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:   0.4,  Val Acc: 89.02%,  Time: 0:04:57 </span><br><span class="line">Iter:   9600,  Train Loss:  0.47,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.25%,  Time: 0:05:01 </span><br><span class="line">Iter:   9700,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 88.91%,  Time: 0:05:04 </span><br><span class="line">Iter:   9800,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:  0.39,  Val Acc: 89.03%,  Time: 0:05:07 </span><br><span class="line">Epoch [8&#x2F;20]</span><br><span class="line">Iter:   9900,  Train Loss:  0.51,  Train Acc: 78.91%,  Val Loss:  0.39,  Val Acc: 89.19%,  Time: 0:05:11 </span><br><span class="line">Iter:  10000,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:   0.4,  Val Acc: 88.75%,  Time: 0:05:14 </span><br><span class="line">Iter:  10100,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.38,  Val Acc: 89.28%,  Time: 0:05:17 </span><br><span class="line">Iter:  10200,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:05:20 </span><br><span class="line">Iter:  10300,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 89.62%,  Time: 0:05:24 *</span><br><span class="line">Iter:  10400,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.22%,  Time: 0:05:27 </span><br><span class="line">Iter:  10500,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.41,  Val Acc: 88.86%,  Time: 0:05:31 </span><br><span class="line">Iter:  10600,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:   0.4,  Val Acc: 89.12%,  Time: 0:05:34 </span><br><span class="line">Iter:  10700,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 88.98%,  Time: 0:05:37 </span><br><span class="line">Iter:  10800,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.39,  Val Acc: 88.85%,  Time: 0:05:41 </span><br><span class="line">Iter:  10900,  Train Loss:  0.42,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 89.07%,  Time: 0:05:45 </span><br><span class="line">Iter:  11000,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.04%,  Time: 0:05:48 </span><br><span class="line">Iter:  11100,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 88.94%,  Time: 0:05:51 </span><br><span class="line">Iter:  11200,  Train Loss:  0.44,  Train Acc: 82.81%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:05:54 </span><br><span class="line">Epoch [9&#x2F;20]</span><br><span class="line">Iter:  11300,  Train Loss:  0.33,  Train Acc: 86.72%,  Val Loss:  0.38,  Val Acc: 89.51%,  Time: 0:05:57 </span><br><span class="line">Iter:  11400,  Train Loss:  0.39,  Train Acc: 85.16%,  Val Loss:   0.4,  Val Acc: 88.89%,  Time: 0:06:01 </span><br><span class="line">Iter:  11500,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:   0.4,  Val Acc: 89.20%,  Time: 0:06:04 </span><br><span class="line">Iter:  11600,  Train Loss:  0.19,  Train Acc: 95.31%,  Val Loss:  0.37,  Val Acc: 89.70%,  Time: 0:06:06 </span><br><span class="line">Iter:  11700,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.89%,  Time: 0:06:11 *</span><br><span class="line">Iter:  11800,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 89.17%,  Time: 0:06:14 </span><br><span class="line">Iter:  11900,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.39,  Val Acc: 89.47%,  Time: 0:06:17 </span><br><span class="line">Iter:  12000,  Train Loss:  0.48,  Train Acc: 86.72%,  Val Loss:  0.39,  Val Acc: 89.43%,  Time: 0:06:20 </span><br><span class="line">Iter:  12100,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.36,  Val Acc: 89.82%,  Time: 0:06:27 *</span><br><span class="line">Iter:  12200,  Train Loss:   0.3,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.53%,  Time: 0:06:30 </span><br><span class="line">Iter:  12300,  Train Loss:   0.3,  Train Acc: 92.97%,  Val Loss:  0.39,  Val Acc: 88.93%,  Time: 0:06:33 </span><br><span class="line">Iter:  12400,  Train Loss:  0.23,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.35%,  Time: 0:06:36 </span><br><span class="line">Iter:  12500,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 89.21%,  Time: 0:06:39 </span><br><span class="line">Iter:  12600,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.75%,  Time: 0:06:44 *</span><br><span class="line">Epoch [10&#x2F;20]</span><br><span class="line">Iter:  12700,  Train Loss:  0.27,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.64%,  Time: 0:06:48 *</span><br><span class="line">Iter:  12800,  Train Loss:   0.2,  Train Acc: 91.41%,  Val Loss:  0.37,  Val Acc: 89.23%,  Time: 0:06:51 </span><br><span class="line">Iter:  12900,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 89.47%,  Time: 0:06:54 </span><br><span class="line">Iter:  13000,  Train Loss:  0.38,  Train Acc: 91.41%,  Val Loss:   0.4,  Val Acc: 89.49%,  Time: 0:06:58 </span><br><span class="line">Iter:  13100,  Train Loss:  0.41,  Train Acc: 88.28%,  Val Loss:  0.37,  Val Acc: 89.39%,  Time: 0:07:01 </span><br><span class="line">Iter:  13200,  Train Loss:  0.55,  Train Acc: 85.16%,  Val Loss:  0.37,  Val Acc: 89.49%,  Time: 0:07:06 </span><br><span class="line">Iter:  13300,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 89.90%,  Time: 0:07:09 </span><br><span class="line">Iter:  13400,  Train Loss:  0.28,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 89.26%,  Time: 0:07:12 </span><br><span class="line">Iter:  13500,  Train Loss:   0.3,  Train Acc: 88.28%,  Val Loss:  0.38,  Val Acc: 89.65%,  Time: 0:07:15 </span><br><span class="line">Iter:  13600,  Train Loss:  0.34,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 88.99%,  Time: 0:07:18 </span><br><span class="line">Iter:  13700,  Train Loss:  0.26,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.44%,  Time: 0:07:21 </span><br><span class="line">Iter:  13800,  Train Loss:  0.43,  Train Acc: 83.59%,  Val Loss:  0.35,  Val Acc: 89.62%,  Time: 0:07:25 </span><br><span class="line">Iter:  13900,  Train Loss:  0.28,  Train Acc: 92.19%,  Val Loss:   0.4,  Val Acc: 88.97%,  Time: 0:07:29 </span><br><span class="line">Iter:  14000,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.51%,  Time: 0:07:32 </span><br><span class="line">Epoch [11&#x2F;20]</span><br><span class="line">Iter:  14100,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.14%,  Time: 0:07:35 </span><br><span class="line">Iter:  14200,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.37,  Val Acc: 89.48%,  Time: 0:07:39 </span><br><span class="line">Iter:  14300,  Train Loss:  0.27,  Train Acc: 89.84%,  Val Loss:  0.37,  Val Acc: 89.69%,  Time: 0:07:42 </span><br><span class="line">Iter:  14400,  Train Loss:  0.23,  Train Acc: 90.62%,  Val Loss:  0.37,  Val Acc: 89.92%,  Time: 0:07:46 </span><br><span class="line">Iter:  14500,  Train Loss:  0.33,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.72%,  Time: 0:07:49 </span><br><span class="line">Iter:  14600,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.37,  Val Acc: 89.83%,  Time: 0:07:52 </span><br><span class="line">Iter:  14700,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 89.63%,  Time: 0:07:55 </span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.34,  Test Acc: 89.88%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.8745    0.8710    0.8727      1000</span><br><span class="line">       realty     0.9173    0.9210    0.9192      1000</span><br><span class="line">       stocks     0.8308    0.8350    0.8329      1000</span><br><span class="line">    education     0.9356    0.9450    0.9403      1000</span><br><span class="line">      science     0.8395    0.8160    0.8276      1000</span><br><span class="line">      society     0.9097    0.9070    0.9084      1000</span><br><span class="line">     politics     0.8994    0.8760    0.8875      1000</span><br><span class="line">       sports     0.9896    0.9530    0.9710      1000</span><br><span class="line">         game     0.9312    0.9070    0.9189      1000</span><br><span class="line">entertainment     0.8661    0.9570    0.9093      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.8988     10000</span><br><span class="line">    macro avg     0.8994    0.8988    0.8988     10000</span><br><span class="line"> weighted avg     0.8994    0.8988    0.8988     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[871  18  74   6  13   7   5   1   0   5]</span><br><span class="line"> [ 16 921  21   1   9  15   4   1   1  11]</span><br><span class="line"> [ 59  22 835   1  38   4  25   1  10   5]</span><br><span class="line"> [  3   4   0 945   8  11   8   1   4  16]</span><br><span class="line"> [ 15  10  40   9 816  15  24   1  38  32]</span><br><span class="line"> [  5  12   1  21   7 907  22   0   7  18]</span><br><span class="line"> [ 15   3  24  12  21  26 876   0   2  21]</span><br><span class="line"> [  1   2   2   3   5   3   6 953   0  25]</span><br><span class="line"> [  6   5   5   6  47   4   3   2 907  15]</span><br><span class="line"> [  5   7   3   6   8   5   1   3   5 957]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure>

<p>长文本（标题+正文）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14883.19it&#x2F;s]</span><br><span class="line">10000it [00:00, 12805.79it&#x2F;s]</span><br><span class="line">10000it [00:00, 14804.58it&#x2F;s]Time usage: 0:00:14</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (postion_embedding): Positional_Encoding(</span><br><span class="line">    (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  )</span><br><span class="line">  (encoder): Encoder(</span><br><span class="line">    (attention): Multi_Head_Attention(</span><br><span class="line">      (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">      (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">    (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">      (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">      (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">      (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">      (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (encoders): ModuleList(</span><br><span class="line">    (0): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): Encoder(</span><br><span class="line">      (attention): Multi_Head_Attention(</span><br><span class="line">        (fc_Q): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_K): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (fc_V): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (attention): Scaled_Dot_Product_Attention()</span><br><span class="line">        (fc): Linear(in_features&#x3D;300, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">      (feed_forward): Position_wise_Feed_Forward(</span><br><span class="line">        (fc1): Linear(in_features&#x3D;300, out_features&#x3D;1024, bias&#x3D;True)</span><br><span class="line">        (fc2): Linear(in_features&#x3D;1024, out_features&#x3D;300, bias&#x3D;True)</span><br><span class="line">        (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">        (layer_norm): LayerNorm((300,), eps&#x3D;1e-05, elementwise_affine&#x3D;True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (fc1): Linear(in_features&#x3D;9600, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.5,  Train Acc:  8.59%,  Val Loss:   5.1,  Val Acc: 10.00%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:   1.2,  Train Acc: 54.69%,  Val Loss:   1.4,  Val Acc: 59.11%,  Time: 0:00:05 *</span><br><span class="line">Iter:    200,  Train Loss:  0.91,  Train Acc: 64.06%,  Val Loss:  0.91,  Val Acc: 73.32%,  Time: 0:00:08 *</span><br><span class="line">Iter:    300,  Train Loss:  0.93,  Train Acc: 66.41%,  Val Loss:  0.67,  Val Acc: 80.04%,  Time: 0:00:11 *</span><br><span class="line">Iter:    400,  Train Loss:  0.94,  Train Acc: 68.75%,  Val Loss:  0.67,  Val Acc: 81.33%,  Time: 0:00:14</span><br><span class="line">Iter:    500,  Train Loss:   0.7,  Train Acc: 75.00%,  Val Loss:  0.59,  Val Acc: 83.27%,  Time: 0:00:18 *</span><br><span class="line">Iter:    600,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.56,  Val Acc: 84.59%,  Time: 0:00:21 *</span><br><span class="line">Iter:    700,  Train Loss:  0.59,  Train Acc: 79.69%,  Val Loss:  0.53,  Val Acc: 85.44%,  Time: 0:00:24 *</span><br><span class="line">Iter:    800,  Train Loss:  0.47,  Train Acc: 85.16%,  Val Loss:  0.66,  Val Acc: 82.96%,  Time: 0:00:27</span><br><span class="line">Iter:    900,  Train Loss:  0.71,  Train Acc: 81.25%,  Val Loss:  0.54,  Val Acc: 85.88%,  Time: 0:00:30</span><br><span class="line">Iter:   1000,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.48,  Val Acc: 86.87%,  Time: 0:00:34 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.51,  Train Acc: 84.38%,  Val Loss:   0.5,  Val Acc: 86.69%,  Time: 0:00:37</span><br><span class="line">Iter:   1200,  Train Loss:  0.63,  Train Acc: 82.03%,  Val Loss:   0.5,  Val Acc: 87.30%,  Time: 0:00:40</span><br><span class="line">Iter:   1300,  Train Loss:  0.48,  Train Acc: 83.59%,  Val Loss:  0.48,  Val Acc: 87.17%,  Time: 0:00:43</span><br><span class="line">Iter:   1400,  Train Loss:  0.52,  Train Acc: 83.59%,  Val Loss:  0.49,  Val Acc: 87.48%,  Time: 0:00:46</span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:   0.6,  Train Acc: 79.69%,  Val Loss:  0.48,  Val Acc: 87.80%,  Time: 0:00:49 *</span><br><span class="line">Iter:   1600,  Train Loss:  0.46,  Train Acc: 86.72%,  Val Loss:  0.48,  Val Acc: 87.64%,  Time: 0:00:50</span><br><span class="line">Iter:   1700,  Train Loss:  0.43,  Train Acc: 85.16%,  Val Loss:  0.56,  Val Acc: 85.89%,  Time: 0:00:52</span><br><span class="line">Iter:   1800,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.43,  Val Acc: 88.56%,  Time: 0:00:54 *</span><br><span class="line">Iter:   1900,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.43,  Val Acc: 89.19%,  Time: 0:00:56</span><br><span class="line">Iter:   2000,  Train Loss:  0.43,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 88.27%,  Time: 0:00:57</span><br><span class="line">Iter:   2100,  Train Loss:  0.58,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 88.74%,  Time: 0:00:59 *</span><br><span class="line">Iter:   2200,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:  0.45,  Val Acc: 88.39%,  Time: 0:01:01</span><br><span class="line">Iter:   2300,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 89.75%,  Time: 0:01:02 *</span><br><span class="line">Iter:   2400,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 88.05%,  Time: 0:01:04</span><br><span class="line">Iter:   2500,  Train Loss:  0.55,  Train Acc: 82.03%,  Val Loss:  0.45,  Val Acc: 88.85%,  Time: 0:01:05</span><br><span class="line">Iter:   2600,  Train Loss:  0.44,  Train Acc: 83.59%,  Val Loss:  0.44,  Val Acc: 89.37%,  Time: 0:01:07</span><br><span class="line">Iter:   2700,  Train Loss:  0.52,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 89.23%,  Time: 0:01:09</span><br><span class="line">Iter:   2800,  Train Loss:   0.5,  Train Acc: 82.03%,  Val Loss:  0.41,  Val Acc: 89.03%,  Time: 0:01:11</span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.41,  Val Acc: 89.40%,  Time: 0:01:12</span><br><span class="line">Iter:   3000,  Train Loss:  0.38,  Train Acc: 86.72%,  Val Loss:  0.41,  Val Acc: 89.39%,  Time: 0:01:14</span><br><span class="line">Iter:   3100,  Train Loss:  0.26,  Train Acc: 87.50%,  Val Loss:  0.45,  Val Acc: 89.05%,  Time: 0:01:15</span><br><span class="line">Iter:   3200,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 90.10%,  Time: 0:01:17 *</span><br><span class="line">Iter:   3300,  Train Loss:  0.37,  Train Acc: 91.41%,  Val Loss:   0.4,  Val Acc: 90.25%,  Time: 0:01:19</span><br><span class="line">Iter:   3400,  Train Loss:  0.43,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 89.23%,  Time: 0:01:20</span><br><span class="line">Iter:   3500,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.35,  Val Acc: 90.79%,  Time: 0:01:22 *</span><br><span class="line">Iter:   3600,  Train Loss:  0.51,  Train Acc: 87.50%,  Val Loss:   0.4,  Val Acc: 89.65%,  Time: 0:01:24</span><br><span class="line">Iter:   3700,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 90.74%,  Time: 0:01:26 *</span><br><span class="line">Iter:   3800,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 90.38%,  Time: 0:01:27</span><br><span class="line">Iter:   3900,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.41,  Val Acc: 89.36%,  Time: 0:01:29</span><br><span class="line">Iter:   4000,  Train Loss:  0.47,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 90.55%,  Time: 0:01:31</span><br><span class="line">Iter:   4100,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:  0.36,  Val Acc: 90.73%,  Time: 0:01:33</span><br><span class="line">Iter:   4200,  Train Loss:  0.31,  Train Acc: 92.97%,  Val Loss:  0.45,  Val Acc: 88.83%,  Time: 0:01:35</span><br><span class="line">Epoch [4&#x2F;20]</span><br><span class="line">Iter:   4300,  Train Loss:   0.2,  Train Acc: 93.75%,  Val Loss:   0.4,  Val Acc: 89.56%,  Time: 0:01:37</span><br><span class="line">Iter:   4400,  Train Loss:  0.34,  Train Acc: 86.72%,  Val Loss:  0.34,  Val Acc: 90.85%,  Time: 0:01:38 *</span><br><span class="line">Iter:   4500,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.45,  Val Acc: 89.03%,  Time: 0:01:40</span><br><span class="line">Iter:   4600,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 90.92%,  Time: 0:01:42</span><br><span class="line">Iter:   4700,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.38,  Val Acc: 90.14%,  Time: 0:01:44</span><br><span class="line">Iter:   4800,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 90.78%,  Time: 0:01:46</span><br><span class="line">Iter:   4900,  Train Loss:  0.49,  Train Acc: 85.16%,  Val Loss:  0.36,  Val Acc: 90.53%,  Time: 0:01:47</span><br><span class="line">Iter:   5000,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 90.31%,  Time: 0:01:49</span><br><span class="line">Iter:   5100,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 90.70%,  Time: 0:01:51 *</span><br><span class="line">Iter:   5200,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 90.89%,  Time: 0:01:53 *</span><br><span class="line">Iter:   5300,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.32,  Val Acc: 91.47%,  Time: 0:01:54 *</span><br><span class="line">Iter:   5400,  Train Loss:  0.28,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 91.17%,  Time: 0:01:56</span><br><span class="line">Iter:   5500,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.32,  Val Acc: 90.96%,  Time: 0:01:58 *</span><br><span class="line">Iter:   5600,  Train Loss:  0.35,  Train Acc: 87.50%,  Val Loss:  0.31,  Val Acc: 91.41%,  Time: 0:02:00 *</span><br><span class="line">Epoch [5&#x2F;20]</span><br><span class="line">Iter:   5700,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.33,  Val Acc: 90.82%,  Time: 0:02:02</span><br><span class="line">Iter:   5800,  Train Loss:  0.28,  Train Acc: 90.62%,  Val Loss:  0.32,  Val Acc: 91.13%,  Time: 0:02:03</span><br><span class="line">Iter:   5900,  Train Loss:  0.28,  Train Acc: 87.50%,  Val Loss:  0.33,  Val Acc: 90.78%,  Time: 0:02:05</span><br><span class="line">Iter:   6000,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.31,  Val Acc: 91.33%,  Time: 0:02:07</span><br><span class="line">Iter:   6100,  Train Loss:  0.18,  Train Acc: 95.31%,  Val Loss:  0.36,  Val Acc: 90.30%,  Time: 0:02:08</span><br><span class="line">Iter:   6200,  Train Loss:  0.46,  Train Acc: 84.38%,  Val Loss:  0.35,  Val Acc: 90.57%,  Time: 0:02:10</span><br><span class="line">Iter:   6300,  Train Loss:  0.24,  Train Acc: 91.41%,  Val Loss:   0.3,  Val Acc: 91.65%,  Time: 0:02:12 *</span><br><span class="line">Iter:   6400,  Train Loss:   0.2,  Train Acc: 92.97%,  Val Loss:   0.3,  Val Acc: 91.65%,  Time: 0:02:14</span><br><span class="line">Iter:   6500,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.31,  Val Acc: 91.30%,  Time: 0:02:16</span><br><span class="line">Iter:   6600,  Train Loss:  0.32,  Train Acc: 86.72%,  Val Loss:   0.3,  Val Acc: 91.93%,  Time: 0:02:18</span><br><span class="line">Iter:   6700,  Train Loss:   0.2,  Train Acc: 94.53%,  Val Loss:  0.31,  Val Acc: 91.30%,  Time: 0:02:20</span><br><span class="line">Iter:   6800,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.71%,  Time: 0:02:21 *</span><br><span class="line">Iter:   6900,  Train Loss:  0.21,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.89%,  Time: 0:02:23</span><br><span class="line">Iter:   7000,  Train Loss:  0.44,  Train Acc: 85.94%,  Val Loss:   0.3,  Val Acc: 91.50%,  Time: 0:02:25</span><br><span class="line">Epoch [6&#x2F;20]</span><br><span class="line">Iter:   7100,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.28,  Val Acc: 91.57%,  Time: 0:02:27 *</span><br><span class="line">Iter:   7200,  Train Loss:   0.2,  Train Acc: 92.19%,  Val Loss:  0.29,  Val Acc: 91.81%,  Time: 0:02:29</span><br><span class="line">Iter:   7300,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.34,  Val Acc: 90.86%,  Time: 0:02:31</span><br><span class="line">Iter:   7400,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.29,  Val Acc: 91.68%,  Time: 0:02:33</span><br><span class="line">Iter:   7500,  Train Loss:  0.21,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 92.08%,  Time: 0:02:35</span><br><span class="line">Iter:   7600,  Train Loss:  0.26,  Train Acc: 89.06%,  Val Loss:   0.3,  Val Acc: 91.82%,  Time: 0:02:36</span><br><span class="line">Iter:   7700,  Train Loss:  0.23,  Train Acc: 90.62%,  Val Loss:   0.3,  Val Acc: 91.81%,  Time: 0:02:38</span><br><span class="line">Iter:   7800,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.28,  Val Acc: 91.81%,  Time: 0:02:40 *</span><br><span class="line">Iter:   7900,  Train Loss:  0.34,  Train Acc: 91.41%,  Val Loss:  0.27,  Val Acc: 92.38%,  Time: 0:02:42 *</span><br><span class="line">Iter:   8000,  Train Loss:  0.51,  Train Acc: 82.81%,  Val Loss:  0.28,  Val Acc: 92.01%,  Time: 0:02:44</span><br><span class="line">Iter:   8100,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.69%,  Time: 0:02:46</span><br><span class="line">Iter:   8200,  Train Loss:  0.34,  Train Acc: 89.84%,  Val Loss:  0.31,  Val Acc: 91.31%,  Time: 0:02:48</span><br><span class="line">Iter:   8300,  Train Loss:  0.34,  Train Acc: 87.50%,  Val Loss:  0.32,  Val Acc: 91.25%,  Time: 0:02:49</span><br><span class="line">Iter:   8400,  Train Loss:  0.27,  Train Acc: 92.19%,  Val Loss:  0.28,  Val Acc: 92.02%,  Time: 0:02:51</span><br><span class="line">Epoch [7&#x2F;20]</span><br><span class="line">Iter:   8500,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.79%,  Time: 0:02:53</span><br><span class="line">Iter:   8600,  Train Loss:  0.26,  Train Acc: 89.84%,  Val Loss:  0.28,  Val Acc: 92.21%,  Time: 0:02:55</span><br><span class="line">Iter:   8700,  Train Loss:  0.25,  Train Acc: 89.84%,  Val Loss:  0.29,  Val Acc: 91.82%,  Time: 0:02:57</span><br><span class="line">Iter:   8800,  Train Loss:  0.23,  Train Acc: 92.97%,  Val Loss:  0.27,  Val Acc: 91.80%,  Time: 0:02:59</span><br><span class="line">Iter:   8900,  Train Loss:  0.29,  Train Acc: 92.19%,  Val Loss:  0.29,  Val Acc: 91.47%,  Time: 0:03:01</span><br><span class="line">Iter:   9000,  Train Loss:  0.17,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.84%,  Time: 0:03:03</span><br><span class="line">Iter:   9100,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.31,  Val Acc: 90.89%,  Time: 0:03:05</span><br><span class="line">Iter:   9200,  Train Loss:  0.18,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.57%,  Time: 0:03:07</span><br><span class="line">Iter:   9300,  Train Loss:  0.34,  Train Acc: 89.84%,  Val Loss:  0.27,  Val Acc: 91.93%,  Time: 0:03:08</span><br><span class="line">Iter:   9400,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.27,  Val Acc: 92.09%,  Time: 0:03:10</span><br><span class="line">Iter:   9500,  Train Loss:  0.25,  Train Acc: 92.19%,  Val Loss:  0.29,  Val Acc: 92.05%,  Time: 0:03:12</span><br><span class="line">Iter:   9600,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.28,  Val Acc: 92.03%,  Time: 0:03:13</span><br><span class="line">Iter:   9700,  Train Loss:  0.26,  Train Acc: 89.84%,  Val Loss:   0.3,  Val Acc: 91.83%,  Time: 0:03:15</span><br><span class="line">Iter:   9800,  Train Loss:  0.19,  Train Acc: 92.97%,  Val Loss:  0.29,  Val Acc: 91.76%,  Time: 0:03:17</span><br><span class="line">Epoch [8&#x2F;20]</span><br><span class="line">Iter:   9900,  Train Loss:  0.29,  Train Acc: 88.28%,  Val Loss:  0.28,  Val Acc: 91.96%,  Time: 0:03:18</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.26,  Test Acc: 92.28%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9250    0.8760    0.8998      1000</span><br><span class="line">       realty     0.9103    0.9340    0.9220      1000</span><br><span class="line">       stocks     0.8696    0.9000    0.8845      1000</span><br><span class="line">    education     0.9550    0.9330    0.9439      1000</span><br><span class="line">      science     0.9078    0.8660    0.8864      1000</span><br><span class="line">      society     0.9214    0.8790    0.8997      1000</span><br><span class="line">     politics     0.8917    0.9300    0.9104      1000</span><br><span class="line">       sports     0.9879    0.9820    0.9850      1000</span><br><span class="line">         game     0.9367    0.9620    0.9492      1000</span><br><span class="line">entertainment     0.9262    0.9660    0.9457      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9228     10000</span><br><span class="line">    macro avg     0.9231    0.9228    0.9227     10000</span><br><span class="line"> weighted avg     0.9231    0.9228    0.9227     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[876  19  72   1   8   9   7   0   0   8]</span><br><span class="line"> [  9 934  19   2  10  11   8   0   2   5]</span><br><span class="line"> [ 36  12 900   2  22   2  17   1   6   2]</span><br><span class="line"> [  1  12   4 933   2  12  24   1   3   8]</span><br><span class="line"> [ 12   8  22   5 866  10  15   1  43  18]</span><br><span class="line"> [  4  19   5  24  17 879  33   2   3  14]</span><br><span class="line"> [  4   9   7   5  10  23 930   1   2   9]</span><br><span class="line"> [  1   2   2   0   2   2   2 982   0   7]</span><br><span class="line"> [  4   6   4   1  13   2   2   0 962   6]</span><br><span class="line"> [  0   5   0   4   4   4   5   6   6 966]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure>



<h2 id="3-4-cnn模型"><a href="#3-4-cnn模型" class="headerlink" title="3.4 cnn模型"></a>3.4 cnn模型</h2><p>1.运行脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=0,1 </span><br><span class="line">nohup python run.py --model TextCNN &gt; nohup_cnn.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>2.运行log</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:03, 47432.48it&#x2F;s]</span><br><span class="line">10000it [00:00, 46997.85it&#x2F;s]</span><br><span class="line">10000it [00:00, 48451.14it&#x2F;s]Time usage: 0:00:04</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (convs): ModuleList(</span><br><span class="line">    (0): Conv2d(1, 256, kernel_size&#x3D;(2, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(1, 256, kernel_size&#x3D;(3, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (2): Conv2d(1, 256, kernel_size&#x3D;(4, 300), stride&#x3D;(1, 1))</span><br><span class="line">  )</span><br><span class="line">  (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  (fc): Linear(in_features&#x3D;768, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc: 14.84%,  Val Loss:   2.7,  Val Acc: 10.70%,  Time: 0:00:02 *</span><br><span class="line">Iter:    100,  Train Loss:  0.77,  Train Acc: 70.31%,  Val Loss:   0.7,  Val Acc: 78.36%,  Time: 0:00:07 *</span><br><span class="line">Iter:    200,  Train Loss:  0.74,  Train Acc: 74.22%,  Val Loss:  0.55,  Val Acc: 83.18%,  Time: 0:00:12 *</span><br><span class="line">Iter:    300,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.49,  Val Acc: 84.72%,  Time: 0:00:16 *</span><br><span class="line">Iter:    400,  Train Loss:  0.72,  Train Acc: 79.69%,  Val Loss:  0.48,  Val Acc: 85.10%,  Time: 0:00:20 *</span><br><span class="line">Iter:    500,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.44,  Val Acc: 86.10%,  Time: 0:00:24 *</span><br><span class="line">Iter:    600,  Train Loss:  0.49,  Train Acc: 84.38%,  Val Loss:  0.42,  Val Acc: 86.77%,  Time: 0:00:27 *</span><br><span class="line">Iter:    700,  Train Loss:  0.49,  Train Acc: 83.59%,  Val Loss:  0.41,  Val Acc: 87.15%,  Time: 0:00:31 *</span><br><span class="line">Iter:    800,  Train Loss:  0.43,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 87.80%,  Time: 0:00:36 *</span><br><span class="line">Iter:    900,  Train Loss:  0.47,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 88.04%,  Time: 0:00:40 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.38,  Val Acc: 88.54%,  Time: 0:00:45 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.38,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 88.64%,  Time: 0:00:50 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.38,  Train Acc: 86.72%,  Val Loss:  0.37,  Val Acc: 88.99%,  Time: 0:00:55 *</span><br><span class="line">Iter:   1300,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.81%,  Time: 0:01:00 *</span><br><span class="line">Iter:   1400,  Train Loss:  0.52,  Train Acc: 82.81%,  Val Loss:  0.35,  Val Acc: 88.96%,  Time: 0:01:04 *</span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:   0.4,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.09%,  Time: 0:01:10 *</span><br><span class="line">Iter:   1600,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.15%,  Time: 0:01:14</span><br><span class="line">Iter:   1700,  Train Loss:   0.4,  Train Acc: 86.72%,  Val Loss:  0.35,  Val Acc: 89.68%,  Time: 0:01:18 *</span><br><span class="line">Iter:   1800,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.90%,  Time: 0:01:22</span><br><span class="line">Iter:   1900,  Train Loss:  0.36,  Train Acc: 89.84%,  Val Loss:  0.34,  Val Acc: 89.37%,  Time: 0:01:27 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.35,  Train Acc: 85.16%,  Val Loss:  0.35,  Val Acc: 89.47%,  Time: 0:01:31</span><br><span class="line">Iter:   2100,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.40%,  Time: 0:01:35 *</span><br><span class="line">Iter:   2200,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.43%,  Time: 0:01:40</span><br><span class="line">Iter:   2300,  Train Loss:  0.31,  Train Acc: 93.75%,  Val Loss:  0.34,  Val Acc: 89.67%,  Time: 0:01:44 *</span><br><span class="line">Iter:   2400,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.90%,  Time: 0:01:49 *</span><br><span class="line">Iter:   2500,  Train Loss:  0.16,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.18%,  Time: 0:01:53 *</span><br><span class="line">Iter:   2600,  Train Loss:   0.4,  Train Acc: 83.59%,  Val Loss:  0.33,  Val Acc: 90.04%,  Time: 0:01:58</span><br><span class="line">Iter:   2700,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.90%,  Time: 0:02:02</span><br><span class="line">Iter:   2800,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.67%,  Time: 0:02:06</span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 89.80%,  Time: 0:02:11</span><br><span class="line">Iter:   3000,  Train Loss:  0.25,  Train Acc: 91.41%,  Val Loss:  0.34,  Val Acc: 89.71%,  Time: 0:02:15</span><br><span class="line">Iter:   3100,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 89.98%,  Time: 0:02:19</span><br><span class="line">Iter:   3200,  Train Loss:  0.39,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 89.88%,  Time: 0:02:24</span><br><span class="line">Iter:   3300,  Train Loss:  0.29,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.11%,  Time: 0:02:28 *</span><br><span class="line">Iter:   3400,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.97%,  Time: 0:02:33</span><br><span class="line">Iter:   3500,  Train Loss:  0.16,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 90.13%,  Time: 0:02:38</span><br><span class="line">Iter:   3600,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:02:42</span><br><span class="line">Iter:   3700,  Train Loss:  0.36,  Train Acc: 86.72%,  Val Loss:  0.33,  Val Acc: 90.08%,  Time: 0:02:47</span><br><span class="line">Iter:   3800,  Train Loss:  0.36,  Train Acc: 84.38%,  Val Loss:  0.32,  Val Acc: 90.17%,  Time: 0:02:52 *</span><br><span class="line">Iter:   3900,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 90.20%,  Time: 0:02:56</span><br><span class="line">Iter:   4000,  Train Loss:  0.24,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 90.23%,  Time: 0:03:01</span><br><span class="line">Iter:   4100,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:03:05</span><br><span class="line">Iter:   4200,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 89.95%,  Time: 0:03:11</span><br><span class="line">Epoch [4&#x2F;20]</span><br><span class="line">Iter:   4300,  Train Loss:  0.19,  Train Acc: 92.97%,  Val Loss:  0.32,  Val Acc: 90.27%,  Time: 0:03:15 *</span><br><span class="line">Iter:   4400,  Train Loss:  0.19,  Train Acc: 93.75%,  Val Loss:  0.32,  Val Acc: 90.37%,  Time: 0:03:19</span><br><span class="line">Iter:   4500,  Train Loss:  0.37,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 90.21%,  Time: 0:03:24</span><br><span class="line">Iter:   4600,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 90.02%,  Time: 0:03:29</span><br><span class="line">Iter:   4700,  Train Loss:  0.48,  Train Acc: 87.50%,  Val Loss:  0.32,  Val Acc: 90.25%,  Time: 0:03:34</span><br><span class="line">Iter:   4800,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 90.34%,  Time: 0:03:38</span><br><span class="line">Iter:   4900,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 90.26%,  Time: 0:03:41</span><br><span class="line">Iter:   5000,  Train Loss:  0.21,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.88%,  Time: 0:03:46</span><br><span class="line">Iter:   5100,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.21%,  Time: 0:03:50</span><br><span class="line">Iter:   5200,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 90.12%,  Time: 0:03:55</span><br><span class="line">Iter:   5300,  Train Loss:  0.19,  Train Acc: 96.09%,  Val Loss:  0.32,  Val Acc: 90.46%,  Time: 0:04:00 *</span><br><span class="line">Iter:   5400,  Train Loss:  0.47,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 90.17%,  Time: 0:04:04</span><br><span class="line">Iter:   5500,  Train Loss:  0.27,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.41%,  Time: 0:04:08</span><br><span class="line">Iter:   5600,  Train Loss:  0.13,  Train Acc: 95.31%,  Val Loss:  0.33,  Val Acc: 90.17%,  Time: 0:04:13</span><br><span class="line">Epoch [5&#x2F;20]</span><br><span class="line">Iter:   5700,  Train Loss:  0.24,  Train Acc: 94.53%,  Val Loss:  0.34,  Val Acc: 90.07%,  Time: 0:04:18</span><br><span class="line">Iter:   5800,  Train Loss:  0.16,  Train Acc: 93.75%,  Val Loss:  0.34,  Val Acc: 90.14%,  Time: 0:04:22</span><br><span class="line">Iter:   5900,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 90.52%,  Time: 0:04:28</span><br><span class="line">Iter:   6000,  Train Loss:  0.15,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.45%,  Time: 0:04:32</span><br><span class="line">Iter:   6100,  Train Loss:  0.26,  Train Acc: 89.06%,  Val Loss:  0.32,  Val Acc: 90.58%,  Time: 0:04:36</span><br><span class="line">Iter:   6200,  Train Loss:  0.13,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 90.46%,  Time: 0:04:41</span><br><span class="line">Iter:   6300,  Train Loss:  0.12,  Train Acc: 96.88%,  Val Loss:  0.33,  Val Acc: 90.56%,  Time: 0:04:45</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.29,  Test Acc: 91.34%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9193    0.8890    0.9039      1000</span><br><span class="line">       realty     0.9134    0.9490    0.9308      1000</span><br><span class="line">       stocks     0.8363    0.8790    0.8571      1000</span><br><span class="line">    education     0.9540    0.9550    0.9545      1000</span><br><span class="line">      science     0.8800    0.8580    0.8689      1000</span><br><span class="line">      society     0.9067    0.9140    0.9104      1000</span><br><span class="line">     politics     0.9054    0.8900    0.8976      1000</span><br><span class="line">       sports     0.9578    0.9540    0.9559      1000</span><br><span class="line">         game     0.9383    0.9130    0.9255      1000</span><br><span class="line">entertainment     0.9265    0.9330    0.9297      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9134     10000</span><br><span class="line">    macro avg     0.9138    0.9134    0.9134     10000</span><br><span class="line"> weighted avg     0.9138    0.9134    0.9134     10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[889  17  59   4  10   9   7   3   0   2]</span><br><span class="line"> [  9 949  13   2   4   8   4   2   3   6]</span><br><span class="line"> [ 40  24 879   3  21   0  25   3   4   1]</span><br><span class="line"> [  2   2   1 955   6  13   5   2   1  13]</span><br><span class="line"> [  8   8  38   6 858  15  16   5  36  10]</span><br><span class="line"> [  4  22   3  11  10 914  26   2   2   6]</span><br><span class="line"> [  9   7  32  10  15  27 890   4   0   6]</span><br><span class="line"> [  3   2   7   1   3   8   4 954   2  16]</span><br><span class="line"> [  2   1  15   4  36   4   3   8 913  14]</span><br><span class="line"> [  1   7   4   5  12  10   3  13  12 933]]</span><br><span class="line">Time usage: 0:00:00</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure>

<p>长文本（标题+正文）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">Loading data...</span><br><span class="line">Vocab size:4762</span><br><span class="line">180000it [00:12, 14970.72it&#x2F;s]</span><br><span class="line">10000it [00:00, 13435.87it&#x2F;s]</span><br><span class="line">10000it [00:00, 16773.90it&#x2F;s]</span><br><span class="line">Time usage: 0:00:13</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (convs): ModuleList(</span><br><span class="line">    (0): Conv2d(1, 256, kernel_size&#x3D;(2, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(1, 256, kernel_size&#x3D;(3, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (2): Conv2d(1, 256, kernel_size&#x3D;(4, 300), stride&#x3D;(1, 1))</span><br><span class="line">  )</span><br><span class="line">  (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  (fc): Linear(in_features&#x3D;768, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">start model</span><br><span class="line">start network</span><br><span class="line">&lt;bound method Module.parameters of Model(</span><br><span class="line">  (embedding): Embedding(4762, 300)</span><br><span class="line">  (convs): ModuleList(</span><br><span class="line">    (0): Conv2d(1, 256, kernel_size&#x3D;(2, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (1): Conv2d(1, 256, kernel_size&#x3D;(3, 300), stride&#x3D;(1, 1))</span><br><span class="line">    (2): Conv2d(1, 256, kernel_size&#x3D;(4, 300), stride&#x3D;(1, 1))</span><br><span class="line">  )</span><br><span class="line">  (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  (fc): Linear(in_features&#x3D;768, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)&gt;</span><br><span class="line">start train</span><br><span class="line">Epoch [1&#x2F;20]</span><br><span class="line">Iter:      0,  Train Loss:   2.3,  Train Acc: 11.72%,  Val Loss:   2.5,  Val Acc: 10.00%,  Time: 0:00:01 *</span><br><span class="line">Iter:    100,  Train Loss:  0.53,  Train Acc: 83.59%,  Val Loss:  0.53,  Val Acc: 84.06%,  Time: 0:00:07 *</span><br><span class="line">Iter:    200,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.42,  Val Acc: 87.16%,  Time: 0:00:13 *</span><br><span class="line">Iter:    300,  Train Loss:  0.57,  Train Acc: 80.47%,  Val Loss:  0.37,  Val Acc: 88.62%,  Time: 0:00:17 *</span><br><span class="line">Iter:    400,  Train Loss:  0.43,  Train Acc: 88.28%,  Val Loss:  0.34,  Val Acc: 89.45%,  Time: 0:00:23 *</span><br><span class="line">Iter:    500,  Train Loss:   0.3,  Train Acc: 89.84%,  Val Loss:  0.32,  Val Acc: 89.96%,  Time: 0:00:29 *</span><br><span class="line">Iter:    600,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 89.63%,  Time: 0:00:33</span><br><span class="line">Iter:    700,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:   0.3,  Val Acc: 90.63%,  Time: 0:00:39 *</span><br><span class="line">Iter:    800,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.29,  Val Acc: 91.21%,  Time: 0:00:44 *</span><br><span class="line">Iter:    900,  Train Loss:  0.47,  Train Acc: 87.50%,  Val Loss:  0.28,  Val Acc: 91.13%,  Time: 0:00:49 *</span><br><span class="line">Iter:   1000,  Train Loss:  0.28,  Train Acc: 89.84%,  Val Loss:  0.27,  Val Acc: 91.50%,  Time: 0:00:55 *</span><br><span class="line">Iter:   1100,  Train Loss:  0.33,  Train Acc: 90.62%,  Val Loss:  0.26,  Val Acc: 92.00%,  Time: 0:01:01 *</span><br><span class="line">Iter:   1200,  Train Loss:  0.51,  Train Acc: 86.72%,  Val Loss:  0.26,  Val Acc: 91.98%,  Time: 0:01:06</span><br><span class="line">Iter:   1300,  Train Loss:  0.28,  Train Acc: 88.28%,  Val Loss:  0.26,  Val Acc: 91.88%,  Time: 0:01:12 *</span><br><span class="line">Iter:   1400,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.26,  Val Acc: 91.82%,  Time: 0:01:16 *</span><br><span class="line">Epoch [2&#x2F;20]</span><br><span class="line">Iter:   1500,  Train Loss:   0.4,  Train Acc: 89.84%,  Val Loss:  0.25,  Val Acc: 92.30%,  Time: 0:01:22 *</span><br><span class="line">Iter:   1600,  Train Loss:  0.35,  Train Acc: 92.19%,  Val Loss:  0.25,  Val Acc: 92.14%,  Time: 0:01:28 *</span><br><span class="line">Iter:   1700,  Train Loss:  0.31,  Train Acc: 93.75%,  Val Loss:  0.25,  Val Acc: 92.26%,  Time: 0:01:33</span><br><span class="line">Iter:   1800,  Train Loss:  0.23,  Train Acc: 89.84%,  Val Loss:  0.26,  Val Acc: 91.56%,  Time: 0:01:39</span><br><span class="line">Iter:   1900,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.49%,  Time: 0:01:43 *</span><br><span class="line">Iter:   2000,  Train Loss:  0.33,  Train Acc: 91.41%,  Val Loss:  0.25,  Val Acc: 92.26%,  Time: 0:01:49</span><br><span class="line">Iter:   2100,  Train Loss:  0.31,  Train Acc: 92.97%,  Val Loss:  0.24,  Val Acc: 92.43%,  Time: 0:01:55 *</span><br><span class="line">Iter:   2200,  Train Loss:  0.27,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.63%,  Time: 0:01:59 *</span><br><span class="line">Iter:   2300,  Train Loss:  0.22,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.51%,  Time: 0:02:06</span><br><span class="line">Iter:   2400,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.24,  Val Acc: 92.71%,  Time: 0:02:12</span><br><span class="line">Iter:   2500,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.25,  Val Acc: 92.40%,  Time: 0:02:17</span><br><span class="line">Iter:   2600,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 92.69%,  Time: 0:02:24 *</span><br><span class="line">Iter:   2700,  Train Loss:  0.23,  Train Acc: 93.75%,  Val Loss:  0.23,  Val Acc: 92.76%,  Time: 0:02:29</span><br><span class="line">Iter:   2800,  Train Loss:  0.24,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 92.76%,  Time: 0:02:35 *</span><br><span class="line">Epoch [3&#x2F;20]</span><br><span class="line">Iter:   2900,  Train Loss:  0.13,  Train Acc: 96.09%,  Val Loss:  0.24,  Val Acc: 92.43%,  Time: 0:02:41</span><br><span class="line">Iter:   3000,  Train Loss:  0.17,  Train Acc: 92.19%,  Val Loss:  0.23,  Val Acc: 92.86%,  Time: 0:02:47</span><br><span class="line">Iter:   3100,  Train Loss:  0.21,  Train Acc: 94.53%,  Val Loss:  0.23,  Val Acc: 92.68%,  Time: 0:02:54</span><br><span class="line">Iter:   3200,  Train Loss:  0.14,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 92.91%,  Time: 0:02:58 *</span><br><span class="line">Iter:   3300,  Train Loss:  0.18,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.83%,  Time: 0:03:05</span><br><span class="line">Iter:   3400,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.75%,  Time: 0:03:10</span><br><span class="line">Iter:   3500,  Train Loss:  0.18,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 93.05%,  Time: 0:03:16</span><br><span class="line">Iter:   3600,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.24,  Val Acc: 92.68%,  Time: 0:03:22</span><br><span class="line">Iter:   3700,  Train Loss:  0.13,  Train Acc: 96.09%,  Val Loss:  0.23,  Val Acc: 92.89%,  Time: 0:03:28</span><br><span class="line">Iter:   3800,  Train Loss:  0.21,  Train Acc: 92.19%,  Val Loss:  0.24,  Val Acc: 92.93%,  Time: 0:03:34</span><br><span class="line">Iter:   3900,  Train Loss:  0.13,  Train Acc: 95.31%,  Val Loss:  0.23,  Val Acc: 93.03%,  Time: 0:03:39</span><br><span class="line">Iter:   4000,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.23,  Val Acc: 93.02%,  Time: 0:03:46</span><br><span class="line">Iter:   4100,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.24,  Val Acc: 92.72%,  Time: 0:03:52</span><br><span class="line">Iter:   4200,  Train Loss:  0.15,  Train Acc: 93.75%,  Val Loss:  0.24,  Val Acc: 92.88%,  Time: 0:03:58</span><br><span class="line">No optimization for a long time, auto-stopping...</span><br><span class="line">Test Loss:  0.23,  Test Acc: 92.71%</span><br><span class="line">Precision, Recall and F1-Score...</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">      finance     0.9365    0.9000    0.9179      1000</span><br><span class="line">       realty     0.9260    0.9380    0.9319      1000</span><br><span class="line">       stocks     0.8817    0.9170    0.8990      1000</span><br><span class="line">    education     0.9568    0.9520    0.9544      1000</span><br><span class="line">      science     0.9192    0.8760    0.8971      1000</span><br><span class="line">      society     0.8951    0.8960    0.8956      1000</span><br><span class="line">     politics     0.9204    0.9130    0.9167      1000</span><br><span class="line">       sports     0.9732    0.9810    0.9771      1000</span><br><span class="line">         game     0.9360    0.9510    0.9435      1000</span><br><span class="line">entertainment     0.9275    0.9470    0.9372      1000</span><br><span class="line"></span><br><span class="line">     accuracy                         0.9271     10000</span><br><span class="line">    macro avg     0.9272    0.9271    0.9270     10000</span><br><span class="line"> weighted avg     0.9272    0.9271    0.9270     10000</span><br><span class="line"></span><br><span class="line">Confusion Matrix...</span><br><span class="line">[[900  18  57   0   5   9   7   0   0   4]</span><br><span class="line"> [  5 938  16   2   9  10   7   2   6   5]</span><br><span class="line"> [ 32  14 917   4  11   0  15   0   4   3]</span><br><span class="line"> [  1   2   3 952   1  20   9   0   0  12]</span><br><span class="line"> [  5   6  32   5 876  20  11   2  34   9]</span><br><span class="line"> [  5  18   0  19  18 896  20   3   3  18]</span><br><span class="line"> [  8   8   9   9   5  29 913   5   4  10]</span><br><span class="line"> [  2   3   1   0   2   1   2 981   0   8]</span><br><span class="line"> [  1   4   4   0  20   4   2   9 951   5]</span><br><span class="line"> [  2   2   1   4   6  12   6   6  14 947]]</span><br><span class="line">Time usage: 0:00:01</span><br><span class="line">finish train</span><br></pre></td></tr></table></figure>



<h1 id="4-评价"><a href="#4-评价" class="headerlink" title="4 评价"></a>4 评价</h1><h2 id="4-1-评价指标"><a href="#4-1-评价指标" class="headerlink" title="4.1 评价指标"></a>4.1 评价指标</h2><h2 id="4-2-结果汇总"><a href="#4-2-结果汇总" class="headerlink" title="4.2 结果汇总"></a>4.2 结果汇总</h2><blockquote>
<p>参考：</p>
<p>[1] Convolutional Neural Networks for Sentence Classification<br>[2] Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification<br>[3] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br>[4] Attention Is All You Need</p>
<p><a target="_blank" rel="noopener" href="https://github.com/649453932/Chinese-Text-Classification-Pytorch">https://github.com/649453932/Chinese-Text-Classification-Pytorch</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch">https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch</a></p>
</blockquote>

    </div>

    
    
    

    <!--添加版权信息-->
    <div>
        
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script>
  <script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script>
  <link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css">

  <p><span>本文标题:</span>chinese_text_classify</a></p>
  <p><span>本文作者:</span>xydaytoy</a></p>
  <p><span>本文链接:</span><a href="/2021/11/28/chinese-text-classify/" title="chinese_text_classify">http://example.com/2021/11/28/chinese-text-classify/</a>
    <span class="copy-path"  title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://example.com/2021/11/28/chinese-text-classify/"  aria-label="复制成功！"></i></span>
  </p>
  <p><span>版权声明:</span>本博客所有文章除特别声明外，均采用<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND</a>许可协议。转载请注明出处！</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    clipboard.on('success', $(function(){
      $(".fa-clipboard").click(function(){
        swal({   
          title: "",   
          text: '复制成功',   
          html: false,
          timer: 500,   
          showConfirmButton: false
        });
      });
    }));  
</script>

        
    </div>

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/11/27/data-preprocess/" rel="prev" title="data_preprocess">
      <i class="fa fa-chevron-left"></i> data_preprocess
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/11/28/linux-tips/" rel="next" title="linux-tips">
      linux-tips <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7%E5%8F%8A%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="nav-number">1.</span> <span class="nav-text">1 相关工具及目录结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 相关工具</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E4%B8%8E%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 目录结构与初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.2.1 目录结构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E6%95%B0%E6%8D%AE%E7%9A%84%E5%87%86%E5%A4%87"><span class="nav-number">2.</span> <span class="nav-text">2 数据的准备</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-THUCNews"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 THUCNews</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 原始数据格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%9F%E6%88%90"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2 数据集生成</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">3.</span> <span class="nav-text">3 训练过程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-bert%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 bert模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-bilstm-attention%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 bilstm+attention模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-transformer%E6%A8%A1%E5%9E%8B%EF%BC%88encoder%EF%BC%89"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 transformer模型（encoder）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-cnn%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 cnn模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-%E8%AF%84%E4%BB%B7"><span class="nav-number">4.</span> <span class="nav-text">4 评价</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 评价指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-%E7%BB%93%E6%9E%9C%E6%B1%87%E6%80%BB"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 结果汇总</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">xydaytoy</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/xydaytoy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xydaytoy" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:daytoy2018@gmail.com" title="E-Mail → mailto:daytoy2018@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xydaytoy</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
